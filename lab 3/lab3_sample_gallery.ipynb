{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3 Sample Gallery (Codec + Mel)\n",
        "\n",
        "This notebook is now codec-aware and defaults to the codec run path.\n",
        "\n",
        "Use it to:\n",
        "- load an existing run (`lab3_codec_transfer` or `lab3_synthesis`)\n",
        "- optionally generate fresh samples from checkpoint\n",
        "- listen to **source chunk vs generated** side-by-side\n",
        "- compare quick similarity metrics (wave cosine / MFCC cosine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import importlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import soundfile as sf\n",
        "    HAS_SF = True\n",
        "except Exception:\n",
        "    HAS_SF = False\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "cwd = Path.cwd()\n",
        "if (cwd / 'src').exists() and ((cwd / 'run_lab3.py').exists() or (cwd / 'run_lab3_codec.py').exists()):\n",
        "    LAB3_DIR = cwd\n",
        "    REPO_ROOT = cwd.parent\n",
        "elif (cwd / 'lab 3' / 'src').exists():\n",
        "    LAB3_DIR = cwd / 'lab 3'\n",
        "    REPO_ROOT = cwd\n",
        "else:\n",
        "    raise RuntimeError('Run from repo root or lab 3 directory.')\n",
        "\n",
        "if str(LAB3_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(LAB3_DIR))\n",
        "\n",
        "if not hasattr(torch, '_utils'):\n",
        "    torch._utils = importlib.import_module('torch._utils')\n",
        "\n",
        "import subprocess\n",
        "import importlib.util\n",
        "\n",
        "def _ensure_pkg(mod_name: str, pip_name: str | None = None):\n",
        "    if importlib.util.find_spec(mod_name) is None:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name or mod_name])\n",
        "\n",
        "_ensure_pkg('transformers')\n",
        "_ensure_pkg('sentencepiece')\n",
        "\n",
        "from src.lab3_data import load_cache, stratified_split_indices, stratified_group_split_indices\n",
        "from src.lab3_codec_data import load_codec_cache\n",
        "from src.lab3_codec_models import CodecLatentTranslator\n",
        "from src.lab3_codec_bridge import FrozenEncodec\n",
        "from src.lab3_codec_train import build_style_centroid_bank, build_style_exemplar_bank\n",
        "from src.lab3_sampling import resolve_next_run_name\n",
        "\n",
        "print('torch:', torch.__version__)\n",
        "print('soundfile_available:', HAS_SF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69c1d2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defaults now target codec pipeline run1005\n",
        "RUN_KIND = 'codec'   # 'codec' | 'mel' | 'auto'\n",
        "\n",
        "CODEC_RUNS_ROOT = REPO_ROOT / 'saves2' / 'lab3_codec_transfer'\n",
        "MEL_RUNS_ROOT = REPO_ROOT / 'saves2' / 'lab3_synthesis'\n",
        "\n",
        "RUN_DIR = CODEC_RUNS_ROOT / 'run1011'  # set None to auto-pick latest runN in selected root\n",
        "OUTPUT_TAG = 'posttrain_samples'\n",
        "\n",
        "GENERATE_NEW_SAMPLES = False\n",
        "FORCE_REGENERATE = False\n",
        "N_GENERATION_SAMPLES = 50\n",
        "VAL_RATIO = 0.15\n",
        "SEED = 328\n",
        "\n",
        "CODEC_COND_MODE = 'mix'   # centroid | exemplar | mix\n",
        "CODEC_COND_ALPHA = 0.35\n",
        "CODEC_STYLE_JITTER_STD = 0.03\n",
        "\n",
        "PREVIEW_RANDOM = True\n",
        "PREVIEW_N = 8\n",
        "EXPORT_SOURCE_PAIRS = True\n",
        "\n",
        "DEVICE = 'auto'\n",
        "\n",
        "def _latest_run(root: Path):\n",
        "    if not root.exists():\n",
        "        return None\n",
        "    cands = []\n",
        "    for d in root.iterdir():\n",
        "        if d.is_dir() and d.name.startswith('run') and d.name[3:].isdigit() and (d / 'run_state.json').exists():\n",
        "            cands.append((int(d.name[3:]), d))\n",
        "    if not cands:\n",
        "        return None\n",
        "    cands.sort(key=lambda x: x[0])\n",
        "    return cands[-1][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f72d53e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_DIR is None:\n",
        "    root = CODEC_RUNS_ROOT if RUN_KIND in ('codec', 'auto') else MEL_RUNS_ROOT\n",
        "    RUN_DIR = _latest_run(root)\n",
        "\n",
        "if RUN_DIR is None or not RUN_DIR.exists():\n",
        "    raise FileNotFoundError(f'RUN_DIR not found: {RUN_DIR}')\n",
        "\n",
        "run_state_path = RUN_DIR / 'run_state.json'\n",
        "if not run_state_path.exists():\n",
        "    raise FileNotFoundError(run_state_path)\n",
        "\n",
        "run_state = json.loads(run_state_path.read_text(encoding='utf-8'))\n",
        "cfg = run_state.get('config', {}) if isinstance(run_state, dict) else {}\n",
        "\n",
        "if RUN_KIND == 'auto':\n",
        "    if (RUN_DIR / 'cache' / 'codec_cache_arrays.npz').exists() and (RUN_DIR / 'checkpoints' / 'stage3_latest.pt').exists():\n",
        "        run_kind = 'codec'\n",
        "    else:\n",
        "        run_kind = 'mel'\n",
        "else:\n",
        "    run_kind = RUN_KIND\n",
        "\n",
        "print('RUN_DIR:', RUN_DIR)\n",
        "print('run_kind:', run_kind)\n",
        "print('run_name:', run_state.get('run_name', RUN_DIR.name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04fc46ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(int(SEED))\n",
        "if DEVICE == 'auto':\n",
        "    device_t = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "else:\n",
        "    device_t = DEVICE\n",
        "\n",
        "samples_dir = RUN_DIR / 'samples' / OUTPUT_TAG\n",
        "samples_dir.mkdir(parents=True, exist_ok=True)\n",
        "summary_csv = samples_dir / 'generation_summary.csv'\n",
        "\n",
        "if run_kind != 'codec':\n",
        "    raise RuntimeError('This notebook is now intended for codec runs. Set RUN_KIND=codec or auto with codec run dir.')\n",
        "\n",
        "# Load codec cache\n",
        "cache_dir = RUN_DIR / 'cache'\n",
        "idx_df, arrays, genre_to_idx, cache_meta = load_codec_cache(cache_dir)\n",
        "n_genres = len(genre_to_idx)\n",
        "idx_to_genre = {int(v): str(k) for k, v in genre_to_idx.items()}\n",
        "\n",
        "# Build train/val split for sampling\n",
        "if 'track_id' in idx_df.columns and bool(cfg.get('split_by_track', True)):\n",
        "    train_idx, val_idx = stratified_group_split_indices(\n",
        "        arrays['genre_idx'],\n",
        "        idx_df['track_id'].astype(str).to_numpy(),\n",
        "        val_ratio=float(cfg.get('val_ratio', VAL_RATIO)),\n",
        "        seed=int(cfg.get('seed', SEED)),\n",
        "    )\n",
        "else:\n",
        "    train_idx, val_idx = stratified_split_indices(\n",
        "        arrays['genre_idx'],\n",
        "        val_ratio=float(cfg.get('val_ratio', VAL_RATIO)),\n",
        "        seed=int(cfg.get('seed', SEED)),\n",
        "    )\n",
        "\n",
        "style_centroid_bank = build_style_centroid_bank(arrays['z_style'], arrays['genre_idx'], n_genres=n_genres).to(device_t)\n",
        "style_exemplar_bank = build_style_exemplar_bank(arrays['z_style'][train_idx], arrays['genre_idx'][train_idx], n_genres=n_genres)\n",
        "\n",
        "# Load codec model + checkpoint\n",
        "codec = FrozenEncodec(\n",
        "    model_id=str(cfg.get('codec_model_id', 'facebook/encodec_24khz')),\n",
        "    bandwidth=float(cfg.get('codec_bandwidth', 6.0)),\n",
        "    chunk_seconds=float(cfg.get('codec_chunk_seconds', 5.0)),\n",
        "    device=device_t,\n",
        ")\n",
        "\n",
        "G = CodecLatentTranslator(\n",
        "    in_channels=int(arrays['q_emb'].shape[1]),\n",
        "    z_content_dim=int(arrays['z_content'].shape[1]),\n",
        "    z_style_dim=int(arrays['z_style'].shape[1]),\n",
        "    hidden_channels=int(cfg.get('translator_hidden_channels', 256)),\n",
        "    n_blocks=int(cfg.get('translator_blocks', 10)),\n",
        "    noise_dim=int(cfg.get('translator_noise_dim', 32)),\n",
        "    residual_scale=float(cfg.get('translator_residual_scale', 0.5)),\n",
        ").to(device_t)\n",
        "\n",
        "ckpt_candidates = [RUN_DIR / 'checkpoints' / 'stage3_latest.pt', RUN_DIR / 'checkpoints' / 'stage2_latest.pt', RUN_DIR / 'checkpoints' / 'stage1_latest.pt']\n",
        "ckpt_path = next((p for p in ckpt_candidates if p.exists()), None)\n",
        "if ckpt_path is None:\n",
        "    raise FileNotFoundError('No stage checkpoint found in run checkpoints/')\n",
        "\n",
        "try:\n",
        "    payload = torch.load(str(ckpt_path), map_location='cpu', weights_only=False)\n",
        "except TypeError:\n",
        "    payload = torch.load(str(ckpt_path), map_location='cpu')\n",
        "G.load_state_dict(payload['generator'], strict=True)\n",
        "G.eval()\n",
        "\n",
        "print('Loaded checkpoint:', ckpt_path.name)\n",
        "print('cache rows:', len(idx_df), 'val rows:', len(val_idx), 'genres:', genre_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e558efc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _pick_target(src_g: int, n_genres: int, i: int) -> int:\n",
        "    # balanced random-like by cycling + anti-clash\n",
        "    tgt = int((i + src_g + 1) % n_genres)\n",
        "    if tgt == int(src_g):\n",
        "        tgt = int((tgt + 1) % n_genres)\n",
        "    return tgt\n",
        "\n",
        "def _z_tgt_for_genre(tgt_g: int):\n",
        "    z_cent = style_centroid_bank[tgt_g:tgt_g+1].to(device_t).float()\n",
        "    ex_bank = style_exemplar_bank.get(int(tgt_g))\n",
        "    if ex_bank is None or int(ex_bank.shape[0]) == 0:\n",
        "        z_ex = z_cent\n",
        "    else:\n",
        "        j = int(rng.integers(0, int(ex_bank.shape[0])))\n",
        "        z_ex = ex_bank[j:j+1].to(device_t).float()\n",
        "    if CODEC_COND_MODE == 'centroid':\n",
        "        z = z_cent\n",
        "    elif CODEC_COND_MODE == 'exemplar':\n",
        "        z = z_ex\n",
        "    else:\n",
        "        z = float(CODEC_COND_ALPHA) * z_cent + (1.0 - float(CODEC_COND_ALPHA)) * z_ex\n",
        "    if float(CODEC_STYLE_JITTER_STD) > 0.0:\n",
        "        z = z + torch.randn_like(z) * float(CODEC_STYLE_JITTER_STD)\n",
        "    return torch.nn.functional.normalize(z, dim=-1)\n",
        "\n",
        "if (GENERATE_NEW_SAMPLES and (FORCE_REGENERATE or (not summary_csv.exists()))):\n",
        "    rows = []\n",
        "    for i in range(int(N_GENERATION_SAMPLES)):\n",
        "        ridx = int(val_idx[int(rng.integers(0, len(val_idx)))])\n",
        "        src_g = int(arrays['genre_idx'][ridx])\n",
        "        tgt_g = _pick_target(src_g, n_genres=n_genres, i=i)\n",
        "\n",
        "        q_src = torch.from_numpy(arrays['q_emb'][ridx:ridx+1]).to(device_t).float()\n",
        "        zc = torch.from_numpy(arrays['z_content'][ridx:ridx+1]).to(device_t).float()\n",
        "        z_tgt = _z_tgt_for_genre(tgt_g)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_hat = G(q_src=q_src, z_content=zc, z_style_tgt=z_tgt)\n",
        "            wav = codec.decode_embeddings(q_hat)[0, 0].detach().cpu().numpy().astype(np.float32)\n",
        "        wav = wav / (np.max(np.abs(wav)) + 1e-8)\n",
        "\n",
        "        out_wav = samples_dir / f'sample_{i:04d}_src{src_g}_tgt{tgt_g}.wav'\n",
        "        sf.write(str(out_wav), wav, int(codec.cfg.sample_rate))\n",
        "\n",
        "        rows.append({\n",
        "            'sample_id': int(i),\n",
        "            'cache_row': int(ridx),\n",
        "            'source_genre_idx': int(src_g),\n",
        "            'target_genre_idx': int(tgt_g),\n",
        "            'source_genre': idx_to_genre.get(int(src_g), str(src_g)),\n",
        "            'target_genre': idx_to_genre.get(int(tgt_g), str(tgt_g)),\n",
        "            'fake_wav': str(out_wav),\n",
        "        })\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(summary_csv, index=False)\n",
        "    print('Generated samples ->', summary_csv)\n",
        "else:\n",
        "    print('Using existing sample summary:', summary_csv)\n",
        "\n",
        "if not summary_csv.exists():\n",
        "    raise FileNotFoundError(f'Missing generation summary: {summary_csv}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99613d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(summary_csv)\n",
        "if 'source_genre' not in df.columns and 'source_genre_idx' in df.columns:\n",
        "    df['source_genre'] = df['source_genre_idx'].map(lambda x: idx_to_genre.get(int(x), str(x)))\n",
        "if 'target_genre' not in df.columns and 'target_genre_idx' in df.columns:\n",
        "    df['target_genre'] = df['target_genre_idx'].map(lambda x: idx_to_genre.get(int(x), str(x)))\n",
        "\n",
        "pair_dir = samples_dir / 'source_pairs'\n",
        "pair_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "chunk_seconds = float(cfg.get('codec_chunk_seconds', 5.0))\n",
        "sr = int(codec.cfg.sample_rate)\n",
        "\n",
        "def _resolve_audio_path(raw_path):\n",
        "    if raw_path is None:\n",
        "        return None\n",
        "    p0 = Path(str(raw_path))\n",
        "    candidates = []\n",
        "    if p0.is_absolute():\n",
        "        candidates.append(p0)\n",
        "    else:\n",
        "        candidates.append(p0)\n",
        "        candidates.append(RUN_DIR / p0)\n",
        "        candidates.append(REPO_ROOT / p0)\n",
        "        candidates.append(samples_dir / p0.name)\n",
        "\n",
        "    # Handle slash/backslash mismatches from CSV logging.\n",
        "    s = str(raw_path)\n",
        "    s_alt = s.replace('\\\\', '/')\n",
        "    p1 = Path(s_alt)\n",
        "    if p1 not in candidates:\n",
        "        candidates.append(p1)\n",
        "        if not p1.is_absolute():\n",
        "            candidates.append(RUN_DIR / p1)\n",
        "            candidates.append(REPO_ROOT / p1)\n",
        "\n",
        "    seen = set()\n",
        "    for c in candidates:\n",
        "        key = str(c)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        if c.exists():\n",
        "            return c.resolve()\n",
        "    return None\n",
        "\n",
        "src_wavs = []\n",
        "fake_wavs_resolved = []\n",
        "wave_cos = []\n",
        "mfcc_cos = []\n",
        "missing_fake = 0\n",
        "\n",
        "for _, r in df.iterrows():\n",
        "    ridx = int(r['cache_row'])\n",
        "    fake_path = _resolve_audio_path(r.get('fake_wav', ''))\n",
        "    meta = idx_df.iloc[ridx]\n",
        "    src_path = Path(str(meta['path']))\n",
        "    start_sec = float(meta.get('start_sec', 0.0))\n",
        "\n",
        "    y_src, _ = librosa.load(str(src_path), sr=sr, mono=True, offset=max(0.0, start_sec), duration=max(0.2, chunk_seconds))\n",
        "\n",
        "    if fake_path is None:\n",
        "        missing_fake += 1\n",
        "        src_wavs.append('')\n",
        "        fake_wavs_resolved.append('')\n",
        "        wave_cos.append(np.nan)\n",
        "        mfcc_cos.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    y_fake, _ = librosa.load(str(fake_path), sr=sr, mono=True)\n",
        "    n = min(len(y_src), len(y_fake))\n",
        "    if n <= 1:\n",
        "        src_wavs.append('')\n",
        "        fake_wavs_resolved.append(str(fake_path))\n",
        "        wave_cos.append(np.nan)\n",
        "        mfcc_cos.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    y_src = y_src[:n].astype(np.float32)\n",
        "    y_fake = y_fake[:n].astype(np.float32)\n",
        "\n",
        "    src_out = pair_dir / f\"sample_{int(r['sample_id']):04d}_source.wav\"\n",
        "    if EXPORT_SOURCE_PAIRS:\n",
        "        sf.write(str(src_out), y_src, sr)\n",
        "        src_wavs.append(str(src_out))\n",
        "    else:\n",
        "        src_wavs.append('')\n",
        "\n",
        "    fake_wavs_resolved.append(str(fake_path))\n",
        "\n",
        "    c = float(np.dot(y_src, y_fake) / ((np.linalg.norm(y_src) * np.linalg.norm(y_fake)) + 1e-12))\n",
        "    wave_cos.append(c)\n",
        "\n",
        "    m1 = librosa.feature.mfcc(y=y_src, sr=sr, n_mfcc=13).mean(axis=1)\n",
        "    m2 = librosa.feature.mfcc(y=y_fake, sr=sr, n_mfcc=13).mean(axis=1)\n",
        "    mcos = float(np.dot(m1, m2) / ((np.linalg.norm(m1) * np.linalg.norm(m2)) + 1e-12))\n",
        "    mfcc_cos.append(mcos)\n",
        "\n",
        "df['source_wav'] = src_wavs\n",
        "df['fake_wav_resolved'] = fake_wavs_resolved\n",
        "df['wave_cosine_src_fake'] = wave_cos\n",
        "df['mfcc_cosine_src_fake'] = mfcc_cos\n",
        "\n",
        "compare_csv = samples_dir / 'generation_compare_with_source.csv'\n",
        "df.to_csv(compare_csv, index=False)\n",
        "print('compare csv:', compare_csv)\n",
        "print('rows:', len(df), '| missing fake files:', int(missing_fake))\n",
        "display(df[['sample_id','source_genre','target_genre','wave_cosine_src_fake','mfcc_cosine_src_fake']].head(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b4625c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Mean wave cosine (src vs fake):', float(df['wave_cosine_src_fake'].mean()))\n",
        "print('Mean MFCC cosine (src vs fake):', float(df['mfcc_cosine_src_fake'].mean()))\n",
        "\n",
        "display(df.groupby(['source_genre','target_genre']).size().rename('count').reset_index())\n",
        "display(df.groupby('target_genre')[['wave_cosine_src_fake','mfcc_cosine_src_fake']].mean().reset_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06787ac0",
      "metadata": {},
      "outputs": [],
      "source": [
        "view_df = df.copy()\n",
        "if PREVIEW_RANDOM:\n",
        "    view_df = view_df.sample(min(int(PREVIEW_N), len(view_df)), random_state=int(SEED)).reset_index(drop=True)\n",
        "else:\n",
        "    view_df = view_df.head(min(int(PREVIEW_N), len(view_df))).reset_index(drop=True)\n",
        "\n",
        "for i, r in view_df.iterrows():\n",
        "    sid = int(r['sample_id'])\n",
        "    print(f\"[{i}] sample={sid} | {r['source_genre']} -> {r['target_genre']} | wave_cos={r['wave_cosine_src_fake']:.3f} | mfcc_cos={r['mfcc_cosine_src_fake']:.3f}\")\n",
        "    src = Path(r['source_wav']) if isinstance(r.get('source_wav',''), str) and len(str(r.get('source_wav',''))) else None\n",
        "    fake_col = r.get('fake_wav_resolved', r.get('fake_wav', ''))\n",
        "    fake = Path(fake_col) if isinstance(fake_col, str) and len(str(fake_col)) else None\n",
        "\n",
        "    if src is not None and src.exists():\n",
        "        print('Source chunk:')\n",
        "        display(Audio(filename=str(src)))\n",
        "    if fake is not None and fake.exists():\n",
        "        print('Generated:')\n",
        "        display(Audio(filename=str(fake)))\n",
        "    else:\n",
        "        print('Generated: [missing file path]')\n",
        "    print('-' * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}