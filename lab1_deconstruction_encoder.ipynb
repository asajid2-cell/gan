{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68218648",
   "metadata": {},
   "source": [
    "# Lab 1 - Deconstruction Encoder (Disentangled Content/Style)\n",
    "\n",
    "This notebook is the complete Lab 1 implementation and audit trail for the **Deconstruction Encoder** in the Deep Generative Genre Remastering pipeline.\n",
    "\n",
    "Lab 1 objective:\n",
    "- Learn a stable content code (`z_content`) that preserves musical identity while removing source-style artifacts.\n",
    "- Learn a style code (`z_style`) that remains informative for source/style discrimination.\n",
    "- Learn a robust music-vs-speech gate that can protect downstream reconstruction from non-musical noise.\n",
    "\n",
    "Primary exit criteria used in this notebook:\n",
    "- Content leakage <= `0.15`\n",
    "- Style accuracy >= `0.85`\n",
    "- Gate AUC >= `0.90`\n",
    "- Invariance cosine >= `0.92` under dual-soundfont render\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54056b11",
   "metadata": {},
   "source": [
    "## Lab 1 Roadmap\n",
    "\n",
    "This notebook is organized as a staged research workflow:\n",
    "\n",
    "1. **Data + manifests sanity**: load all cleaned manifests and verify corpus coverage.\n",
    "2. **Feature + chunk pipeline**: extract log-mel/chroma/tonnetz/tempogram and materialize chunk-level training rows.\n",
    "3. **Curriculum training**: train encoder with phase-specific objectives (content/style/gate) and checkpointing.\n",
    "4. **Disentanglement audits**: invariance, leakage probe, and gate scaling evaluation.\n",
    "5. **Fail-fast preflight + micro-train sharpening**: fast iterative loop to avoid long failed runs.\n",
    "6. **Final result reporting**: summarize Lab 1 pass/fail against target scientific criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ab6a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa: ok\n",
      "soundfile: ok\n",
      "torch: ok\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import sqlite3\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "except ImportError:\n",
    "    librosa = None\n",
    "\n",
    "try:\n",
    "    import soundfile as sf\n",
    "except ImportError:\n",
    "    sf = None\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "except ImportError:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    F = None\n",
    "    Dataset = object\n",
    "    DataLoader = object\n",
    "\n",
    "SEED = 328\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch is not None:\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"librosa: {'ok' if librosa is not None else 'missing'}\")\n",
    "print(f\"soundfile: {'ok' if sf is not None else 'missing'}\")\n",
    "print(f\"torch: {'ok' if torch is not None else 'missing'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49f686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest root: Z:\\DataSets\\_lab1_manifests (exists)\n",
      "cc0_music          -> Z:\\DataSets\\CC0-1.0-Music (exists)\n",
      "xtc_hiphop         -> Z:\\DataSets\\XTc Files of Hip Hop (exists)\n",
      "hh_lfbb            -> Z:\\DataSets\\hh_lfbb (exists)\n",
      "fsd50k             -> Z:\\DataSets\\fsd50k (exists)\n",
      "libirspeech        -> Z:\\DataSets\\libirspeech (exists)\n",
      "the_session        -> Z:\\DataSets\\TheSession-data (exists)\n",
      "lmd_root           -> Z:\\DataSets\\lmd (exists)\n",
      "pdmx_extracted     -> Z:\\DataSets\\lmd\\PDMX_extracted (exists)\n",
      "phase1_render_root -> Z:\\DataSets\\rendered\\phase1_symbolic_audio (exists)\n",
      "\n",
      "manifest:cc0                      -> Z:\\DataSets\\_lab1_manifests\\cc0_audio_clean.csv (exists)\n",
      "manifest:xtc                      -> Z:\\DataSets\\_lab1_manifests\\xtc_audio_clean.csv (exists)\n",
      "manifest:hh_lfbb                  -> Z:\\DataSets\\_lab1_manifests\\hh_lfbb_audio_clean.csv (exists)\n",
      "manifest:fsd50k                   -> Z:\\DataSets\\_lab1_manifests\\fsd50k_audio_clean.csv (exists)\n",
      "manifest:libirspeech              -> Z:\\DataSets\\_lab1_manifests\\libirspeech_audio_clean.csv (exists)\n",
      "manifest:lmd_midi                 -> Z:\\DataSets\\_lab1_manifests\\lmd_midi_manifest.csv (exists)\n",
      "manifest:pdmx_no_license_conflict -> Z:\\DataSets\\_lab1_manifests\\pdmx_no_license_conflict_manifest.csv (exists)\n",
      "manifest:the_session_paths        -> Z:\\DataSets\\_lab1_manifests\\the_session_paths.json (exists)\n",
      "manifest:phase1_audio             -> Z:\\DataSets\\_lab1_manifests\\phase1_symbolic_audio_manifest.csv (exists)\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(r\"Z:\\DataSets\")\n",
    "MANIFEST_ROOT = DATA_ROOT / \"_lab1_manifests\"\n",
    "\n",
    "PATHS = {\n",
    "    # audio corpora\n",
    "    \"cc0_music\": DATA_ROOT / \"CC0-1.0-Music\",\n",
    "    \"xtc_hiphop\": DATA_ROOT / \"XTc Files of Hip Hop\",\n",
    "    \"hh_lfbb\": DATA_ROOT / \"hh_lfbb\",\n",
    "    \"fsd50k\": DATA_ROOT / \"fsd50k\",\n",
    "    \"libirspeech\": DATA_ROOT / \"libirspeech\",\n",
    "\n",
    "    # symbolic / metadata corpora\n",
    "    \"the_session\": DATA_ROOT / \"TheSession-data\",\n",
    "    \"lmd_root\": DATA_ROOT / \"lmd\",\n",
    "    \"pdmx_extracted\": DATA_ROOT / \"lmd\" / \"PDMX_extracted\",\n",
    "\n",
    "    # generated phase-1 audio target location\n",
    "    \"phase1_render_root\": DATA_ROOT / \"rendered\" / \"phase1_symbolic_audio\",\n",
    "}\n",
    "\n",
    "MANIFEST_FILES = {\n",
    "    \"cc0\": MANIFEST_ROOT / \"cc0_audio_clean.csv\",\n",
    "    \"xtc\": MANIFEST_ROOT / \"xtc_audio_clean.csv\",\n",
    "    \"hh_lfbb\": MANIFEST_ROOT / \"hh_lfbb_audio_clean.csv\",\n",
    "    \"fsd50k\": MANIFEST_ROOT / \"fsd50k_audio_clean.csv\",\n",
    "    \"libirspeech\": MANIFEST_ROOT / \"libirspeech_audio_clean.csv\",\n",
    "    \"lmd_midi\": MANIFEST_ROOT / \"lmd_midi_manifest.csv\",\n",
    "    \"pdmx_no_license_conflict\": MANIFEST_ROOT / \"pdmx_no_license_conflict_manifest.csv\",\n",
    "    \"the_session_paths\": MANIFEST_ROOT / \"the_session_paths.json\",\n",
    "    # to be generated after symbolic rendering\n",
    "    \"phase1_audio\": MANIFEST_ROOT / \"phase1_symbolic_audio_manifest.csv\",\n",
    "}\n",
    "\n",
    "print(f\"Manifest root: {MANIFEST_ROOT} ({'exists' if MANIFEST_ROOT.exists() else 'missing'})\")\n",
    "for k, p in PATHS.items():\n",
    "    print(f\"{k:18} -> {p} ({'exists' if p.exists() else 'missing'})\")\n",
    "print()\n",
    "for k, p in MANIFEST_FILES.items():\n",
    "    print(f\"manifest:{k:24} -> {p} ({'exists' if p.exists() else 'missing'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670a59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_EXTS = {\".wav\", \".mp3\", \".flac\", \".ogg\", \".oga\", \".m4a\", \".aiff\", \".aif\"}\n",
    "SYMBOLIC_EXTS = {\".mid\", \".midi\", \".mxl\", \".musicxml\", \".xml\", \".abc\"}\n",
    "\n",
    "\n",
    "def collect_audio_files(root: Path, source: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    if not root.exists():\n",
    "        return pd.DataFrame(columns=[\"source\", \"path\", \"ext\", \"size_bytes\"])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in AUDIO_EXTS:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"source\": source,\n",
    "                    \"path\": str(p),\n",
    "                    \"ext\": p.suffix.lower(),\n",
    "                    \"size_bytes\": p.stat().st_size,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def collect_symbolic_files(root: Path, source: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    if not root.exists():\n",
    "        return pd.DataFrame(columns=[\"source\", \"path\", \"ext\", \"size_bytes\"])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in SYMBOLIC_EXTS:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"source\": source,\n",
    "                    \"path\": str(p),\n",
    "                    \"ext\": p.suffix.lower(),\n",
    "                    \"size_bytes\": p.stat().st_size,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def to_gb(num_bytes: int) -> float:\n",
    "    return round(num_bytes / (1024 ** 3), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f09b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total manifest-backed audio files: 68,035\n",
      "Phase 2 (music): 14,135 | Phase 3 (negative): 53,900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>files</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>size_gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fsd50k</td>\n",
       "      <td>51197</td>\n",
       "      <td>34484018560</td>\n",
       "      <td>32.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc0_music</td>\n",
       "      <td>9156</td>\n",
       "      <td>84394043936</td>\n",
       "      <td>78.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hh_lfbb</td>\n",
       "      <td>3332</td>\n",
       "      <td>18505479728</td>\n",
       "      <td>17.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>libirspeech</td>\n",
       "      <td>2703</td>\n",
       "      <td>359034309</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xtc_hiphop</td>\n",
       "      <td>1647</td>\n",
       "      <td>511291740</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  files   size_bytes  size_gb\n",
       "1       fsd50k  51197  34484018560    32.12\n",
       "0    cc0_music   9156  84394043936    78.60\n",
       "2      hh_lfbb   3332  18505479728    17.23\n",
       "3  libirspeech   2703    359034309     0.33\n",
       "4   xtc_hiphop   1647    511291740     0.48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audio manifests are pre-cleaned and deterministic (no folder crawling in training path)\n",
    "\n",
    "def read_audio_manifest(path: Path, source_name: str) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"source\", \"path\", \"ext\", \"size_bytes\"])\n",
    "    df = pd.read_csv(path)\n",
    "    keep = [c for c in [\"source\", \"path\", \"ext\", \"size_bytes\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    if \"source\" not in df.columns:\n",
    "        df[\"source\"] = source_name\n",
    "    df[\"source\"] = source_name\n",
    "    df = df[df[\"path\"].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "audio_manifests = {\n",
    "    \"cc0_music\": read_audio_manifest(MANIFEST_FILES[\"cc0\"], \"cc0_music\"),\n",
    "    \"xtc_hiphop\": read_audio_manifest(MANIFEST_FILES[\"xtc\"], \"xtc_hiphop\"),\n",
    "    \"hh_lfbb\": read_audio_manifest(MANIFEST_FILES[\"hh_lfbb\"], \"hh_lfbb\"),\n",
    "    \"fsd50k\": read_audio_manifest(MANIFEST_FILES[\"fsd50k\"], \"fsd50k\"),\n",
    "    \"libirspeech\": read_audio_manifest(MANIFEST_FILES[\"libirspeech\"], \"libirspeech\"),\n",
    "}\n",
    "\n",
    "audio_manifest = pd.concat(audio_manifests.values(), ignore_index=True)\n",
    "audio_manifest = audio_manifest.drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
    "\n",
    "phase2_manifest = pd.concat(\n",
    "    [audio_manifests[\"cc0_music\"], audio_manifests[\"xtc_hiphop\"], audio_manifests[\"hh_lfbb\"]],\n",
    "    ignore_index=True,\n",
    ")\n",
    "phase2_manifest[\"is_music\"] = 1\n",
    "\n",
    "phase3_manifest = pd.concat(\n",
    "    [audio_manifests[\"libirspeech\"], audio_manifests[\"fsd50k\"]],\n",
    "    ignore_index=True,\n",
    ")\n",
    "phase3_manifest[\"is_music\"] = 0\n",
    "\n",
    "summary = (\n",
    "    audio_manifest.groupby(\"source\", as_index=False)\n",
    "    .agg(files=(\"path\", \"count\"), size_bytes=(\"size_bytes\", \"sum\"))\n",
    ")\n",
    "if len(summary) > 0:\n",
    "    summary[\"size_gb\"] = summary[\"size_bytes\"].map(to_gb)\n",
    "\n",
    "print(f\"Total manifest-backed audio files: {len(audio_manifest):,}\")\n",
    "print(f\"Phase 2 (music): {len(phase2_manifest):,} | Phase 3 (negative): {len(phase3_manifest):,}\")\n",
    "summary.sort_values(\"files\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c837c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total hours (sampled): 483.93\n",
      "Estimated total 8s chunks @4s stride: 400,499\n"
     ]
    }
   ],
   "source": [
    "# Quick duration + chunk-budget estimate for planning\n",
    "\n",
    "def estimate_total_hours(df: pd.DataFrame, sample_n: int = 300) -> float:\n",
    "    if sf is None or df.empty:\n",
    "        return float(\"nan\")\n",
    "    sample = df.sample(min(sample_n, len(df)), random_state=SEED)\n",
    "    durations = []\n",
    "    for p in sample[\"path\"]:\n",
    "        try:\n",
    "            info = sf.info(p)\n",
    "            durations.append(float(info.duration))\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not durations:\n",
    "        return float(\"nan\")\n",
    "    avg_dur = float(np.mean(durations))\n",
    "    total_sec = avg_dur * len(df)\n",
    "    return total_sec / 3600.0\n",
    "\n",
    "\n",
    "def estimate_chunk_count(df: pd.DataFrame, chunk_seconds: float = 8.0, stride_seconds: float = 4.0, sample_n: int = 300) -> float:\n",
    "    if sf is None or df.empty:\n",
    "        return float(\"nan\")\n",
    "    sample = df.sample(min(sample_n, len(df)), random_state=SEED)\n",
    "    chunk_counts = []\n",
    "    for p in sample[\"path\"]:\n",
    "        try:\n",
    "            dur = float(sf.info(p).duration)\n",
    "            if dur < chunk_seconds:\n",
    "                c = 1\n",
    "            else:\n",
    "                c = 1 + int((dur - chunk_seconds) // stride_seconds)\n",
    "            chunk_counts.append(c)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not chunk_counts:\n",
    "        return float(\"nan\")\n",
    "    avg_chunks = float(np.mean(chunk_counts))\n",
    "    return avg_chunks * len(df)\n",
    "\n",
    "\n",
    "est_hours = estimate_total_hours(audio_manifest)\n",
    "est_chunks = estimate_chunk_count(audio_manifest, chunk_seconds=8.0, stride_seconds=4.0)\n",
    "\n",
    "print(f\"Estimated total hours (sampled): {est_hours:.2f}\" if not np.isnan(est_hours) else \"Duration estimate unavailable.\")\n",
    "print(f\"Estimated total 8s chunks @4s stride: {est_chunks:,.0f}\" if not np.isnan(est_chunks) else \"Chunk estimate unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce2af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheSession records loaded: 53,765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tune_id</th>\n",
       "      <th>setting_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>meter</th>\n",
       "      <th>mode</th>\n",
       "      <th>abc</th>\n",
       "      <th>composer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15326</td>\n",
       "      <td>28560</td>\n",
       "      <td>'S Ann An Ìle</td>\n",
       "      <td>strathspey</td>\n",
       "      <td>4/4</td>\n",
       "      <td>Gmajor</td>\n",
       "      <td>|:G&gt;A B&gt;G c&gt;A B&gt;G|E&lt;E A&gt;G F&lt;D D2|G&gt;A B&gt;G c&gt;A B...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15326</td>\n",
       "      <td>28582</td>\n",
       "      <td>'S Ann An Ìle</td>\n",
       "      <td>strathspey</td>\n",
       "      <td>4/4</td>\n",
       "      <td>Gmajor</td>\n",
       "      <td>uD2|:{F}v[G,2G2]uB&gt;ud c&gt;A B&gt;G|{D}E2 uA&gt;uG F&lt;D ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14625</td>\n",
       "      <td>26955</td>\n",
       "      <td>'S Daor An Tabac</td>\n",
       "      <td>reel</td>\n",
       "      <td>4/4</td>\n",
       "      <td>Bminor</td>\n",
       "      <td>|:eAAB eABB|eAAB gedB|eAAB eABB|G2AB gedB:|\\r\\...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tune_id setting_id              name        type meter    mode  \\\n",
       "0   15326      28560     'S Ann An Ìle  strathspey   4/4  Gmajor   \n",
       "1   15326      28582     'S Ann An Ìle  strathspey   4/4  Gmajor   \n",
       "2   14625      26955  'S Daor An Tabac        reel   4/4  Bminor   \n",
       "\n",
       "                                                 abc composer  \n",
       "0  |:G>A B>G c>A B>G|E<E A>G F<D D2|G>A B>G c>A B...           \n",
       "1  uD2|:{F}v[G,2G2]uB>ud c>A B>G|{D}E2 uA>uG F<D ...           \n",
       "2  |:eAAB eABB|eAAB gedB|eAAB eABB|G2AB gedB:|\\r\\...           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheSession SQLite tables: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aliases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recordings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sessions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tune_popularity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tunes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name\n",
       "0          aliases\n",
       "1           events\n",
       "2       recordings\n",
       "3         sessions\n",
       "4             sets\n",
       "5  tune_popularity\n",
       "6            tunes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TheSession symbolic metadata (style-invariant melody/rhythm reference)\n",
    "tunes_json = PATHS[\"the_session\"] / \"json\" / \"tunes.json\"\n",
    "session_db = PATHS[\"the_session\"] / \"thesession.db\"\n",
    "\n",
    "the_session_df = pd.DataFrame()\n",
    "if tunes_json.exists():\n",
    "    with open(tunes_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    the_session_df = pd.DataFrame(data)\n",
    "    cols = [c for c in [\"tune_id\", \"setting_id\", \"name\", \"type\", \"meter\", \"mode\", \"abc\", \"composer\"] if c in the_session_df.columns]\n",
    "    the_session_df = the_session_df[cols]\n",
    "\n",
    "print(f\"TheSession records loaded: {len(the_session_df):,}\")\n",
    "if not the_session_df.empty:\n",
    "    display(the_session_df.head(3))\n",
    "\n",
    "if session_db.exists():\n",
    "    with sqlite3.connect(session_db) as conn:\n",
    "        tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", conn)\n",
    "    print(f\"TheSession SQLite tables: {len(tables)}\")\n",
    "    display(tables.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ed64c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDMX no_license_conflict rows: 222,820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existence_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exists_data_json_path</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exists_metadata_json_path</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exists_mxl_path</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exists_mid_path</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           existence_ratio\n",
       "exists_data_json_path                  1.0\n",
       "exists_metadata_json_path              1.0\n",
       "exists_mxl_path                        1.0\n",
       "exists_mid_path                        1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMD MIDI rows: 271,265\n",
      "TheSession manifest loaded: True\n",
      "{'db': 'Z:\\\\DataSets\\\\TheSession-data\\\\thesession.db', 'tunes_json': 'Z:\\\\DataSets\\\\TheSession-data\\\\json\\\\tunes.json', 'sets_json': 'Z:\\\\DataSets\\\\TheSession-data\\\\json\\\\sets.json', 'exists_db': True, 'exists_tunes_json': True, 'exists_sets_json': True}\n",
      "Phase 1 rendered audio manifest present: True\n"
     ]
    }
   ],
   "source": [
    "# Symbolic teacher manifests (Phase 1 source)\n",
    "\n",
    "pdmx_manifest = pd.DataFrame()\n",
    "if MANIFEST_FILES[\"pdmx_no_license_conflict\"].exists():\n",
    "    pdmx_manifest = pd.read_csv(MANIFEST_FILES[\"pdmx_no_license_conflict\"])\n",
    "\n",
    "lmd_midi_manifest = pd.DataFrame()\n",
    "if MANIFEST_FILES[\"lmd_midi\"].exists():\n",
    "    lmd_midi_manifest = pd.read_csv(MANIFEST_FILES[\"lmd_midi\"])\n",
    "\n",
    "the_session_paths = {}\n",
    "if MANIFEST_FILES[\"the_session_paths\"].exists():\n",
    "    with open(MANIFEST_FILES[\"the_session_paths\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        the_session_paths = json.load(f)\n",
    "\n",
    "print(f\"PDMX no_license_conflict rows: {len(pdmx_manifest):,}\")\n",
    "if len(pdmx_manifest):\n",
    "    exists_cols = [c for c in pdmx_manifest.columns if c.startswith(\"exists_\")]\n",
    "    if exists_cols:\n",
    "        display(pdmx_manifest[exists_cols].mean().rename(\"existence_ratio\").to_frame())\n",
    "\n",
    "print(f\"LMD MIDI rows: {len(lmd_midi_manifest):,}\")\n",
    "print(\"TheSession manifest loaded:\", bool(the_session_paths))\n",
    "if the_session_paths:\n",
    "    print(the_session_paths)\n",
    "\n",
    "phase1_audio_ready = MANIFEST_FILES[\"phase1_audio\"].exists()\n",
    "print(f\"Phase 1 rendered audio manifest present: {phase1_audio_ready}\")\n",
    "if not phase1_audio_ready:\n",
    "    print(\"Phase 1 training audio is not materialized yet. Render PDMX/TheSession to WAV, then write phase1_symbolic_audio_manifest.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173c18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 1 feature extraction: content-focused representations\n",
    "\n",
    "\n",
    "def load_audio_mono_48k(path: str, sample_rate: int = 22050, max_seconds: float | None = 12.0, start_sec: float = 0.0):\n",
    "    if librosa is None:\n",
    "        raise ImportError(\"librosa is required for feature extraction\")\n",
    "\n",
    "    y, sr = librosa.load(\n",
    "        path,\n",
    "        sr=sample_rate,\n",
    "        mono=True,\n",
    "        offset=max(0.0, float(start_sec)),\n",
    "        duration=None if max_seconds is None else float(max_seconds),\n",
    "        dtype=np.float32,\n",
    "        res_type=\"soxr_hq\",\n",
    "    )\n",
    "\n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f\"Empty audio: {path}\")\n",
    "\n",
    "    y = librosa.util.normalize(y)\n",
    "    return y, sr\n",
    "\n",
    "\n",
    "def extract_lab1_features(y: np.ndarray, sr: int = 22050, n_fft: int = 1024, hop: int = 256) -> Dict[str, np.ndarray]:\n",
    "    # Full feature set (more expensive)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop, n_mels=96)\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop)\n",
    "    harmonic = librosa.effects.harmonic(y)\n",
    "    tonnetz = librosa.feature.tonnetz(y=harmonic, sr=sr)\n",
    "\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop)\n",
    "\n",
    "    return {\n",
    "        \"log_mel\": log_mel.astype(np.float32),\n",
    "        \"chroma\": chroma.astype(np.float32),\n",
    "        \"tonnetz\": tonnetz.astype(np.float32),\n",
    "        \"tempogram\": tempogram.astype(np.float32),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_lab1_features_light(y: np.ndarray, sr: int = 22050, n_fft: int = 1024, hop: int = 256) -> Dict[str, np.ndarray]:\n",
    "    # Lightweight path for smoke checks and low-memory runs\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop, n_mels=96)\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return {\"log_mel\": log_mel.astype(np.float32)}\n",
    "\n",
    "\n",
    "def extract_from_path(\n",
    "    path: str,\n",
    "    sample_rate: int = 22050,\n",
    "    max_seconds: float | None = 12.0,\n",
    "    lightweight: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    y, sr = load_audio_mono_48k(path, sample_rate=sample_rate, max_seconds=max_seconds)\n",
    "\n",
    "    if lightweight:\n",
    "        return extract_lab1_features_light(y, sr=sr)\n",
    "\n",
    "    try:\n",
    "        return extract_lab1_features(y, sr=sr)\n",
    "    except MemoryError:\n",
    "        # fallback so notebook execution does not crash during inspection\n",
    "        return extract_lab1_features_light(y, sr=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58769281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lloyd Rodgers - One Questions of Discipline and the Naivete of Flowers (Act I).mp3: {'log_mel': (96, 517)}\n",
      "Ming Hang - transient affection.mp3.mp3: {'log_mel': (96, 517)}\n",
      "108569.wav: {'log_mel': (96, 48)}\n",
      "328413.wav: {'log_mel': (96, 517)}\n",
      "72bpm_hh_lfbb_mid_001_04.wav: {'log_mel': (96, 517)}\n",
      "89bpm_hh_lfbb_mid_009_09.wav: {'log_mel': (96, 517)}\n",
      "6319-275224-0019.flac: {'log_mel': (96, 498)}\n",
      "6313-66129-0021.flac: {'log_mel': (96, 517)}\n"
     ]
    }
   ],
   "source": [
    "# Smoke test feature extraction on a few files from each available source\n",
    "if librosa is None:\n",
    "    print(\"Install librosa to run this cell.\")\n",
    "else:\n",
    "    test_rows = []\n",
    "    for src, group in audio_manifest.groupby(\"source\"):\n",
    "        test_rows.extend(group.sample(min(2, len(group)), random_state=SEED)[\"path\"].tolist())\n",
    "\n",
    "    for p in test_rows[:8]:\n",
    "        feats = extract_from_path(p, sample_rate=22050, max_seconds=6.0, lightweight=True)\n",
    "        shapes = {k: tuple(v.shape) for k, v in feats.items()}\n",
    "        print(f\"{Path(p).name}: {shapes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3254c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk index + dataset (implementation track)\n",
    "\n",
    "def load_audio_chunk_48k(path: str, start_sec: float, duration_sec: float, sample_rate: int = 48000) -> np.ndarray:\n",
    "    if librosa is None:\n",
    "        raise ImportError(\"librosa is required for chunk loading\")\n",
    "\n",
    "    y, _ = librosa.load(\n",
    "        path,\n",
    "        sr=sample_rate,\n",
    "        mono=True,\n",
    "        offset=max(0.0, float(start_sec)),\n",
    "        duration=float(duration_sec),\n",
    "        dtype=np.float32,\n",
    "        res_type=\"soxr_hq\",\n",
    "    )\n",
    "    target_len = int(duration_sec * sample_rate)\n",
    "\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)), mode=\"constant\")\n",
    "    elif len(y) > target_len:\n",
    "        y = y[:target_len]\n",
    "\n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f\"Empty chunk loaded from {path}\")\n",
    "\n",
    "    y = librosa.util.normalize(y)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_chunk_index(\n",
    "    manifest_df: pd.DataFrame,\n",
    "    chunk_seconds: float = 8.0,\n",
    "    stride_seconds: float = 4.0,\n",
    "    max_files_per_source: int = 300,\n",
    "    max_chunks_per_file: int = 8,\n",
    ") -> pd.DataFrame:\n",
    "    if sf is None:\n",
    "        raise ImportError(\"soundfile is required to build chunk index\")\n",
    "\n",
    "    rows = []\n",
    "    for src, g in manifest_df.groupby(\"source\"):\n",
    "        g = g.sample(min(max_files_per_source, len(g)), random_state=SEED).reset_index(drop=True)\n",
    "        for _, row in g.iterrows():\n",
    "            path = row[\"path\"]\n",
    "            try:\n",
    "                dur = float(sf.info(path).duration)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if dur <= 0.1:\n",
    "                continue\n",
    "\n",
    "            if dur <= chunk_seconds:\n",
    "                starts = [0.0]\n",
    "            else:\n",
    "                n = 1 + int((dur - chunk_seconds) // stride_seconds)\n",
    "                n = min(n, max_chunks_per_file)\n",
    "                starts = [i * stride_seconds for i in range(n)]\n",
    "\n",
    "            for s in starts:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"source\": src,\n",
    "                        \"path\": path,\n",
    "                        \"start_sec\": float(s),\n",
    "                        \"duration_sec\": float(chunk_seconds),\n",
    "                        \"is_music\": int(row.get(\"is_music\", 1)),\n",
    "                        \"source_idx\": int(row.get(\"source_idx\", 0)),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def augment_wave(y: np.ndarray) -> np.ndarray:\n",
    "    # Small perturbations for content-invariance training\n",
    "    gain = np.random.uniform(0.8, 1.2)\n",
    "    noise = np.random.normal(0.0, 0.003, size=y.shape).astype(np.float32)\n",
    "    y_aug = (y * gain + noise).astype(np.float32)\n",
    "    y_aug = np.clip(y_aug, -1.0, 1.0)\n",
    "    return y_aug\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_log_mel_fast(y: np.ndarray, sr: int, n_fft: int = 1024, hop: int = 256, n_mels: int = 96) -> np.ndarray:\n",
    "    \"\"\"Lightweight training feature path: log-mel only (no CQT/tonnetz/tempogram).\"\"\"\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop,\n",
    "        n_mels=n_mels,\n",
    "        fmin=20,\n",
    "        fmax=sr // 2,\n",
    "        power=2.0,\n",
    "    )\n",
    "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return log_mel.astype(np.float32)\n",
    "\n",
    "\n",
    "class Lab1ChunkDataset(Dataset):\n",
    "    def __init__(self, chunk_df: pd.DataFrame, sample_rate: int = 48000):\n",
    "        self.df = chunk_df.reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        y = load_audio_chunk_48k(\n",
    "            path=row[\"path\"],\n",
    "            start_sec=row[\"start_sec\"],\n",
    "            duration_sec=row[\"duration_sec\"],\n",
    "            sample_rate=self.sample_rate,\n",
    "        )\n",
    "        y_aug = augment_wave(y)\n",
    "\n",
    "        try:\n",
    "            log_mel = extract_log_mel_fast(y, sr=self.sample_rate)\n",
    "            log_mel_aug = extract_log_mel_fast(y_aug, sr=self.sample_rate)\n",
    "        except MemoryError:\n",
    "            # Last-resort fallback: shorter FFT footprint\n",
    "            log_mel = extract_log_mel_fast(y, sr=self.sample_rate, n_fft=512, hop=128, n_mels=80)\n",
    "            log_mel_aug = extract_log_mel_fast(y_aug, sr=self.sample_rate, n_fft=512, hop=128, n_mels=80)\n",
    "\n",
    "        if torch is not None:\n",
    "            return {\n",
    "                \"log_mel\": torch.from_numpy(log_mel),\n",
    "                \"log_mel_aug\": torch.from_numpy(log_mel_aug),\n",
    "                \"source_idx\": torch.tensor(int(row[\"source_idx\"]), dtype=torch.long),\n",
    "                \"is_music\": torch.tensor(int(row[\"is_music\"]), dtype=torch.float32),\n",
    "                \"source\": row[\"source\"],\n",
    "                \"path\": row[\"path\"],\n",
    "                \"start_sec\": float(row[\"start_sec\"]),\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"log_mel\": log_mel,\n",
    "            \"log_mel_aug\": log_mel_aug,\n",
    "            \"source_idx\": int(row[\"source_idx\"]),\n",
    "            \"is_music\": float(row[\"is_music\"]),\n",
    "            \"source\": row[\"source\"],\n",
    "            \"path\": row[\"path\"],\n",
    "            \"start_sec\": float(row[\"start_sec\"]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4470f",
   "metadata": {},
   "source": [
    "### Trainer Architecture\n",
    "\n",
    "The trainer cell below implements the core Lab 1 model and optimization workflow:\n",
    "- Shared convolutional backbone over chunked log-mel.\n",
    "- Dual latent heads (`z_content`, `z_style`).\n",
    "- Style classifier on `z_style` and adversarial style probe on `z_content` (GRL).\n",
    "- Music gate head for speech-vs-music discrimination.\n",
    "- Phase-aware loss weighting and hard-negative curriculum.\n",
    "- Optional teacher-anchor regularization to prevent catastrophic forgetting during Phase-3 sharpening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39553a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived old run to: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f_archived_20260211_011128\n",
      "Run dir: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f\n",
      "Device: cuda\n",
      "Resume state: phase_idx=0 epoch=1 step=1 global_step=0\n",
      "Saved before-training examples: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f\\examples_before.csv (4 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>path</th>\n",
       "      <th>music_prob</th>\n",
       "      <th>style_pred_idx</th>\n",
       "      <th>style_pred_prob</th>\n",
       "      <th>top1_idx</th>\n",
       "      <th>top1_prob</th>\n",
       "      <th>top2_idx</th>\n",
       "      <th>top2_prob</th>\n",
       "      <th>top3_idx</th>\n",
       "      <th>top3_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc0_music</td>\n",
       "      <td>Lloyd Rodgers - One Questions of Discipline an...</td>\n",
       "      <td>Z:\\DataSets\\CC0-1.0-Music\\freemusicarchive.org...</td>\n",
       "      <td>0.380886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186144</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186144</td>\n",
       "      <td>5</td>\n",
       "      <td>0.171784</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hh_lfbb</td>\n",
       "      <td>72bpm_hh_lfbb_mid_001_04.wav</td>\n",
       "      <td>Z:\\DataSets\\hh_lfbb\\72bpm_hh_lfbb_mid_001_04.wav</td>\n",
       "      <td>0.324834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185225</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185225</td>\n",
       "      <td>5</td>\n",
       "      <td>0.172108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>libirspeech</td>\n",
       "      <td>6319-275224-0019.flac</td>\n",
       "      <td>Z:\\DataSets\\libirspeech\\LibriSpeech\\dev-clean\\...</td>\n",
       "      <td>0.361139</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185548</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185548</td>\n",
       "      <td>5</td>\n",
       "      <td>0.171884</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xtc_hiphop</td>\n",
       "      <td>FD1404_lop_098bpm.wav</td>\n",
       "      <td>Z:\\DataSets\\XTc Files of Hip Hop\\e-Lab - XTc F...</td>\n",
       "      <td>0.360715</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185436</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185436</td>\n",
       "      <td>5</td>\n",
       "      <td>0.172002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                               file  \\\n",
       "0    cc0_music  Lloyd Rodgers - One Questions of Discipline an...   \n",
       "1      hh_lfbb                       72bpm_hh_lfbb_mid_001_04.wav   \n",
       "2  libirspeech                              6319-275224-0019.flac   \n",
       "3   xtc_hiphop                              FD1404_lop_098bpm.wav   \n",
       "\n",
       "                                                path  music_prob  \\\n",
       "0  Z:\\DataSets\\CC0-1.0-Music\\freemusicarchive.org...    0.380886   \n",
       "1   Z:\\DataSets\\hh_lfbb\\72bpm_hh_lfbb_mid_001_04.wav    0.324834   \n",
       "2  Z:\\DataSets\\libirspeech\\LibriSpeech\\dev-clean\\...    0.361139   \n",
       "3  Z:\\DataSets\\XTc Files of Hip Hop\\e-Lab - XTc F...    0.360715   \n",
       "\n",
       "   style_pred_idx  style_pred_prob  top1_idx  top1_prob  top2_idx  top2_prob  \\\n",
       "0               1         0.186144         1   0.186144         5   0.171784   \n",
       "1               1         0.185225         1   0.185225         5   0.172108   \n",
       "2               1         0.185548         1   0.185548         5   0.171884   \n",
       "3               1         0.185436         1   0.185436         5   0.172002   \n",
       "\n",
       "   top3_idx  top3_prob  \n",
       "0         0   0.171135  \n",
       "1         0   0.170396  \n",
       "2         0   0.170404  \n",
       "3         0   0.170435  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] files=589 train_chunks=1,427 val_chunks=174 epochs=50\n",
      "trainable params tensors: 26/26\n",
      "step 1/3 | epoch 1/50 | batch 1/60 | global_step 1 | loss_total 1.5414\n",
      "step 1/3 | epoch 1/50 | batch 2/60 | global_step 2 | loss_total 1.3966\n",
      "step 1/3 | epoch 1/50 | batch 3/60 | global_step 3 | loss_total 1.3880\n",
      "step 1/3 | epoch 1/50 | batch 4/60 | global_step 4 | loss_total 1.3576\n",
      "step 1/3 | epoch 1/50 | batch 5/60 | global_step 5 | loss_total 1.3954\n",
      "step 1/3 | epoch 1/50 | batch 6/60 | global_step 6 | loss_total 1.3424\n",
      "step 1/3 | epoch 1/50 | batch 7/60 | global_step 7 | loss_total 1.3245\n",
      "step 1/3 | epoch 1/50 | batch 8/60 | global_step 8 | loss_total 1.2952\n",
      "step 1/3 | epoch 1/50 | batch 9/60 | global_step 9 | loss_total 1.2862\n",
      "step 1/3 | epoch 1/50 | batch 10/60 | global_step 10 | loss_total 1.2702\n",
      "step 1/3 | epoch 1/50 | batch 11/60 | global_step 11 | loss_total 1.3239\n",
      "step 1/3 | epoch 1/50 | batch 12/60 | global_step 12 | loss_total 1.2424\n",
      "step 1/3 | epoch 1/50 | batch 13/60 | global_step 13 | loss_total 1.3027\n",
      "step 1/3 | epoch 1/50 | batch 14/60 | global_step 14 | loss_total 1.2133\n",
      "step 1/3 | epoch 1/50 | batch 15/60 | global_step 15 | loss_total 1.3209\n",
      "step 1/3 | epoch 1/50 | batch 16/60 | global_step 16 | loss_total 1.2274\n",
      "step 1/3 | epoch 1/50 | batch 17/60 | global_step 17 | loss_total 1.2714\n",
      "step 1/3 | epoch 1/50 | batch 18/60 | global_step 18 | loss_total 1.2294\n",
      "step 1/3 | epoch 1/50 | batch 19/60 | global_step 19 | loss_total 1.2033\n",
      "step 1/3 | epoch 1/50 | batch 20/60 | global_step 20 | loss_total 1.1903\n",
      "step 1/3 | epoch 1/50 | batch 21/60 | global_step 21 | loss_total 1.1610\n",
      "step 1/3 | epoch 1/50 | batch 22/60 | global_step 22 | loss_total 1.1508\n",
      "step 1/3 | epoch 1/50 | batch 23/60 | global_step 23 | loss_total 1.1442\n",
      "step 1/3 | epoch 1/50 | batch 24/60 | global_step 24 | loss_total 1.1353\n",
      "step 1/3 | epoch 1/50 | batch 25/60 | global_step 25 | loss_total 1.1281\n",
      "step 1/3 | epoch 1/50 | batch 26/60 | global_step 26 | loss_total 1.1136\n",
      "step 1/3 | epoch 1/50 | batch 27/60 | global_step 27 | loss_total 1.1350\n",
      "step 1/3 | epoch 1/50 | batch 28/60 | global_step 28 | loss_total 1.0758\n",
      "step 1/3 | epoch 1/50 | batch 29/60 | global_step 29 | loss_total 1.0881\n",
      "step 1/3 | epoch 1/50 | batch 30/60 | global_step 30 | loss_total 1.1034\n",
      "step 1/3 | epoch 1/50 | batch 31/60 | global_step 31 | loss_total 1.0718\n",
      "step 1/3 | epoch 1/50 | batch 32/60 | global_step 32 | loss_total 1.0863\n",
      "step 1/3 | epoch 1/50 | batch 33/60 | global_step 33 | loss_total 1.0571\n",
      "step 1/3 | epoch 1/50 | batch 34/60 | global_step 34 | loss_total 1.0448\n",
      "step 1/3 | epoch 1/50 | batch 35/60 | global_step 35 | loss_total 1.0363\n",
      "step 1/3 | epoch 1/50 | batch 36/60 | global_step 36 | loss_total 1.0419\n",
      "step 1/3 | epoch 1/50 | batch 37/60 | global_step 37 | loss_total 1.0414\n",
      "step 1/3 | epoch 1/50 | batch 38/60 | global_step 38 | loss_total 1.0158\n",
      "step 1/3 | epoch 1/50 | batch 39/60 | global_step 39 | loss_total 1.0087\n",
      "step 1/3 | epoch 1/50 | batch 40/60 | global_step 40 | loss_total 1.0156\n",
      "step 1/3 | epoch 1/50 | batch 41/60 | global_step 41 | loss_total 1.0192\n",
      "step 1/3 | epoch 1/50 | batch 42/60 | global_step 42 | loss_total 1.0026\n",
      "step 1/3 | epoch 1/50 | batch 43/60 | global_step 43 | loss_total 0.9965\n",
      "step 1/3 | epoch 1/50 | batch 44/60 | global_step 44 | loss_total 0.9773\n",
      "step 1/3 | epoch 1/50 | batch 45/60 | global_step 45 | loss_total 0.9787\n",
      "step 1/3 | epoch 1/50 | batch 46/60 | global_step 46 | loss_total 0.9766\n",
      "step 1/3 | epoch 1/50 | batch 47/60 | global_step 47 | loss_total 0.9671\n",
      "step 1/3 | epoch 1/50 | batch 48/60 | global_step 48 | loss_total 0.9661\n",
      "step 1/3 | epoch 1/50 | batch 49/60 | global_step 49 | loss_total 0.9626\n",
      "step 1/3 | epoch 1/50 | batch 50/60 | global_step 50 | loss_total 0.9484\n",
      "step 1/3 | epoch 1/50 | batch 51/60 | global_step 51 | loss_total 0.9552\n",
      "step 1/3 | epoch 1/50 | batch 52/60 | global_step 52 | loss_total 0.9452\n",
      "step 1/3 | epoch 1/50 | batch 53/60 | global_step 53 | loss_total 0.9403\n",
      "step 1/3 | epoch 1/50 | batch 54/60 | global_step 54 | loss_total 0.9422\n",
      "step 1/3 | epoch 1/50 | batch 55/60 | global_step 55 | loss_total 0.9247\n",
      "step 1/3 | epoch 1/50 | batch 56/60 | global_step 56 | loss_total 0.9236\n",
      "step 1/3 | epoch 1/50 | batch 57/60 | global_step 57 | loss_total 0.9192\n",
      "step 1/3 | epoch 1/50 | batch 58/60 | global_step 58 | loss_total 0.9164\n",
      "step 1/3 | epoch 1/50 | batch 59/60 | global_step 59 | loss_total 0.9054\n",
      "step 1/3 | epoch 1/50 | batch 60/60 | global_step 60 | loss_total 0.9056\n",
      "[epoch done] step 1/3 epoch 1/50 | train_total=1.1142 val_total=0.9182\n",
      "step 1/3 | epoch 2/50 | batch 1/60 | global_step 61 | loss_total 0.9036\n",
      "step 1/3 | epoch 2/50 | batch 2/60 | global_step 62 | loss_total 0.9010\n",
      "step 1/3 | epoch 2/50 | batch 3/60 | global_step 63 | loss_total 0.8902\n",
      "step 1/3 | epoch 2/50 | batch 4/60 | global_step 64 | loss_total 0.9125\n",
      "step 1/3 | epoch 2/50 | batch 5/60 | global_step 65 | loss_total 0.8914\n",
      "step 1/3 | epoch 2/50 | batch 6/60 | global_step 66 | loss_total 0.8399\n",
      "step 1/3 | epoch 2/50 | batch 7/60 | global_step 67 | loss_total 0.8805\n",
      "step 1/3 | epoch 2/50 | batch 8/60 | global_step 68 | loss_total 0.9221\n",
      "step 1/3 | epoch 2/50 | batch 9/60 | global_step 69 | loss_total 0.8434\n",
      "step 1/3 | epoch 2/50 | batch 10/60 | global_step 70 | loss_total 0.8691\n",
      "step 1/3 | epoch 2/50 | batch 11/60 | global_step 71 | loss_total 0.8607\n",
      "step 1/3 | epoch 2/50 | batch 12/60 | global_step 72 | loss_total 0.8848\n",
      "step 1/3 | epoch 2/50 | batch 13/60 | global_step 73 | loss_total 0.8266\n",
      "step 1/3 | epoch 2/50 | batch 14/60 | global_step 74 | loss_total 0.8790\n",
      "step 1/3 | epoch 2/50 | batch 15/60 | global_step 75 | loss_total 0.8224\n",
      "step 1/3 | epoch 2/50 | batch 16/60 | global_step 76 | loss_total 0.8498\n",
      "step 1/3 | epoch 2/50 | batch 17/60 | global_step 77 | loss_total 0.8176\n",
      "step 1/3 | epoch 2/50 | batch 18/60 | global_step 78 | loss_total 0.8320\n",
      "step 1/3 | epoch 2/50 | batch 19/60 | global_step 79 | loss_total 0.8120\n",
      "step 1/3 | epoch 2/50 | batch 20/60 | global_step 80 | loss_total 0.8183\n",
      "step 1/3 | epoch 2/50 | batch 21/60 | global_step 81 | loss_total 0.8029\n",
      "step 1/3 | epoch 2/50 | batch 22/60 | global_step 82 | loss_total 0.8104\n",
      "step 1/3 | epoch 2/50 | batch 23/60 | global_step 83 | loss_total 0.7893\n",
      "step 1/3 | epoch 2/50 | batch 24/60 | global_step 84 | loss_total 0.8446\n",
      "step 1/3 | epoch 2/50 | batch 25/60 | global_step 85 | loss_total 0.8360\n",
      "step 1/3 | epoch 2/50 | batch 26/60 | global_step 86 | loss_total 0.7620\n",
      "step 1/3 | epoch 2/50 | batch 27/60 | global_step 87 | loss_total 0.8144\n",
      "step 1/3 | epoch 2/50 | batch 28/60 | global_step 88 | loss_total 0.7996\n",
      "step 1/3 | epoch 2/50 | batch 29/60 | global_step 89 | loss_total 0.8011\n",
      "step 1/3 | epoch 2/50 | batch 30/60 | global_step 90 | loss_total 0.8493\n",
      "step 1/3 | epoch 2/50 | batch 31/60 | global_step 91 | loss_total 0.7702\n",
      "step 1/3 | epoch 2/50 | batch 32/60 | global_step 92 | loss_total 0.8085\n",
      "step 1/3 | epoch 2/50 | batch 33/60 | global_step 93 | loss_total 0.7871\n",
      "step 1/3 | epoch 2/50 | batch 34/60 | global_step 94 | loss_total 0.7688\n",
      "step 1/3 | epoch 2/50 | batch 35/60 | global_step 95 | loss_total 0.7555\n",
      "step 1/3 | epoch 2/50 | batch 36/60 | global_step 96 | loss_total 0.7641\n",
      "step 1/3 | epoch 2/50 | batch 37/60 | global_step 97 | loss_total 0.7448\n",
      "step 1/3 | epoch 2/50 | batch 38/60 | global_step 98 | loss_total 0.7567\n",
      "step 1/3 | epoch 2/50 | batch 39/60 | global_step 99 | loss_total 0.7740\n",
      "step 1/3 | epoch 2/50 | batch 40/60 | global_step 100 | loss_total 0.8436\n",
      "step 1/3 | epoch 2/50 | batch 41/60 | global_step 101 | loss_total 0.7296\n",
      "step 1/3 | epoch 2/50 | batch 42/60 | global_step 102 | loss_total 0.7719\n",
      "step 1/3 | epoch 2/50 | batch 43/60 | global_step 103 | loss_total 0.8222\n",
      "step 1/3 | epoch 2/50 | batch 44/60 | global_step 104 | loss_total 0.8160\n",
      "step 1/3 | epoch 2/50 | batch 45/60 | global_step 105 | loss_total 0.8147\n",
      "step 1/3 | epoch 2/50 | batch 46/60 | global_step 106 | loss_total 0.7939\n",
      "step 1/3 | epoch 2/50 | batch 47/60 | global_step 107 | loss_total 0.7694\n",
      "step 1/3 | epoch 2/50 | batch 48/60 | global_step 108 | loss_total 0.7993\n",
      "step 1/3 | epoch 2/50 | batch 49/60 | global_step 109 | loss_total 0.7925\n",
      "step 1/3 | epoch 2/50 | batch 50/60 | global_step 110 | loss_total 0.7860\n",
      "step 1/3 | epoch 2/50 | batch 51/60 | global_step 111 | loss_total 0.7919\n",
      "step 1/3 | epoch 2/50 | batch 52/60 | global_step 112 | loss_total 0.7753\n",
      "step 1/3 | epoch 2/50 | batch 53/60 | global_step 113 | loss_total 0.8174\n",
      "step 1/3 | epoch 2/50 | batch 54/60 | global_step 114 | loss_total 0.8746\n",
      "step 1/3 | epoch 2/50 | batch 55/60 | global_step 115 | loss_total 0.8305\n",
      "step 1/3 | epoch 2/50 | batch 56/60 | global_step 116 | loss_total 0.7906\n",
      "step 1/3 | epoch 2/50 | batch 57/60 | global_step 117 | loss_total 0.7829\n",
      "step 1/3 | epoch 2/50 | batch 58/60 | global_step 118 | loss_total 0.7756\n",
      "step 1/3 | epoch 2/50 | batch 59/60 | global_step 119 | loss_total 0.8064\n",
      "step 1/3 | epoch 2/50 | batch 60/60 | global_step 120 | loss_total 0.8199\n",
      "[epoch done] step 1/3 epoch 2/50 | train_total=0.8183 val_total=0.7528\n",
      "step 1/3 | epoch 3/50 | batch 1/60 | global_step 121 | loss_total 0.8188\n",
      "step 1/3 | epoch 3/50 | batch 2/60 | global_step 122 | loss_total 0.8045\n",
      "step 1/3 | epoch 3/50 | batch 3/60 | global_step 123 | loss_total 0.8274\n",
      "step 1/3 | epoch 3/50 | batch 4/60 | global_step 124 | loss_total 0.8035\n",
      "step 1/3 | epoch 3/50 | batch 5/60 | global_step 125 | loss_total 0.8394\n",
      "step 1/3 | epoch 3/50 | batch 6/60 | global_step 126 | loss_total 0.8253\n",
      "step 1/3 | epoch 3/50 | batch 7/60 | global_step 127 | loss_total 0.8530\n",
      "step 1/3 | epoch 3/50 | batch 8/60 | global_step 128 | loss_total 0.8120\n",
      "step 1/3 | epoch 3/50 | batch 9/60 | global_step 129 | loss_total 0.7991\n",
      "step 1/3 | epoch 3/50 | batch 10/60 | global_step 130 | loss_total 0.7857\n",
      "step 1/3 | epoch 3/50 | batch 11/60 | global_step 131 | loss_total 0.8627\n",
      "step 1/3 | epoch 3/50 | batch 12/60 | global_step 132 | loss_total 0.8133\n",
      "step 1/3 | epoch 3/50 | batch 13/60 | global_step 133 | loss_total 0.8026\n",
      "step 1/3 | epoch 3/50 | batch 14/60 | global_step 134 | loss_total 0.7324\n",
      "step 1/3 | epoch 3/50 | batch 15/60 | global_step 135 | loss_total 0.8658\n",
      "step 1/3 | epoch 3/50 | batch 16/60 | global_step 136 | loss_total 0.8335\n",
      "step 1/3 | epoch 3/50 | batch 17/60 | global_step 137 | loss_total 0.8552\n",
      "step 1/3 | epoch 3/50 | batch 18/60 | global_step 138 | loss_total 0.7929\n",
      "step 1/3 | epoch 3/50 | batch 19/60 | global_step 139 | loss_total 0.8227\n",
      "step 1/3 | epoch 3/50 | batch 20/60 | global_step 140 | loss_total 0.7398\n",
      "step 1/3 | epoch 3/50 | batch 21/60 | global_step 141 | loss_total 0.7796\n",
      "step 1/3 | epoch 3/50 | batch 22/60 | global_step 142 | loss_total 0.6947\n",
      "step 1/3 | epoch 3/50 | batch 23/60 | global_step 143 | loss_total 0.7160\n",
      "step 1/3 | epoch 3/50 | batch 24/60 | global_step 144 | loss_total 0.7802\n",
      "step 1/3 | epoch 3/50 | batch 25/60 | global_step 145 | loss_total 0.7846\n",
      "step 1/3 | epoch 3/50 | batch 26/60 | global_step 146 | loss_total 0.7129\n",
      "step 1/3 | epoch 3/50 | batch 27/60 | global_step 147 | loss_total 0.7877\n",
      "step 1/3 | epoch 3/50 | batch 28/60 | global_step 148 | loss_total 0.8548\n",
      "step 1/3 | epoch 3/50 | batch 29/60 | global_step 149 | loss_total 0.7838\n",
      "step 1/3 | epoch 3/50 | batch 30/60 | global_step 150 | loss_total 0.8200\n",
      "step 1/3 | epoch 3/50 | batch 31/60 | global_step 151 | loss_total 0.7426\n",
      "step 1/3 | epoch 3/50 | batch 32/60 | global_step 152 | loss_total 0.6968\n",
      "step 1/3 | epoch 3/50 | batch 33/60 | global_step 153 | loss_total 0.6971\n",
      "step 1/3 | epoch 3/50 | batch 34/60 | global_step 154 | loss_total 0.7343\n",
      "step 1/3 | epoch 3/50 | batch 35/60 | global_step 155 | loss_total 0.6995\n",
      "step 1/3 | epoch 3/50 | batch 36/60 | global_step 156 | loss_total 0.6679\n",
      "step 1/3 | epoch 3/50 | batch 37/60 | global_step 157 | loss_total 0.6961\n",
      "step 1/3 | epoch 3/50 | batch 38/60 | global_step 158 | loss_total 0.7368\n",
      "step 1/3 | epoch 3/50 | batch 39/60 | global_step 159 | loss_total 0.6930\n",
      "step 1/3 | epoch 3/50 | batch 40/60 | global_step 160 | loss_total 0.7096\n",
      "step 1/3 | epoch 3/50 | batch 41/60 | global_step 161 | loss_total 0.6891\n",
      "step 1/3 | epoch 3/50 | batch 42/60 | global_step 162 | loss_total 0.6516\n",
      "step 1/3 | epoch 3/50 | batch 43/60 | global_step 163 | loss_total 0.6980\n",
      "step 1/3 | epoch 3/50 | batch 44/60 | global_step 164 | loss_total 0.7114\n",
      "step 1/3 | epoch 3/50 | batch 45/60 | global_step 165 | loss_total 0.7017\n",
      "step 1/3 | epoch 3/50 | batch 46/60 | global_step 166 | loss_total 0.7134\n",
      "step 1/3 | epoch 3/50 | batch 47/60 | global_step 167 | loss_total 0.6994\n",
      "step 1/3 | epoch 3/50 | batch 48/60 | global_step 168 | loss_total 0.6974\n",
      "step 1/3 | epoch 3/50 | batch 49/60 | global_step 169 | loss_total 0.6712\n",
      "step 1/3 | epoch 3/50 | batch 50/60 | global_step 170 | loss_total 0.6425\n",
      "step 1/3 | epoch 3/50 | batch 51/60 | global_step 171 | loss_total 0.6906\n",
      "step 1/3 | epoch 3/50 | batch 52/60 | global_step 172 | loss_total 0.6571\n",
      "step 1/3 | epoch 3/50 | batch 53/60 | global_step 173 | loss_total 0.6794\n",
      "step 1/3 | epoch 3/50 | batch 54/60 | global_step 174 | loss_total 0.6616\n",
      "step 1/3 | epoch 3/50 | batch 55/60 | global_step 175 | loss_total 0.6556\n",
      "step 1/3 | epoch 3/50 | batch 56/60 | global_step 176 | loss_total 0.6567\n",
      "step 1/3 | epoch 3/50 | batch 57/60 | global_step 177 | loss_total 0.6556\n",
      "step 1/3 | epoch 3/50 | batch 58/60 | global_step 178 | loss_total 0.6621\n",
      "step 1/3 | epoch 3/50 | batch 59/60 | global_step 179 | loss_total 0.6509\n",
      "step 1/3 | epoch 3/50 | batch 60/60 | global_step 180 | loss_total 0.6302\n",
      "[epoch done] step 1/3 epoch 3/50 | train_total=0.7425 val_total=0.5941\n",
      "step 1/3 | epoch 4/50 | batch 1/60 | global_step 181 | loss_total 0.6413\n",
      "step 1/3 | epoch 4/50 | batch 2/60 | global_step 182 | loss_total 0.6552\n",
      "step 1/3 | epoch 4/50 | batch 3/60 | global_step 183 | loss_total 0.6764\n",
      "step 1/3 | epoch 4/50 | batch 4/60 | global_step 184 | loss_total 0.7412\n",
      "step 1/3 | epoch 4/50 | batch 5/60 | global_step 185 | loss_total 0.6772\n",
      "step 1/3 | epoch 4/50 | batch 6/60 | global_step 186 | loss_total 0.6804\n",
      "step 1/3 | epoch 4/50 | batch 7/60 | global_step 187 | loss_total 0.7048\n",
      "step 1/3 | epoch 4/50 | batch 8/60 | global_step 188 | loss_total 0.6494\n",
      "step 1/3 | epoch 4/50 | batch 9/60 | global_step 189 | loss_total 0.6622\n",
      "step 1/3 | epoch 4/50 | batch 10/60 | global_step 190 | loss_total 0.5932\n",
      "step 1/3 | epoch 4/50 | batch 11/60 | global_step 191 | loss_total 0.6604\n",
      "step 1/3 | epoch 4/50 | batch 12/60 | global_step 192 | loss_total 0.6756\n",
      "step 1/3 | epoch 4/50 | batch 13/60 | global_step 193 | loss_total 0.7327\n",
      "step 1/3 | epoch 4/50 | batch 14/60 | global_step 194 | loss_total 0.5767\n",
      "step 1/3 | epoch 4/50 | batch 15/60 | global_step 195 | loss_total 0.5641\n",
      "step 1/3 | epoch 4/50 | batch 16/60 | global_step 196 | loss_total 0.6940\n",
      "step 1/3 | epoch 4/50 | batch 17/60 | global_step 197 | loss_total 0.6293\n",
      "step 1/3 | epoch 4/50 | batch 18/60 | global_step 198 | loss_total 0.5438\n",
      "step 1/3 | epoch 4/50 | batch 19/60 | global_step 199 | loss_total 0.6863\n",
      "step 1/3 | epoch 4/50 | batch 20/60 | global_step 200 | loss_total 0.6940\n",
      "step 1/3 | epoch 4/50 | batch 21/60 | global_step 201 | loss_total 0.6173\n",
      "step 1/3 | epoch 4/50 | batch 22/60 | global_step 202 | loss_total 0.6243\n",
      "step 1/3 | epoch 4/50 | batch 23/60 | global_step 203 | loss_total 0.6352\n",
      "step 1/3 | epoch 4/50 | batch 24/60 | global_step 204 | loss_total 0.5955\n",
      "step 1/3 | epoch 4/50 | batch 25/60 | global_step 205 | loss_total 0.7312\n",
      "step 1/3 | epoch 4/50 | batch 26/60 | global_step 206 | loss_total 0.6448\n",
      "step 1/3 | epoch 4/50 | batch 27/60 | global_step 207 | loss_total 0.5952\n",
      "step 1/3 | epoch 4/50 | batch 28/60 | global_step 208 | loss_total 0.6117\n",
      "step 1/3 | epoch 4/50 | batch 29/60 | global_step 209 | loss_total 0.6113\n",
      "step 1/3 | epoch 4/50 | batch 30/60 | global_step 210 | loss_total 0.5814\n",
      "step 1/3 | epoch 4/50 | batch 31/60 | global_step 211 | loss_total 0.6974\n",
      "step 1/3 | epoch 4/50 | batch 32/60 | global_step 212 | loss_total 0.6533\n",
      "step 1/3 | epoch 4/50 | batch 33/60 | global_step 213 | loss_total 0.5723\n",
      "step 1/3 | epoch 4/50 | batch 34/60 | global_step 214 | loss_total 0.8740\n",
      "step 1/3 | epoch 4/50 | batch 35/60 | global_step 215 | loss_total 0.5433\n",
      "step 1/3 | epoch 4/50 | batch 36/60 | global_step 216 | loss_total 0.6825\n",
      "step 1/3 | epoch 4/50 | batch 37/60 | global_step 217 | loss_total 0.6286\n",
      "step 1/3 | epoch 4/50 | batch 38/60 | global_step 218 | loss_total 0.5672\n",
      "step 1/3 | epoch 4/50 | batch 39/60 | global_step 219 | loss_total 0.6506\n",
      "step 1/3 | epoch 4/50 | batch 40/60 | global_step 220 | loss_total 0.5771\n",
      "step 1/3 | epoch 4/50 | batch 41/60 | global_step 221 | loss_total 0.5735\n",
      "step 1/3 | epoch 4/50 | batch 42/60 | global_step 222 | loss_total 0.6671\n",
      "step 1/3 | epoch 4/50 | batch 43/60 | global_step 223 | loss_total 0.6784\n",
      "step 1/3 | epoch 4/50 | batch 44/60 | global_step 224 | loss_total 0.6264\n",
      "step 1/3 | epoch 4/50 | batch 45/60 | global_step 225 | loss_total 0.6095\n",
      "step 1/3 | epoch 4/50 | batch 46/60 | global_step 226 | loss_total 0.5409\n",
      "step 1/3 | epoch 4/50 | batch 47/60 | global_step 227 | loss_total 0.5349\n",
      "step 1/3 | epoch 4/50 | batch 48/60 | global_step 228 | loss_total 0.6373\n",
      "step 1/3 | epoch 4/50 | batch 49/60 | global_step 229 | loss_total 0.5040\n",
      "step 1/3 | epoch 4/50 | batch 50/60 | global_step 230 | loss_total 0.6783\n",
      "step 1/3 | epoch 4/50 | batch 51/60 | global_step 231 | loss_total 0.6879\n",
      "step 1/3 | epoch 4/50 | batch 52/60 | global_step 232 | loss_total 0.6697\n",
      "step 1/3 | epoch 4/50 | batch 53/60 | global_step 233 | loss_total 0.5111\n",
      "step 1/3 | epoch 4/50 | batch 54/60 | global_step 234 | loss_total 0.5572\n",
      "step 1/3 | epoch 4/50 | batch 55/60 | global_step 235 | loss_total 0.6410\n",
      "step 1/3 | epoch 4/50 | batch 56/60 | global_step 236 | loss_total 0.5412\n",
      "step 1/3 | epoch 4/50 | batch 57/60 | global_step 237 | loss_total 0.5335\n",
      "step 1/3 | epoch 4/50 | batch 58/60 | global_step 238 | loss_total 0.6068\n",
      "step 1/3 | epoch 4/50 | batch 59/60 | global_step 239 | loss_total 0.5734\n",
      "step 1/3 | epoch 4/50 | batch 60/60 | global_step 240 | loss_total 0.6216\n",
      "[epoch done] step 1/3 epoch 4/50 | train_total=0.6304 val_total=0.5052\n",
      "step 1/3 | epoch 5/50 | batch 1/60 | global_step 241 | loss_total 0.6241\n",
      "step 1/3 | epoch 5/50 | batch 2/60 | global_step 242 | loss_total 0.5030\n",
      "step 1/3 | epoch 5/50 | batch 3/60 | global_step 243 | loss_total 0.6125\n",
      "step 1/3 | epoch 5/50 | batch 4/60 | global_step 244 | loss_total 0.5463\n",
      "step 1/3 | epoch 5/50 | batch 5/60 | global_step 245 | loss_total 0.5613\n",
      "step 1/3 | epoch 5/50 | batch 6/60 | global_step 246 | loss_total 0.4845\n",
      "step 1/3 | epoch 5/50 | batch 7/60 | global_step 247 | loss_total 0.4619\n",
      "step 1/3 | epoch 5/50 | batch 8/60 | global_step 248 | loss_total 0.5451\n",
      "step 1/3 | epoch 5/50 | batch 9/60 | global_step 249 | loss_total 0.4578\n",
      "step 1/3 | epoch 5/50 | batch 10/60 | global_step 250 | loss_total 0.4842\n",
      "step 1/3 | epoch 5/50 | batch 11/60 | global_step 251 | loss_total 0.5421\n",
      "step 1/3 | epoch 5/50 | batch 12/60 | global_step 252 | loss_total 0.4924\n",
      "step 1/3 | epoch 5/50 | batch 13/60 | global_step 253 | loss_total 0.4604\n",
      "step 1/3 | epoch 5/50 | batch 14/60 | global_step 254 | loss_total 0.6727\n",
      "step 1/3 | epoch 5/50 | batch 15/60 | global_step 255 | loss_total 0.9779\n",
      "step 1/3 | epoch 5/50 | batch 16/60 | global_step 256 | loss_total 0.7689\n",
      "step 1/3 | epoch 5/50 | batch 17/60 | global_step 257 | loss_total 0.5195\n",
      "step 1/3 | epoch 5/50 | batch 18/60 | global_step 258 | loss_total 0.9326\n",
      "step 1/3 | epoch 5/50 | batch 19/60 | global_step 259 | loss_total 0.5651\n",
      "step 1/3 | epoch 5/50 | batch 20/60 | global_step 260 | loss_total 0.4789\n",
      "step 1/3 | epoch 5/50 | batch 21/60 | global_step 261 | loss_total 0.6925\n",
      "step 1/3 | epoch 5/50 | batch 22/60 | global_step 262 | loss_total 0.6189\n",
      "step 1/3 | epoch 5/50 | batch 23/60 | global_step 263 | loss_total 0.6824\n",
      "step 1/3 | epoch 5/50 | batch 24/60 | global_step 264 | loss_total 0.7568\n",
      "step 1/3 | epoch 5/50 | batch 25/60 | global_step 265 | loss_total 0.5677\n",
      "step 1/3 | epoch 5/50 | batch 26/60 | global_step 266 | loss_total 0.4876\n",
      "step 1/3 | epoch 5/50 | batch 27/60 | global_step 267 | loss_total 0.4899\n",
      "step 1/3 | epoch 5/50 | batch 28/60 | global_step 268 | loss_total 0.4949\n",
      "step 1/3 | epoch 5/50 | batch 29/60 | global_step 269 | loss_total 0.6004\n",
      "step 1/3 | epoch 5/50 | batch 30/60 | global_step 270 | loss_total 0.6167\n",
      "step 1/3 | epoch 5/50 | batch 31/60 | global_step 271 | loss_total 0.5628\n",
      "step 1/3 | epoch 5/50 | batch 32/60 | global_step 272 | loss_total 0.7873\n",
      "step 1/3 | epoch 5/50 | batch 33/60 | global_step 273 | loss_total 0.5173\n",
      "step 1/3 | epoch 5/50 | batch 34/60 | global_step 274 | loss_total 0.6807\n",
      "step 1/3 | epoch 5/50 | batch 35/60 | global_step 275 | loss_total 0.6877\n",
      "step 1/3 | epoch 5/50 | batch 36/60 | global_step 276 | loss_total 0.5409\n",
      "step 1/3 | epoch 5/50 | batch 37/60 | global_step 277 | loss_total 0.6766\n",
      "step 1/3 | epoch 5/50 | batch 38/60 | global_step 278 | loss_total 0.5066\n",
      "step 1/3 | epoch 5/50 | batch 39/60 | global_step 279 | loss_total 0.5592\n",
      "step 1/3 | epoch 5/50 | batch 40/60 | global_step 280 | loss_total 0.6091\n",
      "step 1/3 | epoch 5/50 | batch 41/60 | global_step 281 | loss_total 0.6269\n",
      "step 1/3 | epoch 5/50 | batch 42/60 | global_step 282 | loss_total 0.5162\n",
      "step 1/3 | epoch 5/50 | batch 43/60 | global_step 283 | loss_total 0.5537\n",
      "step 1/3 | epoch 5/50 | batch 44/60 | global_step 284 | loss_total 0.6235\n",
      "step 1/3 | epoch 5/50 | batch 45/60 | global_step 285 | loss_total 0.5376\n",
      "step 1/3 | epoch 5/50 | batch 46/60 | global_step 286 | loss_total 0.5475\n",
      "step 1/3 | epoch 5/50 | batch 47/60 | global_step 287 | loss_total 0.5633\n",
      "step 1/3 | epoch 5/50 | batch 48/60 | global_step 288 | loss_total 0.5915\n",
      "step 1/3 | epoch 5/50 | batch 49/60 | global_step 289 | loss_total 0.5339\n",
      "step 1/3 | epoch 5/50 | batch 50/60 | global_step 290 | loss_total 0.6971\n",
      "step 1/3 | epoch 5/50 | batch 51/60 | global_step 291 | loss_total 0.4745\n",
      "step 1/3 | epoch 5/50 | batch 52/60 | global_step 292 | loss_total 0.5270\n",
      "step 1/3 | epoch 5/50 | batch 53/60 | global_step 293 | loss_total 0.5968\n",
      "step 1/3 | epoch 5/50 | batch 54/60 | global_step 294 | loss_total 0.4938\n",
      "step 1/3 | epoch 5/50 | batch 55/60 | global_step 295 | loss_total 0.5086\n",
      "step 1/3 | epoch 5/50 | batch 56/60 | global_step 296 | loss_total 0.4961\n",
      "step 1/3 | epoch 5/50 | batch 57/60 | global_step 297 | loss_total 0.5383\n",
      "step 1/3 | epoch 5/50 | batch 58/60 | global_step 298 | loss_total 0.5877\n",
      "step 1/3 | epoch 5/50 | batch 59/60 | global_step 299 | loss_total 0.4480\n",
      "step 1/3 | epoch 5/50 | batch 60/60 | global_step 300 | loss_total 0.5000\n",
      "[epoch done] step 1/3 epoch 5/50 | train_total=0.5798 val_total=0.3948\n",
      "step 1/3 | epoch 6/50 | batch 1/60 | global_step 301 | loss_total 0.6081\n",
      "step 1/3 | epoch 6/50 | batch 2/60 | global_step 302 | loss_total 0.6644\n",
      "step 1/3 | epoch 6/50 | batch 3/60 | global_step 303 | loss_total 0.5643\n",
      "step 1/3 | epoch 6/50 | batch 4/60 | global_step 304 | loss_total 0.5896\n",
      "step 1/3 | epoch 6/50 | batch 5/60 | global_step 305 | loss_total 0.6668\n",
      "step 1/3 | epoch 6/50 | batch 6/60 | global_step 306 | loss_total 0.4393\n",
      "step 1/3 | epoch 6/50 | batch 7/60 | global_step 307 | loss_total 0.4801\n",
      "step 1/3 | epoch 6/50 | batch 8/60 | global_step 308 | loss_total 0.5579\n",
      "step 1/3 | epoch 6/50 | batch 9/60 | global_step 309 | loss_total 0.4402\n",
      "step 1/3 | epoch 6/50 | batch 10/60 | global_step 310 | loss_total 0.6005\n",
      "step 1/3 | epoch 6/50 | batch 11/60 | global_step 311 | loss_total 0.5045\n",
      "step 1/3 | epoch 6/50 | batch 12/60 | global_step 312 | loss_total 0.5720\n",
      "step 1/3 | epoch 6/50 | batch 13/60 | global_step 313 | loss_total 0.4785\n",
      "step 1/3 | epoch 6/50 | batch 14/60 | global_step 314 | loss_total 0.5919\n",
      "step 1/3 | epoch 6/50 | batch 15/60 | global_step 315 | loss_total 0.4692\n",
      "step 1/3 | epoch 6/50 | batch 16/60 | global_step 316 | loss_total 0.6257\n",
      "step 1/3 | epoch 6/50 | batch 17/60 | global_step 317 | loss_total 0.4457\n",
      "step 1/3 | epoch 6/50 | batch 18/60 | global_step 318 | loss_total 0.5783\n",
      "step 1/3 | epoch 6/50 | batch 19/60 | global_step 319 | loss_total 0.4535\n",
      "step 1/3 | epoch 6/50 | batch 20/60 | global_step 320 | loss_total 0.6280\n",
      "step 1/3 | epoch 6/50 | batch 21/60 | global_step 321 | loss_total 0.4305\n",
      "step 1/3 | epoch 6/50 | batch 22/60 | global_step 322 | loss_total 0.4133\n",
      "step 1/3 | epoch 6/50 | batch 23/60 | global_step 323 | loss_total 0.5614\n",
      "step 1/3 | epoch 6/50 | batch 24/60 | global_step 324 | loss_total 0.5756\n",
      "step 1/3 | epoch 6/50 | batch 25/60 | global_step 325 | loss_total 0.4009\n",
      "step 1/3 | epoch 6/50 | batch 26/60 | global_step 326 | loss_total 0.5750\n",
      "step 1/3 | epoch 6/50 | batch 27/60 | global_step 327 | loss_total 0.5875\n",
      "step 1/3 | epoch 6/50 | batch 28/60 | global_step 328 | loss_total 0.6310\n",
      "step 1/3 | epoch 6/50 | batch 29/60 | global_step 329 | loss_total 0.4386\n",
      "step 1/3 | epoch 6/50 | batch 30/60 | global_step 330 | loss_total 0.4925\n",
      "step 1/3 | epoch 6/50 | batch 31/60 | global_step 331 | loss_total 0.4816\n",
      "step 1/3 | epoch 6/50 | batch 32/60 | global_step 332 | loss_total 0.6519\n",
      "step 1/3 | epoch 6/50 | batch 33/60 | global_step 333 | loss_total 0.7571\n",
      "step 1/3 | epoch 6/50 | batch 34/60 | global_step 334 | loss_total 0.4112\n",
      "step 1/3 | epoch 6/50 | batch 35/60 | global_step 335 | loss_total 0.4451\n",
      "step 1/3 | epoch 6/50 | batch 36/60 | global_step 336 | loss_total 0.6063\n",
      "step 1/3 | epoch 6/50 | batch 37/60 | global_step 337 | loss_total 0.5969\n",
      "step 1/3 | epoch 6/50 | batch 38/60 | global_step 338 | loss_total 0.4105\n",
      "step 1/3 | epoch 6/50 | batch 39/60 | global_step 339 | loss_total 0.5254\n",
      "step 1/3 | epoch 6/50 | batch 40/60 | global_step 340 | loss_total 0.5444\n",
      "step 1/3 | epoch 6/50 | batch 41/60 | global_step 341 | loss_total 0.4848\n",
      "step 1/3 | epoch 6/50 | batch 42/60 | global_step 342 | loss_total 0.4417\n",
      "step 1/3 | epoch 6/50 | batch 43/60 | global_step 343 | loss_total 0.7115\n",
      "step 1/3 | epoch 6/50 | batch 44/60 | global_step 344 | loss_total 0.4941\n",
      "step 1/3 | epoch 6/50 | batch 45/60 | global_step 345 | loss_total 0.7206\n",
      "step 1/3 | epoch 6/50 | batch 46/60 | global_step 346 | loss_total 0.4857\n",
      "step 1/3 | epoch 6/50 | batch 47/60 | global_step 347 | loss_total 0.7233\n",
      "step 1/3 | epoch 6/50 | batch 48/60 | global_step 348 | loss_total 0.5426\n",
      "step 1/3 | epoch 6/50 | batch 49/60 | global_step 349 | loss_total 0.4490\n",
      "step 1/3 | epoch 6/50 | batch 50/60 | global_step 350 | loss_total 0.4271\n",
      "step 1/3 | epoch 6/50 | batch 51/60 | global_step 351 | loss_total 0.3870\n",
      "step 1/3 | epoch 6/50 | batch 52/60 | global_step 352 | loss_total 0.3925\n",
      "step 1/3 | epoch 6/50 | batch 53/60 | global_step 353 | loss_total 0.4175\n",
      "step 1/3 | epoch 6/50 | batch 54/60 | global_step 354 | loss_total 0.5453\n",
      "step 1/3 | epoch 6/50 | batch 55/60 | global_step 355 | loss_total 0.7099\n",
      "step 1/3 | epoch 6/50 | batch 56/60 | global_step 356 | loss_total 0.7529\n",
      "step 1/3 | epoch 6/50 | batch 57/60 | global_step 357 | loss_total 0.5664\n",
      "step 1/3 | epoch 6/50 | batch 58/60 | global_step 358 | loss_total 0.5066\n",
      "step 1/3 | epoch 6/50 | batch 59/60 | global_step 359 | loss_total 0.5805\n",
      "step 1/3 | epoch 6/50 | batch 60/60 | global_step 360 | loss_total 0.3460\n",
      "[epoch done] step 1/3 epoch 6/50 | train_total=0.5363 val_total=0.3810\n",
      "step 1/3 | epoch 7/50 | batch 1/60 | global_step 361 | loss_total 0.6424\n",
      "step 1/3 | epoch 7/50 | batch 2/60 | global_step 362 | loss_total 0.3542\n",
      "step 1/3 | epoch 7/50 | batch 3/60 | global_step 363 | loss_total 0.5488\n",
      "step 1/3 | epoch 7/50 | batch 4/60 | global_step 364 | loss_total 0.4633\n",
      "step 1/3 | epoch 7/50 | batch 5/60 | global_step 365 | loss_total 0.6052\n",
      "step 1/3 | epoch 7/50 | batch 6/60 | global_step 366 | loss_total 0.6930\n",
      "step 1/3 | epoch 7/50 | batch 7/60 | global_step 367 | loss_total 0.5312\n",
      "step 1/3 | epoch 7/50 | batch 8/60 | global_step 368 | loss_total 0.5424\n",
      "step 1/3 | epoch 7/50 | batch 9/60 | global_step 369 | loss_total 0.4673\n",
      "step 1/3 | epoch 7/50 | batch 10/60 | global_step 370 | loss_total 0.5640\n",
      "step 1/3 | epoch 7/50 | batch 11/60 | global_step 371 | loss_total 0.5097\n",
      "step 1/3 | epoch 7/50 | batch 12/60 | global_step 372 | loss_total 0.4686\n",
      "step 1/3 | epoch 7/50 | batch 13/60 | global_step 373 | loss_total 0.4649\n",
      "step 1/3 | epoch 7/50 | batch 14/60 | global_step 374 | loss_total 0.4966\n",
      "step 1/3 | epoch 7/50 | batch 15/60 | global_step 375 | loss_total 0.4496\n",
      "step 1/3 | epoch 7/50 | batch 16/60 | global_step 376 | loss_total 0.4952\n",
      "step 1/3 | epoch 7/50 | batch 17/60 | global_step 377 | loss_total 0.5479\n",
      "step 1/3 | epoch 7/50 | batch 18/60 | global_step 378 | loss_total 0.4578\n",
      "step 1/3 | epoch 7/50 | batch 19/60 | global_step 379 | loss_total 0.5116\n",
      "step 1/3 | epoch 7/50 | batch 20/60 | global_step 380 | loss_total 0.3333\n",
      "step 1/3 | epoch 7/50 | batch 21/60 | global_step 381 | loss_total 0.4041\n",
      "step 1/3 | epoch 7/50 | batch 22/60 | global_step 382 | loss_total 0.4577\n",
      "step 1/3 | epoch 7/50 | batch 23/60 | global_step 383 | loss_total 0.3676\n",
      "step 1/3 | epoch 7/50 | batch 24/60 | global_step 384 | loss_total 0.5791\n",
      "step 1/3 | epoch 7/50 | batch 25/60 | global_step 385 | loss_total 0.5622\n",
      "step 1/3 | epoch 7/50 | batch 26/60 | global_step 386 | loss_total 0.4776\n",
      "step 1/3 | epoch 7/50 | batch 27/60 | global_step 387 | loss_total 0.5528\n",
      "step 1/3 | epoch 7/50 | batch 28/60 | global_step 388 | loss_total 0.4484\n",
      "step 1/3 | epoch 7/50 | batch 29/60 | global_step 389 | loss_total 0.6790\n",
      "step 1/3 | epoch 7/50 | batch 30/60 | global_step 390 | loss_total 0.5545\n",
      "step 1/3 | epoch 7/50 | batch 31/60 | global_step 391 | loss_total 0.4034\n",
      "step 1/3 | epoch 7/50 | batch 32/60 | global_step 392 | loss_total 0.7751\n",
      "step 1/3 | epoch 7/50 | batch 33/60 | global_step 393 | loss_total 0.8696\n",
      "step 1/3 | epoch 7/50 | batch 34/60 | global_step 394 | loss_total 0.3463\n",
      "step 1/3 | epoch 7/50 | batch 35/60 | global_step 395 | loss_total 0.7654\n",
      "step 1/3 | epoch 7/50 | batch 36/60 | global_step 396 | loss_total 0.9029\n",
      "step 1/3 | epoch 7/50 | batch 37/60 | global_step 397 | loss_total 0.4297\n",
      "step 1/3 | epoch 7/50 | batch 38/60 | global_step 398 | loss_total 0.5052\n",
      "step 1/3 | epoch 7/50 | batch 39/60 | global_step 399 | loss_total 0.4003\n",
      "step 1/3 | epoch 7/50 | batch 40/60 | global_step 400 | loss_total 0.4917\n",
      "step 1/3 | epoch 7/50 | batch 41/60 | global_step 401 | loss_total 0.6064\n",
      "step 1/3 | epoch 7/50 | batch 42/60 | global_step 402 | loss_total 0.4075\n",
      "step 1/3 | epoch 7/50 | batch 43/60 | global_step 403 | loss_total 0.5229\n",
      "step 1/3 | epoch 7/50 | batch 44/60 | global_step 404 | loss_total 0.6983\n",
      "step 1/3 | epoch 7/50 | batch 45/60 | global_step 405 | loss_total 0.3940\n",
      "step 1/3 | epoch 7/50 | batch 46/60 | global_step 406 | loss_total 0.5561\n",
      "step 1/3 | epoch 7/50 | batch 47/60 | global_step 407 | loss_total 0.3911\n",
      "step 1/3 | epoch 7/50 | batch 48/60 | global_step 408 | loss_total 0.3853\n",
      "step 1/3 | epoch 7/50 | batch 49/60 | global_step 409 | loss_total 0.5189\n",
      "step 1/3 | epoch 7/50 | batch 50/60 | global_step 410 | loss_total 0.5921\n",
      "step 1/3 | epoch 7/50 | batch 51/60 | global_step 411 | loss_total 0.4878\n",
      "step 1/3 | epoch 7/50 | batch 52/60 | global_step 412 | loss_total 0.4833\n",
      "step 1/3 | epoch 7/50 | batch 53/60 | global_step 413 | loss_total 0.4688\n",
      "step 1/3 | epoch 7/50 | batch 54/60 | global_step 414 | loss_total 0.4941\n",
      "step 1/3 | epoch 7/50 | batch 55/60 | global_step 415 | loss_total 0.4277\n",
      "step 1/3 | epoch 7/50 | batch 56/60 | global_step 416 | loss_total 0.5674\n",
      "step 1/3 | epoch 7/50 | batch 57/60 | global_step 417 | loss_total 0.4371\n",
      "step 1/3 | epoch 7/50 | batch 58/60 | global_step 418 | loss_total 0.3409\n",
      "step 1/3 | epoch 7/50 | batch 59/60 | global_step 419 | loss_total 0.5854\n",
      "step 1/3 | epoch 7/50 | batch 60/60 | global_step 420 | loss_total 0.5403\n",
      "[epoch done] step 1/3 epoch 7/50 | train_total=0.5171 val_total=0.3045\n",
      "step 1/3 | epoch 8/50 | batch 1/60 | global_step 421 | loss_total 0.5749\n",
      "step 1/3 | epoch 8/50 | batch 2/60 | global_step 422 | loss_total 0.5150\n",
      "step 1/3 | epoch 8/50 | batch 3/60 | global_step 423 | loss_total 0.4786\n",
      "step 1/3 | epoch 8/50 | batch 4/60 | global_step 424 | loss_total 0.5612\n",
      "step 1/3 | epoch 8/50 | batch 5/60 | global_step 425 | loss_total 0.6179\n",
      "step 1/3 | epoch 8/50 | batch 6/60 | global_step 426 | loss_total 0.5091\n",
      "step 1/3 | epoch 8/50 | batch 7/60 | global_step 427 | loss_total 0.5637\n",
      "step 1/3 | epoch 8/50 | batch 8/60 | global_step 428 | loss_total 0.3465\n",
      "step 1/3 | epoch 8/50 | batch 9/60 | global_step 429 | loss_total 0.3823\n",
      "step 1/3 | epoch 8/50 | batch 10/60 | global_step 430 | loss_total 0.6471\n",
      "step 1/3 | epoch 8/50 | batch 11/60 | global_step 431 | loss_total 0.6164\n",
      "step 1/3 | epoch 8/50 | batch 12/60 | global_step 432 | loss_total 0.4195\n",
      "step 1/3 | epoch 8/50 | batch 13/60 | global_step 433 | loss_total 0.4890\n",
      "step 1/3 | epoch 8/50 | batch 14/60 | global_step 434 | loss_total 0.6452\n",
      "step 1/3 | epoch 8/50 | batch 15/60 | global_step 435 | loss_total 0.3738\n",
      "step 1/3 | epoch 8/50 | batch 16/60 | global_step 436 | loss_total 0.3683\n",
      "step 1/3 | epoch 8/50 | batch 17/60 | global_step 437 | loss_total 0.4341\n",
      "step 1/3 | epoch 8/50 | batch 18/60 | global_step 438 | loss_total 0.5222\n",
      "step 1/3 | epoch 8/50 | batch 19/60 | global_step 439 | loss_total 0.3523\n",
      "step 1/3 | epoch 8/50 | batch 20/60 | global_step 440 | loss_total 0.3078\n",
      "step 1/3 | epoch 8/50 | batch 21/60 | global_step 441 | loss_total 0.7221\n",
      "step 1/3 | epoch 8/50 | batch 22/60 | global_step 442 | loss_total 0.3224\n",
      "step 1/3 | epoch 8/50 | batch 23/60 | global_step 443 | loss_total 0.4262\n",
      "step 1/3 | epoch 8/50 | batch 24/60 | global_step 444 | loss_total 0.6138\n",
      "step 1/3 | epoch 8/50 | batch 25/60 | global_step 445 | loss_total 0.7594\n",
      "step 1/3 | epoch 8/50 | batch 26/60 | global_step 446 | loss_total 0.4626\n",
      "step 1/3 | epoch 8/50 | batch 27/60 | global_step 447 | loss_total 0.3563\n",
      "step 1/3 | epoch 8/50 | batch 28/60 | global_step 448 | loss_total 0.5816\n",
      "step 1/3 | epoch 8/50 | batch 29/60 | global_step 449 | loss_total 0.4276\n",
      "step 1/3 | epoch 8/50 | batch 30/60 | global_step 450 | loss_total 0.5276\n",
      "step 1/3 | epoch 8/50 | batch 31/60 | global_step 451 | loss_total 0.6274\n",
      "step 1/3 | epoch 8/50 | batch 32/60 | global_step 452 | loss_total 0.3580\n",
      "step 1/3 | epoch 8/50 | batch 33/60 | global_step 453 | loss_total 0.3976\n",
      "step 1/3 | epoch 8/50 | batch 34/60 | global_step 454 | loss_total 0.4474\n",
      "step 1/3 | epoch 8/50 | batch 35/60 | global_step 455 | loss_total 0.4890\n",
      "step 1/3 | epoch 8/50 | batch 36/60 | global_step 456 | loss_total 0.3211\n",
      "step 1/3 | epoch 8/50 | batch 37/60 | global_step 457 | loss_total 0.6471\n",
      "step 1/3 | epoch 8/50 | batch 38/60 | global_step 458 | loss_total 0.3648\n",
      "step 1/3 | epoch 8/50 | batch 39/60 | global_step 459 | loss_total 0.6196\n",
      "step 1/3 | epoch 8/50 | batch 40/60 | global_step 460 | loss_total 0.4463\n",
      "step 1/3 | epoch 8/50 | batch 41/60 | global_step 461 | loss_total 0.6985\n",
      "step 1/3 | epoch 8/50 | batch 42/60 | global_step 462 | loss_total 0.4604\n",
      "step 1/3 | epoch 8/50 | batch 43/60 | global_step 463 | loss_total 0.3390\n",
      "step 1/3 | epoch 8/50 | batch 44/60 | global_step 464 | loss_total 0.3131\n",
      "step 1/3 | epoch 8/50 | batch 45/60 | global_step 465 | loss_total 0.5199\n",
      "step 1/3 | epoch 8/50 | batch 46/60 | global_step 466 | loss_total 0.8145\n",
      "step 1/3 | epoch 8/50 | batch 47/60 | global_step 467 | loss_total 0.6307\n",
      "step 1/3 | epoch 8/50 | batch 48/60 | global_step 468 | loss_total 0.3292\n",
      "step 1/3 | epoch 8/50 | batch 49/60 | global_step 469 | loss_total 0.3518\n",
      "step 1/3 | epoch 8/50 | batch 50/60 | global_step 470 | loss_total 0.5773\n",
      "step 1/3 | epoch 8/50 | batch 51/60 | global_step 471 | loss_total 0.4345\n",
      "step 1/3 | epoch 8/50 | batch 52/60 | global_step 472 | loss_total 0.4355\n",
      "step 1/3 | epoch 8/50 | batch 53/60 | global_step 473 | loss_total 0.4724\n",
      "step 1/3 | epoch 8/50 | batch 54/60 | global_step 474 | loss_total 0.4158\n",
      "step 1/3 | epoch 8/50 | batch 55/60 | global_step 475 | loss_total 0.4747\n",
      "step 1/3 | epoch 8/50 | batch 56/60 | global_step 476 | loss_total 0.7519\n",
      "step 1/3 | epoch 8/50 | batch 57/60 | global_step 477 | loss_total 0.4792\n",
      "step 1/3 | epoch 8/50 | batch 58/60 | global_step 478 | loss_total 0.5346\n",
      "step 1/3 | epoch 8/50 | batch 59/60 | global_step 479 | loss_total 0.4677\n",
      "step 1/3 | epoch 8/50 | batch 60/60 | global_step 480 | loss_total 0.6735\n",
      "[epoch done] step 1/3 epoch 8/50 | train_total=0.4970 val_total=0.3308\n",
      "step 1/3 | epoch 9/50 | batch 1/60 | global_step 481 | loss_total 0.8191\n",
      "step 1/3 | epoch 9/50 | batch 2/60 | global_step 482 | loss_total 0.5911\n",
      "step 1/3 | epoch 9/50 | batch 3/60 | global_step 483 | loss_total 0.4333\n",
      "step 1/3 | epoch 9/50 | batch 4/60 | global_step 484 | loss_total 0.5032\n",
      "step 1/3 | epoch 9/50 | batch 5/60 | global_step 485 | loss_total 0.4033\n",
      "step 1/3 | epoch 9/50 | batch 6/60 | global_step 486 | loss_total 0.6495\n",
      "step 1/3 | epoch 9/50 | batch 7/60 | global_step 487 | loss_total 0.3557\n",
      "step 1/3 | epoch 9/50 | batch 8/60 | global_step 488 | loss_total 0.6303\n",
      "step 1/3 | epoch 9/50 | batch 9/60 | global_step 489 | loss_total 0.5218\n",
      "step 1/3 | epoch 9/50 | batch 10/60 | global_step 490 | loss_total 0.5548\n",
      "step 1/3 | epoch 9/50 | batch 11/60 | global_step 491 | loss_total 0.3879\n",
      "step 1/3 | epoch 9/50 | batch 12/60 | global_step 492 | loss_total 0.4455\n",
      "step 1/3 | epoch 9/50 | batch 13/60 | global_step 493 | loss_total 0.4318\n",
      "step 1/3 | epoch 9/50 | batch 14/60 | global_step 494 | loss_total 0.7394\n",
      "step 1/3 | epoch 9/50 | batch 15/60 | global_step 495 | loss_total 0.4336\n",
      "step 1/3 | epoch 9/50 | batch 16/60 | global_step 496 | loss_total 0.4268\n",
      "step 1/3 | epoch 9/50 | batch 17/60 | global_step 497 | loss_total 0.4028\n",
      "step 1/3 | epoch 9/50 | batch 18/60 | global_step 498 | loss_total 0.3453\n",
      "step 1/3 | epoch 9/50 | batch 19/60 | global_step 499 | loss_total 0.5711\n",
      "step 1/3 | epoch 9/50 | batch 20/60 | global_step 500 | loss_total 0.4808\n",
      "step 1/3 | epoch 9/50 | batch 21/60 | global_step 501 | loss_total 0.4849\n",
      "step 1/3 | epoch 9/50 | batch 22/60 | global_step 502 | loss_total 0.3432\n",
      "step 1/3 | epoch 9/50 | batch 23/60 | global_step 503 | loss_total 0.4462\n",
      "step 1/3 | epoch 9/50 | batch 24/60 | global_step 504 | loss_total 0.4672\n",
      "step 1/3 | epoch 9/50 | batch 25/60 | global_step 505 | loss_total 0.3857\n",
      "step 1/3 | epoch 9/50 | batch 26/60 | global_step 506 | loss_total 0.5863\n",
      "step 1/3 | epoch 9/50 | batch 27/60 | global_step 507 | loss_total 0.4023\n",
      "step 1/3 | epoch 9/50 | batch 28/60 | global_step 508 | loss_total 0.6302\n",
      "step 1/3 | epoch 9/50 | batch 29/60 | global_step 509 | loss_total 0.6215\n",
      "step 1/3 | epoch 9/50 | batch 30/60 | global_step 510 | loss_total 0.6285\n",
      "step 1/3 | epoch 9/50 | batch 31/60 | global_step 511 | loss_total 0.5269\n",
      "step 1/3 | epoch 9/50 | batch 32/60 | global_step 512 | loss_total 0.4452\n",
      "step 1/3 | epoch 9/50 | batch 33/60 | global_step 513 | loss_total 1.0784\n",
      "step 1/3 | epoch 9/50 | batch 34/60 | global_step 514 | loss_total 0.5837\n",
      "step 1/3 | epoch 9/50 | batch 35/60 | global_step 515 | loss_total 0.5187\n",
      "step 1/3 | epoch 9/50 | batch 36/60 | global_step 516 | loss_total 0.4575\n",
      "step 1/3 | epoch 9/50 | batch 37/60 | global_step 517 | loss_total 0.4285\n",
      "step 1/3 | epoch 9/50 | batch 38/60 | global_step 518 | loss_total 0.6165\n",
      "step 1/3 | epoch 9/50 | batch 39/60 | global_step 519 | loss_total 0.4984\n",
      "step 1/3 | epoch 9/50 | batch 40/60 | global_step 520 | loss_total 0.8353\n",
      "step 1/3 | epoch 9/50 | batch 41/60 | global_step 521 | loss_total 0.3612\n",
      "step 1/3 | epoch 9/50 | batch 42/60 | global_step 522 | loss_total 0.5734\n",
      "step 1/3 | epoch 9/50 | batch 43/60 | global_step 523 | loss_total 0.7912\n",
      "step 1/3 | epoch 9/50 | batch 44/60 | global_step 524 | loss_total 0.3365\n",
      "step 1/3 | epoch 9/50 | batch 45/60 | global_step 525 | loss_total 0.5638\n",
      "step 1/3 | epoch 9/50 | batch 46/60 | global_step 526 | loss_total 0.3543\n",
      "step 1/3 | epoch 9/50 | batch 47/60 | global_step 527 | loss_total 0.3803\n",
      "step 1/3 | epoch 9/50 | batch 48/60 | global_step 528 | loss_total 0.3345\n",
      "step 1/3 | epoch 9/50 | batch 49/60 | global_step 529 | loss_total 0.3343\n",
      "step 1/3 | epoch 9/50 | batch 50/60 | global_step 530 | loss_total 0.5616\n",
      "step 1/3 | epoch 9/50 | batch 51/60 | global_step 531 | loss_total 0.6549\n",
      "step 1/3 | epoch 9/50 | batch 52/60 | global_step 532 | loss_total 0.6249\n",
      "step 1/3 | epoch 9/50 | batch 53/60 | global_step 533 | loss_total 0.4387\n",
      "step 1/3 | epoch 9/50 | batch 54/60 | global_step 534 | loss_total 0.5024\n",
      "step 1/3 | epoch 9/50 | batch 55/60 | global_step 535 | loss_total 0.5382\n",
      "step 1/3 | epoch 9/50 | batch 56/60 | global_step 536 | loss_total 0.5432\n",
      "step 1/3 | epoch 9/50 | batch 57/60 | global_step 537 | loss_total 0.3058\n",
      "step 1/3 | epoch 9/50 | batch 58/60 | global_step 538 | loss_total 0.7801\n",
      "step 1/3 | epoch 9/50 | batch 59/60 | global_step 539 | loss_total 0.5542\n",
      "step 1/3 | epoch 9/50 | batch 60/60 | global_step 540 | loss_total 0.3763\n",
      "[epoch done] step 1/3 epoch 9/50 | train_total=0.5170 val_total=0.3178\n",
      "step 1/3 | epoch 10/50 | batch 1/60 | global_step 541 | loss_total 0.3064\n",
      "step 1/3 | epoch 10/50 | batch 2/60 | global_step 542 | loss_total 0.6621\n",
      "step 1/3 | epoch 10/50 | batch 3/60 | global_step 543 | loss_total 0.3767\n",
      "step 1/3 | epoch 10/50 | batch 4/60 | global_step 544 | loss_total 0.5077\n",
      "step 1/3 | epoch 10/50 | batch 5/60 | global_step 545 | loss_total 0.5905\n",
      "step 1/3 | epoch 10/50 | batch 6/60 | global_step 546 | loss_total 0.3310\n",
      "step 1/3 | epoch 10/50 | batch 7/60 | global_step 547 | loss_total 0.2968\n",
      "step 1/3 | epoch 10/50 | batch 8/60 | global_step 548 | loss_total 0.6379\n",
      "step 1/3 | epoch 10/50 | batch 9/60 | global_step 549 | loss_total 0.5729\n",
      "step 1/3 | epoch 10/50 | batch 10/60 | global_step 550 | loss_total 0.6589\n",
      "step 1/3 | epoch 10/50 | batch 11/60 | global_step 551 | loss_total 0.6068\n",
      "step 1/3 | epoch 10/50 | batch 12/60 | global_step 552 | loss_total 0.5541\n",
      "step 1/3 | epoch 10/50 | batch 13/60 | global_step 553 | loss_total 0.5836\n",
      "step 1/3 | epoch 10/50 | batch 14/60 | global_step 554 | loss_total 0.3151\n",
      "step 1/3 | epoch 10/50 | batch 15/60 | global_step 555 | loss_total 0.6788\n",
      "step 1/3 | epoch 10/50 | batch 16/60 | global_step 556 | loss_total 0.4323\n",
      "step 1/3 | epoch 10/50 | batch 17/60 | global_step 557 | loss_total 0.3557\n",
      "step 1/3 | epoch 10/50 | batch 18/60 | global_step 558 | loss_total 0.5650\n",
      "step 1/3 | epoch 10/50 | batch 19/60 | global_step 559 | loss_total 0.7112\n",
      "step 1/3 | epoch 10/50 | batch 20/60 | global_step 560 | loss_total 0.4612\n",
      "step 1/3 | epoch 10/50 | batch 21/60 | global_step 561 | loss_total 0.4257\n",
      "step 1/3 | epoch 10/50 | batch 22/60 | global_step 562 | loss_total 0.4929\n",
      "step 1/3 | epoch 10/50 | batch 23/60 | global_step 563 | loss_total 0.5362\n",
      "step 1/3 | epoch 10/50 | batch 24/60 | global_step 564 | loss_total 0.5645\n",
      "step 1/3 | epoch 10/50 | batch 25/60 | global_step 565 | loss_total 0.6042\n",
      "step 1/3 | epoch 10/50 | batch 26/60 | global_step 566 | loss_total 0.4642\n",
      "step 1/3 | epoch 10/50 | batch 27/60 | global_step 567 | loss_total 0.5083\n",
      "step 1/3 | epoch 10/50 | batch 28/60 | global_step 568 | loss_total 0.3803\n",
      "step 1/3 | epoch 10/50 | batch 29/60 | global_step 569 | loss_total 0.6648\n",
      "step 1/3 | epoch 10/50 | batch 30/60 | global_step 570 | loss_total 0.4102\n",
      "step 1/3 | epoch 10/50 | batch 31/60 | global_step 571 | loss_total 0.3328\n",
      "step 1/3 | epoch 10/50 | batch 32/60 | global_step 572 | loss_total 0.4020\n",
      "step 1/3 | epoch 10/50 | batch 33/60 | global_step 573 | loss_total 0.4612\n",
      "step 1/3 | epoch 10/50 | batch 34/60 | global_step 574 | loss_total 0.4683\n",
      "step 1/3 | epoch 10/50 | batch 35/60 | global_step 575 | loss_total 0.4127\n",
      "step 1/3 | epoch 10/50 | batch 36/60 | global_step 576 | loss_total 0.6089\n",
      "step 1/3 | epoch 10/50 | batch 37/60 | global_step 577 | loss_total 0.3749\n",
      "step 1/3 | epoch 10/50 | batch 38/60 | global_step 578 | loss_total 0.9072\n",
      "step 1/3 | epoch 10/50 | batch 39/60 | global_step 579 | loss_total 0.3308\n",
      "step 1/3 | epoch 10/50 | batch 40/60 | global_step 580 | loss_total 0.5274\n",
      "step 1/3 | epoch 10/50 | batch 41/60 | global_step 581 | loss_total 0.4351\n",
      "step 1/3 | epoch 10/50 | batch 42/60 | global_step 582 | loss_total 0.4107\n",
      "step 1/3 | epoch 10/50 | batch 43/60 | global_step 583 | loss_total 0.5452\n",
      "step 1/3 | epoch 10/50 | batch 44/60 | global_step 584 | loss_total 0.4228\n",
      "step 1/3 | epoch 10/50 | batch 45/60 | global_step 585 | loss_total 0.2906\n",
      "step 1/3 | epoch 10/50 | batch 46/60 | global_step 586 | loss_total 0.4873\n",
      "step 1/3 | epoch 10/50 | batch 47/60 | global_step 587 | loss_total 0.4523\n",
      "step 1/3 | epoch 10/50 | batch 48/60 | global_step 588 | loss_total 0.3631\n",
      "step 1/3 | epoch 10/50 | batch 49/60 | global_step 589 | loss_total 0.6600\n",
      "step 1/3 | epoch 10/50 | batch 50/60 | global_step 590 | loss_total 0.8229\n",
      "step 1/3 | epoch 10/50 | batch 51/60 | global_step 591 | loss_total 0.3197\n",
      "step 1/3 | epoch 10/50 | batch 52/60 | global_step 592 | loss_total 0.5271\n",
      "step 1/3 | epoch 10/50 | batch 53/60 | global_step 593 | loss_total 0.4656\n",
      "step 1/3 | epoch 10/50 | batch 54/60 | global_step 594 | loss_total 0.4672\n",
      "step 1/3 | epoch 10/50 | batch 55/60 | global_step 595 | loss_total 0.4649\n",
      "step 1/3 | epoch 10/50 | batch 56/60 | global_step 596 | loss_total 0.3716\n",
      "step 1/3 | epoch 10/50 | batch 57/60 | global_step 597 | loss_total 0.4741\n",
      "step 1/3 | epoch 10/50 | batch 58/60 | global_step 598 | loss_total 0.4666\n",
      "step 1/3 | epoch 10/50 | batch 59/60 | global_step 599 | loss_total 0.3734\n",
      "step 1/3 | epoch 10/50 | batch 60/60 | global_step 600 | loss_total 0.3261\n",
      "[epoch done] step 1/3 epoch 10/50 | train_total=0.4871 val_total=0.3232\n",
      "step 1/3 | epoch 11/50 | batch 1/60 | global_step 601 | loss_total 0.5292\n",
      "step 1/3 | epoch 11/50 | batch 2/60 | global_step 602 | loss_total 0.6791\n",
      "step 1/3 | epoch 11/50 | batch 3/60 | global_step 603 | loss_total 0.2646\n",
      "step 1/3 | epoch 11/50 | batch 4/60 | global_step 604 | loss_total 0.3683\n",
      "step 1/3 | epoch 11/50 | batch 5/60 | global_step 605 | loss_total 0.3003\n",
      "step 1/3 | epoch 11/50 | batch 6/60 | global_step 606 | loss_total 0.3234\n",
      "step 1/3 | epoch 11/50 | batch 7/60 | global_step 607 | loss_total 0.3522\n",
      "step 1/3 | epoch 11/50 | batch 8/60 | global_step 608 | loss_total 0.5910\n",
      "step 1/3 | epoch 11/50 | batch 9/60 | global_step 609 | loss_total 0.3662\n",
      "step 1/3 | epoch 11/50 | batch 10/60 | global_step 610 | loss_total 0.5827\n",
      "step 1/3 | epoch 11/50 | batch 11/60 | global_step 611 | loss_total 1.2075\n",
      "step 1/3 | epoch 11/50 | batch 12/60 | global_step 612 | loss_total 0.7073\n",
      "step 1/3 | epoch 11/50 | batch 13/60 | global_step 613 | loss_total 0.5710\n",
      "step 1/3 | epoch 11/50 | batch 14/60 | global_step 614 | loss_total 0.6163\n",
      "step 1/3 | epoch 11/50 | batch 15/60 | global_step 615 | loss_total 0.3498\n",
      "step 1/3 | epoch 11/50 | batch 16/60 | global_step 616 | loss_total 0.3105\n",
      "step 1/3 | epoch 11/50 | batch 17/60 | global_step 617 | loss_total 0.8167\n",
      "step 1/3 | epoch 11/50 | batch 18/60 | global_step 618 | loss_total 0.4165\n",
      "step 1/3 | epoch 11/50 | batch 19/60 | global_step 619 | loss_total 0.3676\n",
      "step 1/3 | epoch 11/50 | batch 20/60 | global_step 620 | loss_total 0.5420\n",
      "step 1/3 | epoch 11/50 | batch 21/60 | global_step 621 | loss_total 0.3487\n",
      "step 1/3 | epoch 11/50 | batch 22/60 | global_step 622 | loss_total 0.3717\n",
      "step 1/3 | epoch 11/50 | batch 23/60 | global_step 623 | loss_total 0.4867\n",
      "step 1/3 | epoch 11/50 | batch 24/60 | global_step 624 | loss_total 0.3177\n",
      "step 1/3 | epoch 11/50 | batch 25/60 | global_step 625 | loss_total 0.4911\n",
      "step 1/3 | epoch 11/50 | batch 26/60 | global_step 626 | loss_total 0.3172\n",
      "step 1/3 | epoch 11/50 | batch 27/60 | global_step 627 | loss_total 0.3022\n",
      "step 1/3 | epoch 11/50 | batch 28/60 | global_step 628 | loss_total 0.6407\n",
      "step 1/3 | epoch 11/50 | batch 29/60 | global_step 629 | loss_total 0.5169\n",
      "step 1/3 | epoch 11/50 | batch 30/60 | global_step 630 | loss_total 0.3390\n",
      "step 1/3 | epoch 11/50 | batch 31/60 | global_step 631 | loss_total 0.6761\n",
      "step 1/3 | epoch 11/50 | batch 32/60 | global_step 632 | loss_total 0.8324\n",
      "step 1/3 | epoch 11/50 | batch 33/60 | global_step 633 | loss_total 0.6078\n",
      "step 1/3 | epoch 11/50 | batch 34/60 | global_step 634 | loss_total 0.4011\n",
      "step 1/3 | epoch 11/50 | batch 35/60 | global_step 635 | loss_total 0.4801\n",
      "step 1/3 | epoch 11/50 | batch 36/60 | global_step 636 | loss_total 0.4778\n",
      "step 1/3 | epoch 11/50 | batch 37/60 | global_step 637 | loss_total 0.3100\n",
      "step 1/3 | epoch 11/50 | batch 38/60 | global_step 638 | loss_total 0.5948\n",
      "step 1/3 | epoch 11/50 | batch 39/60 | global_step 639 | loss_total 0.3549\n",
      "step 1/3 | epoch 11/50 | batch 40/60 | global_step 640 | loss_total 0.5112\n",
      "step 1/3 | epoch 11/50 | batch 41/60 | global_step 641 | loss_total 0.4848\n",
      "step 1/3 | epoch 11/50 | batch 42/60 | global_step 642 | loss_total 0.5422\n",
      "step 1/3 | epoch 11/50 | batch 43/60 | global_step 643 | loss_total 0.3827\n",
      "step 1/3 | epoch 11/50 | batch 44/60 | global_step 644 | loss_total 0.6596\n",
      "step 1/3 | epoch 11/50 | batch 45/60 | global_step 645 | loss_total 0.3305\n",
      "step 1/3 | epoch 11/50 | batch 46/60 | global_step 646 | loss_total 0.5454\n",
      "step 1/3 | epoch 11/50 | batch 47/60 | global_step 647 | loss_total 0.3922\n",
      "step 1/3 | epoch 11/50 | batch 48/60 | global_step 648 | loss_total 0.4012\n",
      "step 1/3 | epoch 11/50 | batch 49/60 | global_step 649 | loss_total 0.4259\n",
      "step 1/3 | epoch 11/50 | batch 50/60 | global_step 650 | loss_total 0.4294\n",
      "step 1/3 | epoch 11/50 | batch 51/60 | global_step 651 | loss_total 0.6650\n",
      "step 1/3 | epoch 11/50 | batch 52/60 | global_step 652 | loss_total 0.5291\n",
      "step 1/3 | epoch 11/50 | batch 53/60 | global_step 653 | loss_total 0.3598\n",
      "step 1/3 | epoch 11/50 | batch 54/60 | global_step 654 | loss_total 0.3088\n",
      "step 1/3 | epoch 11/50 | batch 55/60 | global_step 655 | loss_total 0.3847\n",
      "step 1/3 | epoch 11/50 | batch 56/60 | global_step 656 | loss_total 0.3547\n",
      "step 1/3 | epoch 11/50 | batch 57/60 | global_step 657 | loss_total 0.5792\n",
      "step 1/3 | epoch 11/50 | batch 58/60 | global_step 658 | loss_total 0.4539\n",
      "step 1/3 | epoch 11/50 | batch 59/60 | global_step 659 | loss_total 0.4326\n",
      "step 1/3 | epoch 11/50 | batch 60/60 | global_step 660 | loss_total 0.8460\n",
      "[epoch done] step 1/3 epoch 11/50 | train_total=0.4858 val_total=0.3033\n",
      "step 1/3 | epoch 12/50 | batch 1/60 | global_step 661 | loss_total 0.4986\n",
      "step 1/3 | epoch 12/50 | batch 2/60 | global_step 662 | loss_total 0.3092\n",
      "step 1/3 | epoch 12/50 | batch 3/60 | global_step 663 | loss_total 0.4670\n",
      "step 1/3 | epoch 12/50 | batch 4/60 | global_step 664 | loss_total 0.6022\n",
      "step 1/3 | epoch 12/50 | batch 5/60 | global_step 665 | loss_total 0.2897\n",
      "step 1/3 | epoch 12/50 | batch 6/60 | global_step 666 | loss_total 0.5318\n",
      "step 1/3 | epoch 12/50 | batch 7/60 | global_step 667 | loss_total 0.3156\n",
      "step 1/3 | epoch 12/50 | batch 8/60 | global_step 668 | loss_total 0.5659\n",
      "step 1/3 | epoch 12/50 | batch 9/60 | global_step 669 | loss_total 0.3344\n",
      "step 1/3 | epoch 12/50 | batch 10/60 | global_step 670 | loss_total 0.4498\n",
      "step 1/3 | epoch 12/50 | batch 11/60 | global_step 671 | loss_total 0.6710\n",
      "step 1/3 | epoch 12/50 | batch 12/60 | global_step 672 | loss_total 0.4042\n",
      "step 1/3 | epoch 12/50 | batch 13/60 | global_step 673 | loss_total 0.4305\n",
      "step 1/3 | epoch 12/50 | batch 14/60 | global_step 674 | loss_total 0.4193\n",
      "step 1/3 | epoch 12/50 | batch 15/60 | global_step 675 | loss_total 0.3563\n",
      "step 1/3 | epoch 12/50 | batch 16/60 | global_step 676 | loss_total 0.3083\n",
      "step 1/3 | epoch 12/50 | batch 17/60 | global_step 677 | loss_total 0.5711\n",
      "step 1/3 | epoch 12/50 | batch 18/60 | global_step 678 | loss_total 0.7301\n",
      "step 1/3 | epoch 12/50 | batch 19/60 | global_step 679 | loss_total 0.3276\n",
      "step 1/3 | epoch 12/50 | batch 20/60 | global_step 680 | loss_total 0.3842\n",
      "step 1/3 | epoch 12/50 | batch 21/60 | global_step 681 | loss_total 0.2999\n",
      "step 1/3 | epoch 12/50 | batch 22/60 | global_step 682 | loss_total 0.2854\n",
      "step 1/3 | epoch 12/50 | batch 23/60 | global_step 683 | loss_total 0.2951\n",
      "step 1/3 | epoch 12/50 | batch 24/60 | global_step 684 | loss_total 0.3089\n",
      "step 1/3 | epoch 12/50 | batch 25/60 | global_step 685 | loss_total 0.2751\n",
      "step 1/3 | epoch 12/50 | batch 26/60 | global_step 686 | loss_total 0.2790\n",
      "step 1/3 | epoch 12/50 | batch 27/60 | global_step 687 | loss_total 0.5532\n",
      "step 1/3 | epoch 12/50 | batch 28/60 | global_step 688 | loss_total 1.0500\n",
      "step 1/3 | epoch 12/50 | batch 29/60 | global_step 689 | loss_total 0.2639\n",
      "step 1/3 | epoch 12/50 | batch 30/60 | global_step 690 | loss_total 0.4780\n",
      "step 1/3 | epoch 12/50 | batch 31/60 | global_step 691 | loss_total 0.3271\n",
      "step 1/3 | epoch 12/50 | batch 32/60 | global_step 692 | loss_total 0.4582\n",
      "step 1/3 | epoch 12/50 | batch 33/60 | global_step 693 | loss_total 0.2633\n",
      "step 1/3 | epoch 12/50 | batch 34/60 | global_step 694 | loss_total 0.7089\n",
      "step 1/3 | epoch 12/50 | batch 35/60 | global_step 695 | loss_total 0.3772\n",
      "step 1/3 | epoch 12/50 | batch 36/60 | global_step 696 | loss_total 0.8820\n",
      "step 1/3 | epoch 12/50 | batch 37/60 | global_step 697 | loss_total 0.5382\n",
      "step 1/3 | epoch 12/50 | batch 38/60 | global_step 698 | loss_total 0.5017\n",
      "step 1/3 | epoch 12/50 | batch 39/60 | global_step 699 | loss_total 0.2700\n",
      "step 1/3 | epoch 12/50 | batch 40/60 | global_step 700 | loss_total 0.7700\n",
      "step 1/3 | epoch 12/50 | batch 41/60 | global_step 701 | loss_total 0.4935\n",
      "step 1/3 | epoch 12/50 | batch 42/60 | global_step 702 | loss_total 0.2892\n",
      "step 1/3 | epoch 12/50 | batch 43/60 | global_step 703 | loss_total 0.8871\n",
      "step 1/3 | epoch 12/50 | batch 44/60 | global_step 704 | loss_total 0.5139\n",
      "step 1/3 | epoch 12/50 | batch 45/60 | global_step 705 | loss_total 0.9089\n",
      "step 1/3 | epoch 12/50 | batch 46/60 | global_step 706 | loss_total 0.3021\n",
      "step 1/3 | epoch 12/50 | batch 47/60 | global_step 707 | loss_total 0.6126\n",
      "step 1/3 | epoch 12/50 | batch 48/60 | global_step 708 | loss_total 0.5194\n",
      "step 1/3 | epoch 12/50 | batch 49/60 | global_step 709 | loss_total 0.5444\n",
      "step 1/3 | epoch 12/50 | batch 50/60 | global_step 710 | loss_total 0.3675\n",
      "step 1/3 | epoch 12/50 | batch 51/60 | global_step 711 | loss_total 0.5068\n",
      "step 1/3 | epoch 12/50 | batch 52/60 | global_step 712 | loss_total 0.4951\n",
      "step 1/3 | epoch 12/50 | batch 53/60 | global_step 713 | loss_total 0.5247\n",
      "step 1/3 | epoch 12/50 | batch 54/60 | global_step 714 | loss_total 0.4825\n",
      "step 1/3 | epoch 12/50 | batch 55/60 | global_step 715 | loss_total 0.2830\n",
      "step 1/3 | epoch 12/50 | batch 56/60 | global_step 716 | loss_total 0.4946\n",
      "step 1/3 | epoch 12/50 | batch 57/60 | global_step 717 | loss_total 0.5043\n",
      "step 1/3 | epoch 12/50 | batch 58/60 | global_step 718 | loss_total 0.4882\n",
      "step 1/3 | epoch 12/50 | batch 59/60 | global_step 719 | loss_total 0.4187\n",
      "step 1/3 | epoch 12/50 | batch 60/60 | global_step 720 | loss_total 0.3388\n",
      "[epoch done] step 1/3 epoch 12/50 | train_total=0.4688 val_total=0.2380\n",
      "step 1/3 | epoch 13/50 | batch 1/60 | global_step 721 | loss_total 0.3977\n",
      "step 1/3 | epoch 13/50 | batch 2/60 | global_step 722 | loss_total 0.5287\n",
      "step 1/3 | epoch 13/50 | batch 3/60 | global_step 723 | loss_total 0.6090\n",
      "step 1/3 | epoch 13/50 | batch 4/60 | global_step 724 | loss_total 0.7022\n",
      "step 1/3 | epoch 13/50 | batch 5/60 | global_step 725 | loss_total 0.5537\n",
      "step 1/3 | epoch 13/50 | batch 6/60 | global_step 726 | loss_total 0.4277\n",
      "step 1/3 | epoch 13/50 | batch 7/60 | global_step 727 | loss_total 0.5893\n",
      "step 1/3 | epoch 13/50 | batch 8/60 | global_step 728 | loss_total 0.5174\n",
      "step 1/3 | epoch 13/50 | batch 9/60 | global_step 729 | loss_total 0.3929\n",
      "step 1/3 | epoch 13/50 | batch 10/60 | global_step 730 | loss_total 0.5059\n",
      "step 1/3 | epoch 13/50 | batch 11/60 | global_step 731 | loss_total 0.4846\n",
      "step 1/3 | epoch 13/50 | batch 12/60 | global_step 732 | loss_total 0.2904\n",
      "step 1/3 | epoch 13/50 | batch 13/60 | global_step 733 | loss_total 0.5574\n",
      "step 1/3 | epoch 13/50 | batch 14/60 | global_step 734 | loss_total 0.7840\n",
      "step 1/3 | epoch 13/50 | batch 15/60 | global_step 735 | loss_total 0.8023\n",
      "step 1/3 | epoch 13/50 | batch 16/60 | global_step 736 | loss_total 0.5586\n",
      "step 1/3 | epoch 13/50 | batch 17/60 | global_step 737 | loss_total 0.6780\n",
      "step 1/3 | epoch 13/50 | batch 18/60 | global_step 738 | loss_total 0.4386\n",
      "step 1/3 | epoch 13/50 | batch 19/60 | global_step 739 | loss_total 0.3144\n",
      "step 1/3 | epoch 13/50 | batch 20/60 | global_step 740 | loss_total 0.5780\n",
      "step 1/3 | epoch 13/50 | batch 21/60 | global_step 741 | loss_total 0.3586\n",
      "step 1/3 | epoch 13/50 | batch 22/60 | global_step 742 | loss_total 0.3291\n",
      "step 1/3 | epoch 13/50 | batch 23/60 | global_step 743 | loss_total 0.4955\n",
      "step 1/3 | epoch 13/50 | batch 24/60 | global_step 744 | loss_total 0.4184\n",
      "step 1/3 | epoch 13/50 | batch 25/60 | global_step 745 | loss_total 0.5413\n",
      "step 1/3 | epoch 13/50 | batch 26/60 | global_step 746 | loss_total 0.5105\n",
      "step 1/3 | epoch 13/50 | batch 27/60 | global_step 747 | loss_total 0.3558\n",
      "step 1/3 | epoch 13/50 | batch 28/60 | global_step 748 | loss_total 0.3550\n",
      "step 1/3 | epoch 13/50 | batch 29/60 | global_step 749 | loss_total 0.4111\n",
      "step 1/3 | epoch 13/50 | batch 30/60 | global_step 750 | loss_total 0.3754\n",
      "step 1/3 | epoch 13/50 | batch 31/60 | global_step 751 | loss_total 0.4294\n",
      "step 1/3 | epoch 13/50 | batch 32/60 | global_step 752 | loss_total 0.3421\n",
      "step 1/3 | epoch 13/50 | batch 33/60 | global_step 753 | loss_total 0.3040\n",
      "step 1/3 | epoch 13/50 | batch 34/60 | global_step 754 | loss_total 0.5564\n",
      "step 1/3 | epoch 13/50 | batch 35/60 | global_step 755 | loss_total 0.5805\n",
      "step 1/3 | epoch 13/50 | batch 36/60 | global_step 756 | loss_total 0.3429\n",
      "step 1/3 | epoch 13/50 | batch 37/60 | global_step 757 | loss_total 0.4714\n",
      "step 1/3 | epoch 13/50 | batch 38/60 | global_step 758 | loss_total 0.4429\n",
      "step 1/3 | epoch 13/50 | batch 39/60 | global_step 759 | loss_total 0.4192\n",
      "step 1/3 | epoch 13/50 | batch 40/60 | global_step 760 | loss_total 0.4516\n",
      "step 1/3 | epoch 13/50 | batch 41/60 | global_step 761 | loss_total 0.3561\n",
      "step 1/3 | epoch 13/50 | batch 42/60 | global_step 762 | loss_total 0.6382\n",
      "step 1/3 | epoch 13/50 | batch 43/60 | global_step 763 | loss_total 0.4787\n",
      "step 1/3 | epoch 13/50 | batch 44/60 | global_step 764 | loss_total 0.3104\n",
      "step 1/3 | epoch 13/50 | batch 45/60 | global_step 765 | loss_total 0.2877\n",
      "step 1/3 | epoch 13/50 | batch 46/60 | global_step 766 | loss_total 0.3211\n",
      "step 1/3 | epoch 13/50 | batch 47/60 | global_step 767 | loss_total 0.2873\n",
      "step 1/3 | epoch 13/50 | batch 48/60 | global_step 768 | loss_total 0.5167\n",
      "step 1/3 | epoch 13/50 | batch 49/60 | global_step 769 | loss_total 0.5300\n",
      "step 1/3 | epoch 13/50 | batch 50/60 | global_step 770 | loss_total 0.3632\n",
      "step 1/3 | epoch 13/50 | batch 51/60 | global_step 771 | loss_total 0.3350\n",
      "step 1/3 | epoch 13/50 | batch 52/60 | global_step 772 | loss_total 0.3885\n",
      "step 1/3 | epoch 13/50 | batch 53/60 | global_step 773 | loss_total 0.3935\n",
      "step 1/3 | epoch 13/50 | batch 54/60 | global_step 774 | loss_total 0.4521\n",
      "step 1/3 | epoch 13/50 | batch 55/60 | global_step 775 | loss_total 0.8655\n",
      "step 1/3 | epoch 13/50 | batch 56/60 | global_step 776 | loss_total 0.3299\n",
      "step 1/3 | epoch 13/50 | batch 57/60 | global_step 777 | loss_total 0.6731\n",
      "step 1/3 | epoch 13/50 | batch 58/60 | global_step 778 | loss_total 1.0518\n",
      "step 1/3 | epoch 13/50 | batch 59/60 | global_step 779 | loss_total 0.2960\n",
      "step 1/3 | epoch 13/50 | batch 60/60 | global_step 780 | loss_total 0.5234\n",
      "[epoch done] step 1/3 epoch 13/50 | train_total=0.4767 val_total=0.5354\n",
      "step 1/3 | epoch 14/50 | batch 1/60 | global_step 781 | loss_total 0.5075\n",
      "step 1/3 | epoch 14/50 | batch 2/60 | global_step 782 | loss_total 0.3131\n",
      "step 1/3 | epoch 14/50 | batch 3/60 | global_step 783 | loss_total 0.4511\n",
      "step 1/3 | epoch 14/50 | batch 4/60 | global_step 784 | loss_total 0.2706\n",
      "step 1/3 | epoch 14/50 | batch 5/60 | global_step 785 | loss_total 0.2820\n",
      "step 1/3 | epoch 14/50 | batch 6/60 | global_step 786 | loss_total 0.7628\n",
      "step 1/3 | epoch 14/50 | batch 7/60 | global_step 787 | loss_total 0.6169\n",
      "step 1/3 | epoch 14/50 | batch 8/60 | global_step 788 | loss_total 0.4653\n",
      "step 1/3 | epoch 14/50 | batch 9/60 | global_step 789 | loss_total 0.5427\n",
      "step 1/3 | epoch 14/50 | batch 10/60 | global_step 790 | loss_total 0.3508\n",
      "step 1/3 | epoch 14/50 | batch 11/60 | global_step 791 | loss_total 0.7775\n",
      "step 1/3 | epoch 14/50 | batch 12/60 | global_step 792 | loss_total 0.3181\n",
      "step 1/3 | epoch 14/50 | batch 13/60 | global_step 793 | loss_total 0.4675\n",
      "step 1/3 | epoch 14/50 | batch 14/60 | global_step 794 | loss_total 0.4766\n",
      "step 1/3 | epoch 14/50 | batch 15/60 | global_step 795 | loss_total 0.3525\n",
      "step 1/3 | epoch 14/50 | batch 16/60 | global_step 796 | loss_total 0.3882\n",
      "step 1/3 | epoch 14/50 | batch 17/60 | global_step 797 | loss_total 0.8897\n",
      "step 1/3 | epoch 14/50 | batch 18/60 | global_step 798 | loss_total 0.3944\n",
      "step 1/3 | epoch 14/50 | batch 19/60 | global_step 799 | loss_total 0.7308\n",
      "step 1/3 | epoch 14/50 | batch 20/60 | global_step 800 | loss_total 0.4287\n",
      "step 1/3 | epoch 14/50 | batch 21/60 | global_step 801 | loss_total 0.4188\n",
      "step 1/3 | epoch 14/50 | batch 22/60 | global_step 802 | loss_total 0.3126\n",
      "step 1/3 | epoch 14/50 | batch 23/60 | global_step 803 | loss_total 0.4165\n",
      "step 1/3 | epoch 14/50 | batch 24/60 | global_step 804 | loss_total 0.5257\n",
      "step 1/3 | epoch 14/50 | batch 25/60 | global_step 805 | loss_total 0.7423\n",
      "step 1/3 | epoch 14/50 | batch 26/60 | global_step 806 | loss_total 0.5359\n",
      "step 1/3 | epoch 14/50 | batch 27/60 | global_step 807 | loss_total 0.4850\n",
      "step 1/3 | epoch 14/50 | batch 28/60 | global_step 808 | loss_total 0.3917\n",
      "step 1/3 | epoch 14/50 | batch 29/60 | global_step 809 | loss_total 0.3971\n",
      "step 1/3 | epoch 14/50 | batch 30/60 | global_step 810 | loss_total 0.7031\n",
      "step 1/3 | epoch 14/50 | batch 31/60 | global_step 811 | loss_total 0.4319\n",
      "step 1/3 | epoch 14/50 | batch 32/60 | global_step 812 | loss_total 0.3415\n",
      "step 1/3 | epoch 14/50 | batch 33/60 | global_step 813 | loss_total 0.6588\n",
      "step 1/3 | epoch 14/50 | batch 34/60 | global_step 814 | loss_total 0.3544\n",
      "step 1/3 | epoch 14/50 | batch 35/60 | global_step 815 | loss_total 0.3379\n",
      "step 1/3 | epoch 14/50 | batch 36/60 | global_step 816 | loss_total 0.3464\n",
      "step 1/3 | epoch 14/50 | batch 37/60 | global_step 817 | loss_total 0.3319\n",
      "step 1/3 | epoch 14/50 | batch 38/60 | global_step 818 | loss_total 0.6664\n",
      "step 1/3 | epoch 14/50 | batch 39/60 | global_step 819 | loss_total 0.3244\n",
      "step 1/3 | epoch 14/50 | batch 40/60 | global_step 820 | loss_total 0.8754\n",
      "step 1/3 | epoch 14/50 | batch 41/60 | global_step 821 | loss_total 0.4566\n",
      "step 1/3 | epoch 14/50 | batch 42/60 | global_step 822 | loss_total 0.4393\n",
      "step 1/3 | epoch 14/50 | batch 43/60 | global_step 823 | loss_total 0.3515\n",
      "step 1/3 | epoch 14/50 | batch 44/60 | global_step 824 | loss_total 0.3414\n",
      "step 1/3 | epoch 14/50 | batch 45/60 | global_step 825 | loss_total 0.6226\n",
      "step 1/3 | epoch 14/50 | batch 46/60 | global_step 826 | loss_total 0.2932\n",
      "step 1/3 | epoch 14/50 | batch 47/60 | global_step 827 | loss_total 0.5388\n",
      "step 1/3 | epoch 14/50 | batch 48/60 | global_step 828 | loss_total 0.2749\n",
      "step 1/3 | epoch 14/50 | batch 49/60 | global_step 829 | loss_total 0.6031\n",
      "step 1/3 | epoch 14/50 | batch 50/60 | global_step 830 | loss_total 0.4441\n",
      "step 1/3 | epoch 14/50 | batch 51/60 | global_step 831 | loss_total 0.6319\n",
      "step 1/3 | epoch 14/50 | batch 52/60 | global_step 832 | loss_total 0.3615\n",
      "step 1/3 | epoch 14/50 | batch 53/60 | global_step 833 | loss_total 0.3156\n",
      "step 1/3 | epoch 14/50 | batch 54/60 | global_step 834 | loss_total 0.2681\n",
      "step 1/3 | epoch 14/50 | batch 55/60 | global_step 835 | loss_total 0.5328\n",
      "step 1/3 | epoch 14/50 | batch 56/60 | global_step 836 | loss_total 0.3316\n",
      "step 1/3 | epoch 14/50 | batch 57/60 | global_step 837 | loss_total 0.3035\n",
      "step 1/3 | epoch 14/50 | batch 58/60 | global_step 838 | loss_total 0.6200\n",
      "step 1/3 | epoch 14/50 | batch 59/60 | global_step 839 | loss_total 0.5664\n",
      "step 1/3 | epoch 14/50 | batch 60/60 | global_step 840 | loss_total 0.3384\n",
      "[epoch done] step 1/3 epoch 14/50 | train_total=0.4670 val_total=0.3219\n",
      "step 1/3 | epoch 15/50 | batch 1/60 | global_step 841 | loss_total 0.8128\n",
      "step 1/3 | epoch 15/50 | batch 2/60 | global_step 842 | loss_total 0.6162\n",
      "step 1/3 | epoch 15/50 | batch 3/60 | global_step 843 | loss_total 0.4913\n",
      "step 1/3 | epoch 15/50 | batch 4/60 | global_step 844 | loss_total 0.4838\n",
      "step 1/3 | epoch 15/50 | batch 5/60 | global_step 845 | loss_total 0.3348\n",
      "step 1/3 | epoch 15/50 | batch 6/60 | global_step 846 | loss_total 0.3367\n",
      "step 1/3 | epoch 15/50 | batch 7/60 | global_step 847 | loss_total 0.6215\n",
      "step 1/3 | epoch 15/50 | batch 8/60 | global_step 848 | loss_total 0.6345\n",
      "step 1/3 | epoch 15/50 | batch 9/60 | global_step 849 | loss_total 0.6622\n",
      "step 1/3 | epoch 15/50 | batch 10/60 | global_step 850 | loss_total 0.3435\n",
      "step 1/3 | epoch 15/50 | batch 11/60 | global_step 851 | loss_total 0.5703\n",
      "step 1/3 | epoch 15/50 | batch 12/60 | global_step 852 | loss_total 0.3758\n",
      "step 1/3 | epoch 15/50 | batch 13/60 | global_step 853 | loss_total 0.3781\n",
      "step 1/3 | epoch 15/50 | batch 14/60 | global_step 854 | loss_total 0.4397\n",
      "step 1/3 | epoch 15/50 | batch 15/60 | global_step 855 | loss_total 0.4362\n",
      "step 1/3 | epoch 15/50 | batch 16/60 | global_step 856 | loss_total 0.2663\n",
      "step 1/3 | epoch 15/50 | batch 17/60 | global_step 857 | loss_total 0.4692\n",
      "step 1/3 | epoch 15/50 | batch 18/60 | global_step 858 | loss_total 0.2756\n",
      "step 1/3 | epoch 15/50 | batch 19/60 | global_step 859 | loss_total 0.6229\n",
      "step 1/3 | epoch 15/50 | batch 20/60 | global_step 860 | loss_total 0.3484\n",
      "step 1/3 | epoch 15/50 | batch 21/60 | global_step 861 | loss_total 0.2961\n",
      "step 1/3 | epoch 15/50 | batch 22/60 | global_step 862 | loss_total 0.2701\n",
      "step 1/3 | epoch 15/50 | batch 23/60 | global_step 863 | loss_total 0.2666\n",
      "step 1/3 | epoch 15/50 | batch 24/60 | global_step 864 | loss_total 0.2671\n",
      "step 1/3 | epoch 15/50 | batch 25/60 | global_step 865 | loss_total 0.6009\n",
      "step 1/3 | epoch 15/50 | batch 26/60 | global_step 866 | loss_total 0.3189\n",
      "step 1/3 | epoch 15/50 | batch 27/60 | global_step 867 | loss_total 0.6195\n",
      "step 1/3 | epoch 15/50 | batch 28/60 | global_step 868 | loss_total 0.2328\n",
      "step 1/3 | epoch 15/50 | batch 29/60 | global_step 869 | loss_total 0.5743\n",
      "step 1/3 | epoch 15/50 | batch 30/60 | global_step 870 | loss_total 0.3199\n",
      "step 1/3 | epoch 15/50 | batch 31/60 | global_step 871 | loss_total 0.2180\n",
      "step 1/3 | epoch 15/50 | batch 32/60 | global_step 872 | loss_total 0.2613\n",
      "step 1/3 | epoch 15/50 | batch 33/60 | global_step 873 | loss_total 0.3157\n",
      "step 1/3 | epoch 15/50 | batch 34/60 | global_step 874 | loss_total 1.1271\n",
      "step 1/3 | epoch 15/50 | batch 35/60 | global_step 875 | loss_total 0.3759\n",
      "step 1/3 | epoch 15/50 | batch 36/60 | global_step 876 | loss_total 0.2136\n",
      "step 1/3 | epoch 15/50 | batch 37/60 | global_step 877 | loss_total 0.3895\n",
      "step 1/3 | epoch 15/50 | batch 38/60 | global_step 878 | loss_total 0.2131\n",
      "step 1/3 | epoch 15/50 | batch 39/60 | global_step 879 | loss_total 0.2145\n",
      "step 1/3 | epoch 15/50 | batch 40/60 | global_step 880 | loss_total 0.2379\n",
      "step 1/3 | epoch 15/50 | batch 41/60 | global_step 881 | loss_total 0.3033\n",
      "step 1/3 | epoch 15/50 | batch 42/60 | global_step 882 | loss_total 0.2268\n",
      "step 1/3 | epoch 15/50 | batch 43/60 | global_step 883 | loss_total 0.2765\n",
      "step 1/3 | epoch 15/50 | batch 44/60 | global_step 884 | loss_total 0.7937\n",
      "step 1/3 | epoch 15/50 | batch 45/60 | global_step 885 | loss_total 0.4014\n",
      "step 1/3 | epoch 15/50 | batch 46/60 | global_step 886 | loss_total 0.5684\n",
      "step 1/3 | epoch 15/50 | batch 47/60 | global_step 887 | loss_total 0.3491\n",
      "step 1/3 | epoch 15/50 | batch 48/60 | global_step 888 | loss_total 0.4187\n",
      "step 1/3 | epoch 15/50 | batch 49/60 | global_step 889 | loss_total 0.2276\n",
      "step 1/3 | epoch 15/50 | batch 50/60 | global_step 890 | loss_total 0.1830\n",
      "step 1/3 | epoch 15/50 | batch 51/60 | global_step 891 | loss_total 0.2717\n",
      "step 1/3 | epoch 15/50 | batch 52/60 | global_step 892 | loss_total 0.3304\n",
      "step 1/3 | epoch 15/50 | batch 53/60 | global_step 893 | loss_total 0.3621\n",
      "step 1/3 | epoch 15/50 | batch 54/60 | global_step 894 | loss_total 0.4686\n",
      "step 1/3 | epoch 15/50 | batch 55/60 | global_step 895 | loss_total 0.7618\n",
      "step 1/3 | epoch 15/50 | batch 56/60 | global_step 896 | loss_total 0.2659\n",
      "step 1/3 | epoch 15/50 | batch 57/60 | global_step 897 | loss_total 0.4047\n",
      "step 1/3 | epoch 15/50 | batch 58/60 | global_step 898 | loss_total 0.1894\n",
      "step 1/3 | epoch 15/50 | batch 59/60 | global_step 899 | loss_total 0.2059\n",
      "step 1/3 | epoch 15/50 | batch 60/60 | global_step 900 | loss_total 0.3462\n",
      "[epoch done] step 1/3 epoch 15/50 | train_total=0.4035 val_total=0.2191\n",
      "step 1/3 | epoch 16/50 | batch 1/60 | global_step 901 | loss_total 0.5916\n",
      "step 1/3 | epoch 16/50 | batch 2/60 | global_step 902 | loss_total 0.4354\n",
      "step 1/3 | epoch 16/50 | batch 3/60 | global_step 903 | loss_total 0.5040\n",
      "step 1/3 | epoch 16/50 | batch 4/60 | global_step 904 | loss_total 0.5754\n",
      "step 1/3 | epoch 16/50 | batch 5/60 | global_step 905 | loss_total 0.7141\n",
      "step 1/3 | epoch 16/50 | batch 6/60 | global_step 906 | loss_total 0.4513\n",
      "step 1/3 | epoch 16/50 | batch 7/60 | global_step 907 | loss_total 0.3511\n",
      "step 1/3 | epoch 16/50 | batch 8/60 | global_step 908 | loss_total 0.2487\n",
      "step 1/3 | epoch 16/50 | batch 9/60 | global_step 909 | loss_total 1.1730\n",
      "step 1/3 | epoch 16/50 | batch 10/60 | global_step 910 | loss_total 0.3933\n",
      "step 1/3 | epoch 16/50 | batch 11/60 | global_step 911 | loss_total 0.9076\n",
      "step 1/3 | epoch 16/50 | batch 12/60 | global_step 912 | loss_total 0.9718\n",
      "step 1/3 | epoch 16/50 | batch 13/60 | global_step 913 | loss_total 0.3270\n",
      "step 1/3 | epoch 16/50 | batch 14/60 | global_step 914 | loss_total 0.1966\n",
      "step 1/3 | epoch 16/50 | batch 15/60 | global_step 915 | loss_total 0.2018\n",
      "step 1/3 | epoch 16/50 | batch 16/60 | global_step 916 | loss_total 0.3116\n",
      "step 1/3 | epoch 16/50 | batch 17/60 | global_step 917 | loss_total 0.2909\n",
      "step 1/3 | epoch 16/50 | batch 18/60 | global_step 918 | loss_total 0.1671\n",
      "step 1/3 | epoch 16/50 | batch 19/60 | global_step 919 | loss_total 0.1625\n",
      "step 1/3 | epoch 16/50 | batch 20/60 | global_step 920 | loss_total 0.2327\n",
      "step 1/3 | epoch 16/50 | batch 21/60 | global_step 921 | loss_total 0.3992\n",
      "step 1/3 | epoch 16/50 | batch 22/60 | global_step 922 | loss_total 0.6809\n",
      "step 1/3 | epoch 16/50 | batch 23/60 | global_step 923 | loss_total 0.4555\n",
      "step 1/3 | epoch 16/50 | batch 24/60 | global_step 924 | loss_total 0.5807\n",
      "step 1/3 | epoch 16/50 | batch 25/60 | global_step 925 | loss_total 0.9431\n",
      "step 1/3 | epoch 16/50 | batch 26/60 | global_step 926 | loss_total 0.3872\n",
      "step 1/3 | epoch 16/50 | batch 27/60 | global_step 927 | loss_total 0.3204\n",
      "step 1/3 | epoch 16/50 | batch 28/60 | global_step 928 | loss_total 0.4164\n",
      "step 1/3 | epoch 16/50 | batch 29/60 | global_step 929 | loss_total 0.2507\n",
      "step 1/3 | epoch 16/50 | batch 30/60 | global_step 930 | loss_total 1.1066\n",
      "step 1/3 | epoch 16/50 | batch 31/60 | global_step 931 | loss_total 0.5739\n",
      "step 1/3 | epoch 16/50 | batch 32/60 | global_step 932 | loss_total 0.6485\n",
      "step 1/3 | epoch 16/50 | batch 33/60 | global_step 933 | loss_total 0.3388\n",
      "step 1/3 | epoch 16/50 | batch 34/60 | global_step 934 | loss_total 0.3730\n",
      "step 1/3 | epoch 16/50 | batch 35/60 | global_step 935 | loss_total 0.2856\n",
      "step 1/3 | epoch 16/50 | batch 36/60 | global_step 936 | loss_total 0.3184\n",
      "step 1/3 | epoch 16/50 | batch 37/60 | global_step 937 | loss_total 0.2622\n",
      "step 1/3 | epoch 16/50 | batch 38/60 | global_step 938 | loss_total 0.3841\n",
      "step 1/3 | epoch 16/50 | batch 39/60 | global_step 939 | loss_total 0.5321\n",
      "step 1/3 | epoch 16/50 | batch 40/60 | global_step 940 | loss_total 0.5086\n",
      "step 1/3 | epoch 16/50 | batch 41/60 | global_step 941 | loss_total 0.3789\n",
      "step 1/3 | epoch 16/50 | batch 42/60 | global_step 942 | loss_total 0.2877\n",
      "step 1/3 | epoch 16/50 | batch 43/60 | global_step 943 | loss_total 0.6619\n",
      "step 1/3 | epoch 16/50 | batch 44/60 | global_step 944 | loss_total 0.2564\n",
      "step 1/3 | epoch 16/50 | batch 45/60 | global_step 945 | loss_total 0.3513\n",
      "step 1/3 | epoch 16/50 | batch 46/60 | global_step 946 | loss_total 0.3479\n",
      "step 1/3 | epoch 16/50 | batch 47/60 | global_step 947 | loss_total 0.6187\n",
      "step 1/3 | epoch 16/50 | batch 48/60 | global_step 948 | loss_total 0.3799\n",
      "step 1/3 | epoch 16/50 | batch 49/60 | global_step 949 | loss_total 0.2762\n",
      "step 1/3 | epoch 16/50 | batch 50/60 | global_step 950 | loss_total 0.5572\n",
      "step 1/3 | epoch 16/50 | batch 51/60 | global_step 951 | loss_total 0.3930\n",
      "step 1/3 | epoch 16/50 | batch 52/60 | global_step 952 | loss_total 0.3036\n",
      "step 1/3 | epoch 16/50 | batch 53/60 | global_step 953 | loss_total 0.3065\n",
      "step 1/3 | epoch 16/50 | batch 54/60 | global_step 954 | loss_total 0.3998\n",
      "step 1/3 | epoch 16/50 | batch 55/60 | global_step 955 | loss_total 0.5367\n",
      "step 1/3 | epoch 16/50 | batch 56/60 | global_step 956 | loss_total 0.4983\n",
      "step 1/3 | epoch 16/50 | batch 57/60 | global_step 957 | loss_total 0.5144\n",
      "step 1/3 | epoch 16/50 | batch 58/60 | global_step 958 | loss_total 0.6065\n",
      "step 1/3 | epoch 16/50 | batch 59/60 | global_step 959 | loss_total 0.3982\n",
      "step 1/3 | epoch 16/50 | batch 60/60 | global_step 960 | loss_total 0.2713\n",
      "[epoch done] step 1/3 epoch 16/50 | train_total=0.4553 val_total=0.2450\n",
      "step 1/3 | epoch 17/50 | batch 1/60 | global_step 961 | loss_total 0.3041\n",
      "step 1/3 | epoch 17/50 | batch 2/60 | global_step 962 | loss_total 0.4125\n",
      "step 1/3 | epoch 17/50 | batch 3/60 | global_step 963 | loss_total 0.2965\n",
      "step 1/3 | epoch 17/50 | batch 4/60 | global_step 964 | loss_total 0.3765\n",
      "step 1/3 | epoch 17/50 | batch 5/60 | global_step 965 | loss_total 0.2947\n",
      "step 1/3 | epoch 17/50 | batch 6/60 | global_step 966 | loss_total 0.2795\n",
      "step 1/3 | epoch 17/50 | batch 7/60 | global_step 967 | loss_total 0.5989\n",
      "step 1/3 | epoch 17/50 | batch 8/60 | global_step 968 | loss_total 1.0969\n",
      "step 1/3 | epoch 17/50 | batch 9/60 | global_step 969 | loss_total 0.4399\n",
      "step 1/3 | epoch 17/50 | batch 10/60 | global_step 970 | loss_total 0.2830\n",
      "step 1/3 | epoch 17/50 | batch 11/60 | global_step 971 | loss_total 0.4711\n",
      "step 1/3 | epoch 17/50 | batch 12/60 | global_step 972 | loss_total 0.4642\n",
      "step 1/3 | epoch 17/50 | batch 13/60 | global_step 973 | loss_total 0.6269\n",
      "step 1/3 | epoch 17/50 | batch 14/60 | global_step 974 | loss_total 0.4719\n",
      "step 1/3 | epoch 17/50 | batch 15/60 | global_step 975 | loss_total 0.3744\n",
      "step 1/3 | epoch 17/50 | batch 16/60 | global_step 976 | loss_total 0.3760\n",
      "step 1/3 | epoch 17/50 | batch 17/60 | global_step 977 | loss_total 0.3437\n",
      "step 1/3 | epoch 17/50 | batch 18/60 | global_step 978 | loss_total 0.2969\n",
      "step 1/3 | epoch 17/50 | batch 19/60 | global_step 979 | loss_total 0.2868\n",
      "step 1/3 | epoch 17/50 | batch 20/60 | global_step 980 | loss_total 0.2970\n",
      "step 1/3 | epoch 17/50 | batch 21/60 | global_step 981 | loss_total 0.5687\n",
      "step 1/3 | epoch 17/50 | batch 22/60 | global_step 982 | loss_total 0.5523\n",
      "step 1/3 | epoch 17/50 | batch 23/60 | global_step 983 | loss_total 0.3434\n",
      "step 1/3 | epoch 17/50 | batch 24/60 | global_step 984 | loss_total 0.3365\n",
      "step 1/3 | epoch 17/50 | batch 25/60 | global_step 985 | loss_total 0.3445\n",
      "step 1/3 | epoch 17/50 | batch 26/60 | global_step 986 | loss_total 0.6426\n",
      "step 1/3 | epoch 17/50 | batch 27/60 | global_step 987 | loss_total 0.3234\n",
      "step 1/3 | epoch 17/50 | batch 28/60 | global_step 988 | loss_total 0.3244\n",
      "step 1/3 | epoch 17/50 | batch 29/60 | global_step 989 | loss_total 0.3303\n",
      "step 1/3 | epoch 17/50 | batch 30/60 | global_step 990 | loss_total 0.2564\n",
      "step 1/3 | epoch 17/50 | batch 31/60 | global_step 991 | loss_total 0.2829\n",
      "step 1/3 | epoch 17/50 | batch 32/60 | global_step 992 | loss_total 0.2767\n",
      "step 1/3 | epoch 17/50 | batch 33/60 | global_step 993 | loss_total 0.4594\n",
      "step 1/3 | epoch 17/50 | batch 34/60 | global_step 994 | loss_total 0.2834\n",
      "step 1/3 | epoch 17/50 | batch 35/60 | global_step 995 | loss_total 0.4191\n",
      "step 1/3 | epoch 17/50 | batch 36/60 | global_step 996 | loss_total 1.5497\n",
      "step 1/3 | epoch 17/50 | batch 37/60 | global_step 997 | loss_total 0.6106\n",
      "step 1/3 | epoch 17/50 | batch 38/60 | global_step 998 | loss_total 0.3119\n",
      "step 1/3 | epoch 17/50 | batch 39/60 | global_step 999 | loss_total 0.3206\n",
      "step 1/3 | epoch 17/50 | batch 40/60 | global_step 1000 | loss_total 0.5673\n",
      "step 1/3 | epoch 17/50 | batch 41/60 | global_step 1001 | loss_total 0.3264\n",
      "step 1/3 | epoch 17/50 | batch 42/60 | global_step 1002 | loss_total 0.3605\n",
      "step 1/3 | epoch 17/50 | batch 43/60 | global_step 1003 | loss_total 0.4500\n",
      "step 1/3 | epoch 17/50 | batch 44/60 | global_step 1004 | loss_total 0.2772\n",
      "step 1/3 | epoch 17/50 | batch 45/60 | global_step 1005 | loss_total 0.7865\n",
      "step 1/3 | epoch 17/50 | batch 46/60 | global_step 1006 | loss_total 0.3812\n",
      "step 1/3 | epoch 17/50 | batch 47/60 | global_step 1007 | loss_total 0.2894\n",
      "step 1/3 | epoch 17/50 | batch 48/60 | global_step 1008 | loss_total 0.4525\n",
      "step 1/3 | epoch 17/50 | batch 49/60 | global_step 1009 | loss_total 0.2751\n",
      "step 1/3 | epoch 17/50 | batch 50/60 | global_step 1010 | loss_total 0.5582\n",
      "step 1/3 | epoch 17/50 | batch 51/60 | global_step 1011 | loss_total 0.2700\n",
      "step 1/3 | epoch 17/50 | batch 52/60 | global_step 1012 | loss_total 0.2775\n",
      "step 1/3 | epoch 17/50 | batch 53/60 | global_step 1013 | loss_total 0.4745\n",
      "step 1/3 | epoch 17/50 | batch 54/60 | global_step 1014 | loss_total 0.2485\n",
      "step 1/3 | epoch 17/50 | batch 55/60 | global_step 1015 | loss_total 0.5041\n",
      "step 1/3 | epoch 17/50 | batch 56/60 | global_step 1016 | loss_total 0.5590\n",
      "step 1/3 | epoch 17/50 | batch 57/60 | global_step 1017 | loss_total 0.5011\n",
      "step 1/3 | epoch 17/50 | batch 58/60 | global_step 1018 | loss_total 1.1258\n",
      "step 1/3 | epoch 17/50 | batch 59/60 | global_step 1019 | loss_total 0.4100\n",
      "step 1/3 | epoch 17/50 | batch 60/60 | global_step 1020 | loss_total 0.3678\n",
      "[epoch done] step 1/3 epoch 17/50 | train_total=0.4398 val_total=0.1704\n",
      "step 1/3 | epoch 18/50 | batch 1/60 | global_step 1021 | loss_total 0.6629\n",
      "step 1/3 | epoch 18/50 | batch 2/60 | global_step 1022 | loss_total 0.2467\n",
      "step 1/3 | epoch 18/50 | batch 3/60 | global_step 1023 | loss_total 0.2751\n",
      "step 1/3 | epoch 18/50 | batch 4/60 | global_step 1024 | loss_total 0.2584\n",
      "step 1/3 | epoch 18/50 | batch 5/60 | global_step 1025 | loss_total 0.3888\n",
      "step 1/3 | epoch 18/50 | batch 6/60 | global_step 1026 | loss_total 0.2699\n",
      "step 1/3 | epoch 18/50 | batch 7/60 | global_step 1027 | loss_total 0.4884\n",
      "step 1/3 | epoch 18/50 | batch 8/60 | global_step 1028 | loss_total 0.2255\n",
      "step 1/3 | epoch 18/50 | batch 9/60 | global_step 1029 | loss_total 0.2467\n",
      "step 1/3 | epoch 18/50 | batch 10/60 | global_step 1030 | loss_total 0.2400\n",
      "step 1/3 | epoch 18/50 | batch 11/60 | global_step 1031 | loss_total 0.2901\n",
      "step 1/3 | epoch 18/50 | batch 12/60 | global_step 1032 | loss_total 0.2245\n",
      "step 1/3 | epoch 18/50 | batch 13/60 | global_step 1033 | loss_total 0.4666\n",
      "step 1/3 | epoch 18/50 | batch 14/60 | global_step 1034 | loss_total 0.2186\n",
      "step 1/3 | epoch 18/50 | batch 15/60 | global_step 1035 | loss_total 0.2493\n",
      "step 1/3 | epoch 18/50 | batch 16/60 | global_step 1036 | loss_total 0.2860\n",
      "step 1/3 | epoch 18/50 | batch 17/60 | global_step 1037 | loss_total 0.8651\n",
      "step 1/3 | epoch 18/50 | batch 18/60 | global_step 1038 | loss_total 0.2003\n",
      "step 1/3 | epoch 18/50 | batch 19/60 | global_step 1039 | loss_total 0.2456\n",
      "step 1/3 | epoch 18/50 | batch 20/60 | global_step 1040 | loss_total 0.8378\n",
      "step 1/3 | epoch 18/50 | batch 21/60 | global_step 1041 | loss_total 0.2990\n",
      "step 1/3 | epoch 18/50 | batch 22/60 | global_step 1042 | loss_total 0.5717\n",
      "step 1/3 | epoch 18/50 | batch 23/60 | global_step 1043 | loss_total 0.2737\n",
      "step 1/3 | epoch 18/50 | batch 24/60 | global_step 1044 | loss_total 0.2535\n",
      "step 1/3 | epoch 18/50 | batch 25/60 | global_step 1045 | loss_total 0.5042\n",
      "step 1/3 | epoch 18/50 | batch 26/60 | global_step 1046 | loss_total 1.0802\n",
      "step 1/3 | epoch 18/50 | batch 27/60 | global_step 1047 | loss_total 0.7707\n",
      "step 1/3 | epoch 18/50 | batch 28/60 | global_step 1048 | loss_total 0.3749\n",
      "step 1/3 | epoch 18/50 | batch 29/60 | global_step 1049 | loss_total 0.1810\n",
      "step 1/3 | epoch 18/50 | batch 30/60 | global_step 1050 | loss_total 0.5974\n",
      "step 1/3 | epoch 18/50 | batch 31/60 | global_step 1051 | loss_total 0.2658\n",
      "step 1/3 | epoch 18/50 | batch 32/60 | global_step 1052 | loss_total 0.4436\n",
      "step 1/3 | epoch 18/50 | batch 33/60 | global_step 1053 | loss_total 0.2289\n",
      "step 1/3 | epoch 18/50 | batch 34/60 | global_step 1054 | loss_total 0.2707\n",
      "step 1/3 | epoch 18/50 | batch 35/60 | global_step 1055 | loss_total 0.2736\n",
      "step 1/3 | epoch 18/50 | batch 36/60 | global_step 1056 | loss_total 0.6440\n",
      "step 1/3 | epoch 18/50 | batch 37/60 | global_step 1057 | loss_total 0.2037\n",
      "step 1/3 | epoch 18/50 | batch 38/60 | global_step 1058 | loss_total 0.2006\n",
      "step 1/3 | epoch 18/50 | batch 39/60 | global_step 1059 | loss_total 0.2674\n",
      "step 1/3 | epoch 18/50 | batch 40/60 | global_step 1060 | loss_total 0.3262\n",
      "step 1/3 | epoch 18/50 | batch 41/60 | global_step 1061 | loss_total 0.2834\n",
      "step 1/3 | epoch 18/50 | batch 42/60 | global_step 1062 | loss_total 0.7936\n",
      "step 1/3 | epoch 18/50 | batch 43/60 | global_step 1063 | loss_total 0.2068\n",
      "step 1/3 | epoch 18/50 | batch 44/60 | global_step 1064 | loss_total 0.3325\n",
      "step 1/3 | epoch 18/50 | batch 45/60 | global_step 1065 | loss_total 0.3016\n",
      "step 1/3 | epoch 18/50 | batch 46/60 | global_step 1066 | loss_total 0.2563\n",
      "step 1/3 | epoch 18/50 | batch 47/60 | global_step 1067 | loss_total 0.1975\n",
      "step 1/3 | epoch 18/50 | batch 48/60 | global_step 1068 | loss_total 0.2543\n",
      "step 1/3 | epoch 18/50 | batch 49/60 | global_step 1069 | loss_total 0.6321\n",
      "step 1/3 | epoch 18/50 | batch 50/60 | global_step 1070 | loss_total 0.5851\n",
      "step 1/3 | epoch 18/50 | batch 51/60 | global_step 1071 | loss_total 0.2226\n",
      "step 1/3 | epoch 18/50 | batch 52/60 | global_step 1072 | loss_total 0.3605\n",
      "step 1/3 | epoch 18/50 | batch 53/60 | global_step 1073 | loss_total 0.7826\n",
      "step 1/3 | epoch 18/50 | batch 54/60 | global_step 1074 | loss_total 1.0600\n",
      "step 1/3 | epoch 18/50 | batch 55/60 | global_step 1075 | loss_total 0.2753\n",
      "step 1/3 | epoch 18/50 | batch 56/60 | global_step 1076 | loss_total 0.3827\n",
      "step 1/3 | epoch 18/50 | batch 57/60 | global_step 1077 | loss_total 0.4230\n",
      "step 1/3 | epoch 18/50 | batch 58/60 | global_step 1078 | loss_total 0.2497\n",
      "step 1/3 | epoch 18/50 | batch 59/60 | global_step 1079 | loss_total 0.3594\n",
      "step 1/3 | epoch 18/50 | batch 60/60 | global_step 1080 | loss_total 0.5114\n",
      "[epoch done] step 1/3 epoch 18/50 | train_total=0.3948 val_total=0.2869\n",
      "step 1/3 | epoch 19/50 | batch 1/60 | global_step 1081 | loss_total 0.2426\n",
      "step 1/3 | epoch 19/50 | batch 2/60 | global_step 1082 | loss_total 0.2696\n",
      "step 1/3 | epoch 19/50 | batch 3/60 | global_step 1083 | loss_total 0.2775\n",
      "step 1/3 | epoch 19/50 | batch 4/60 | global_step 1084 | loss_total 0.3355\n",
      "step 1/3 | epoch 19/50 | batch 5/60 | global_step 1085 | loss_total 0.2549\n",
      "step 1/3 | epoch 19/50 | batch 6/60 | global_step 1086 | loss_total 0.2643\n",
      "step 1/3 | epoch 19/50 | batch 7/60 | global_step 1087 | loss_total 0.2575\n",
      "step 1/3 | epoch 19/50 | batch 8/60 | global_step 1088 | loss_total 0.3172\n",
      "step 1/3 | epoch 19/50 | batch 9/60 | global_step 1089 | loss_total 0.4240\n",
      "step 1/3 | epoch 19/50 | batch 10/60 | global_step 1090 | loss_total 0.2519\n",
      "step 1/3 | epoch 19/50 | batch 11/60 | global_step 1091 | loss_total 0.2469\n",
      "step 1/3 | epoch 19/50 | batch 12/60 | global_step 1092 | loss_total 0.3142\n",
      "step 1/3 | epoch 19/50 | batch 13/60 | global_step 1093 | loss_total 0.3231\n",
      "step 1/3 | epoch 19/50 | batch 14/60 | global_step 1094 | loss_total 0.2287\n",
      "step 1/3 | epoch 19/50 | batch 15/60 | global_step 1095 | loss_total 0.2170\n",
      "step 1/3 | epoch 19/50 | batch 16/60 | global_step 1096 | loss_total 0.3589\n",
      "step 1/3 | epoch 19/50 | batch 17/60 | global_step 1097 | loss_total 0.4329\n",
      "step 1/3 | epoch 19/50 | batch 18/60 | global_step 1098 | loss_total 1.3078\n",
      "step 1/3 | epoch 19/50 | batch 19/60 | global_step 1099 | loss_total 0.6458\n",
      "step 1/3 | epoch 19/50 | batch 20/60 | global_step 1100 | loss_total 0.6855\n",
      "step 1/3 | epoch 19/50 | batch 21/60 | global_step 1101 | loss_total 0.2530\n",
      "step 1/3 | epoch 19/50 | batch 22/60 | global_step 1102 | loss_total 0.3462\n",
      "step 1/3 | epoch 19/50 | batch 23/60 | global_step 1103 | loss_total 0.8157\n",
      "step 1/3 | epoch 19/50 | batch 24/60 | global_step 1104 | loss_total 0.2181\n",
      "step 1/3 | epoch 19/50 | batch 25/60 | global_step 1105 | loss_total 0.2524\n",
      "step 1/3 | epoch 19/50 | batch 26/60 | global_step 1106 | loss_total 0.6017\n",
      "step 1/3 | epoch 19/50 | batch 27/60 | global_step 1107 | loss_total 0.3196\n",
      "step 1/3 | epoch 19/50 | batch 28/60 | global_step 1108 | loss_total 0.2534\n",
      "step 1/3 | epoch 19/50 | batch 29/60 | global_step 1109 | loss_total 0.9507\n",
      "step 1/3 | epoch 19/50 | batch 30/60 | global_step 1110 | loss_total 0.2883\n",
      "step 1/3 | epoch 19/50 | batch 31/60 | global_step 1111 | loss_total 0.3474\n",
      "step 1/3 | epoch 19/50 | batch 32/60 | global_step 1112 | loss_total 0.6962\n",
      "step 1/3 | epoch 19/50 | batch 33/60 | global_step 1113 | loss_total 0.9067\n",
      "step 1/3 | epoch 19/50 | batch 34/60 | global_step 1114 | loss_total 0.6364\n",
      "step 1/3 | epoch 19/50 | batch 35/60 | global_step 1115 | loss_total 1.1229\n",
      "step 1/3 | epoch 19/50 | batch 36/60 | global_step 1116 | loss_total 0.3693\n",
      "step 1/3 | epoch 19/50 | batch 37/60 | global_step 1117 | loss_total 0.2676\n",
      "step 1/3 | epoch 19/50 | batch 38/60 | global_step 1118 | loss_total 0.6631\n",
      "step 1/3 | epoch 19/50 | batch 39/60 | global_step 1119 | loss_total 0.6438\n",
      "step 1/3 | epoch 19/50 | batch 40/60 | global_step 1120 | loss_total 0.4201\n",
      "step 1/3 | epoch 19/50 | batch 41/60 | global_step 1121 | loss_total 0.3806\n",
      "step 1/3 | epoch 19/50 | batch 42/60 | global_step 1122 | loss_total 0.2825\n",
      "step 1/3 | epoch 19/50 | batch 43/60 | global_step 1123 | loss_total 0.3057\n",
      "step 1/3 | epoch 19/50 | batch 44/60 | global_step 1124 | loss_total 0.3846\n",
      "step 1/3 | epoch 19/50 | batch 45/60 | global_step 1125 | loss_total 0.4080\n",
      "step 1/3 | epoch 19/50 | batch 46/60 | global_step 1126 | loss_total 0.3633\n",
      "step 1/3 | epoch 19/50 | batch 47/60 | global_step 1127 | loss_total 0.2691\n",
      "step 1/3 | epoch 19/50 | batch 48/60 | global_step 1128 | loss_total 0.4162\n",
      "step 1/3 | epoch 19/50 | batch 49/60 | global_step 1129 | loss_total 0.3129\n",
      "step 1/3 | epoch 19/50 | batch 50/60 | global_step 1130 | loss_total 0.4609\n",
      "step 1/3 | epoch 19/50 | batch 51/60 | global_step 1131 | loss_total 0.2941\n",
      "step 1/3 | epoch 19/50 | batch 52/60 | global_step 1132 | loss_total 0.4243\n",
      "step 1/3 | epoch 19/50 | batch 53/60 | global_step 1133 | loss_total 0.2975\n",
      "step 1/3 | epoch 19/50 | batch 54/60 | global_step 1134 | loss_total 0.2657\n",
      "step 1/3 | epoch 19/50 | batch 55/60 | global_step 1135 | loss_total 0.6879\n",
      "step 1/3 | epoch 19/50 | batch 56/60 | global_step 1136 | loss_total 0.4602\n",
      "step 1/3 | epoch 19/50 | batch 57/60 | global_step 1137 | loss_total 0.3320\n",
      "step 1/3 | epoch 19/50 | batch 58/60 | global_step 1138 | loss_total 0.3737\n",
      "step 1/3 | epoch 19/50 | batch 59/60 | global_step 1139 | loss_total 0.2972\n",
      "step 1/3 | epoch 19/50 | batch 60/60 | global_step 1140 | loss_total 0.3200\n",
      "[epoch done] step 1/3 epoch 19/50 | train_total=0.4227 val_total=0.2350\n",
      "step 1/3 | epoch 20/50 | batch 1/60 | global_step 1141 | loss_total 0.2818\n",
      "step 1/3 | epoch 20/50 | batch 2/60 | global_step 1142 | loss_total 0.4657\n",
      "step 1/3 | epoch 20/50 | batch 3/60 | global_step 1143 | loss_total 0.2649\n",
      "step 1/3 | epoch 20/50 | batch 4/60 | global_step 1144 | loss_total 0.2236\n",
      "step 1/3 | epoch 20/50 | batch 5/60 | global_step 1145 | loss_total 0.2716\n",
      "step 1/3 | epoch 20/50 | batch 6/60 | global_step 1146 | loss_total 0.3440\n",
      "step 1/3 | epoch 20/50 | batch 7/60 | global_step 1147 | loss_total 0.2445\n",
      "step 1/3 | epoch 20/50 | batch 8/60 | global_step 1148 | loss_total 0.4455\n",
      "step 1/3 | epoch 20/50 | batch 9/60 | global_step 1149 | loss_total 0.4136\n",
      "step 1/3 | epoch 20/50 | batch 10/60 | global_step 1150 | loss_total 0.2837\n",
      "step 1/3 | epoch 20/50 | batch 11/60 | global_step 1151 | loss_total 0.6070\n",
      "step 1/3 | epoch 20/50 | batch 12/60 | global_step 1152 | loss_total 0.9506\n",
      "step 1/3 | epoch 20/50 | batch 13/60 | global_step 1153 | loss_total 0.4280\n",
      "step 1/3 | epoch 20/50 | batch 14/60 | global_step 1154 | loss_total 0.2849\n",
      "step 1/3 | epoch 20/50 | batch 15/60 | global_step 1155 | loss_total 1.1683\n",
      "step 1/3 | epoch 20/50 | batch 16/60 | global_step 1156 | loss_total 0.4550\n",
      "step 1/3 | epoch 20/50 | batch 17/60 | global_step 1157 | loss_total 0.3984\n",
      "step 1/3 | epoch 20/50 | batch 18/60 | global_step 1158 | loss_total 0.4460\n",
      "step 1/3 | epoch 20/50 | batch 19/60 | global_step 1159 | loss_total 0.5633\n",
      "step 1/3 | epoch 20/50 | batch 20/60 | global_step 1160 | loss_total 0.3228\n",
      "step 1/3 | epoch 20/50 | batch 21/60 | global_step 1161 | loss_total 0.2859\n",
      "step 1/3 | epoch 20/50 | batch 22/60 | global_step 1162 | loss_total 0.5164\n",
      "step 1/3 | epoch 20/50 | batch 23/60 | global_step 1163 | loss_total 0.3072\n",
      "step 1/3 | epoch 20/50 | batch 24/60 | global_step 1164 | loss_total 0.6037\n",
      "step 1/3 | epoch 20/50 | batch 25/60 | global_step 1165 | loss_total 0.5160\n",
      "step 1/3 | epoch 20/50 | batch 26/60 | global_step 1166 | loss_total 0.6301\n",
      "step 1/3 | epoch 20/50 | batch 27/60 | global_step 1167 | loss_total 0.4899\n",
      "step 1/3 | epoch 20/50 | batch 28/60 | global_step 1168 | loss_total 1.2798\n",
      "step 1/3 | epoch 20/50 | batch 29/60 | global_step 1169 | loss_total 0.5880\n",
      "step 1/3 | epoch 20/50 | batch 30/60 | global_step 1170 | loss_total 0.3561\n",
      "step 1/3 | epoch 20/50 | batch 31/60 | global_step 1171 | loss_total 0.3111\n",
      "step 1/3 | epoch 20/50 | batch 32/60 | global_step 1172 | loss_total 0.4318\n",
      "step 1/3 | epoch 20/50 | batch 33/60 | global_step 1173 | loss_total 0.6005\n",
      "step 1/3 | epoch 20/50 | batch 34/60 | global_step 1174 | loss_total 1.0799\n",
      "step 1/3 | epoch 20/50 | batch 35/60 | global_step 1175 | loss_total 0.3154\n",
      "step 1/3 | epoch 20/50 | batch 36/60 | global_step 1176 | loss_total 0.2959\n",
      "step 1/3 | epoch 20/50 | batch 37/60 | global_step 1177 | loss_total 0.6228\n",
      "step 1/3 | epoch 20/50 | batch 38/60 | global_step 1178 | loss_total 0.6398\n",
      "step 1/3 | epoch 20/50 | batch 39/60 | global_step 1179 | loss_total 0.2681\n",
      "step 1/3 | epoch 20/50 | batch 40/60 | global_step 1180 | loss_total 0.5080\n",
      "step 1/3 | epoch 20/50 | batch 41/60 | global_step 1181 | loss_total 0.2626\n",
      "step 1/3 | epoch 20/50 | batch 42/60 | global_step 1182 | loss_total 0.2573\n",
      "step 1/3 | epoch 20/50 | batch 43/60 | global_step 1183 | loss_total 0.8038\n",
      "step 1/3 | epoch 20/50 | batch 44/60 | global_step 1184 | loss_total 0.3200\n",
      "step 1/3 | epoch 20/50 | batch 45/60 | global_step 1185 | loss_total 0.4227\n",
      "step 1/3 | epoch 20/50 | batch 46/60 | global_step 1186 | loss_total 0.2676\n",
      "step 1/3 | epoch 20/50 | batch 47/60 | global_step 1187 | loss_total 0.3052\n",
      "step 1/3 | epoch 20/50 | batch 48/60 | global_step 1188 | loss_total 0.5076\n",
      "step 1/3 | epoch 20/50 | batch 49/60 | global_step 1189 | loss_total 0.3763\n",
      "step 1/3 | epoch 20/50 | batch 50/60 | global_step 1190 | loss_total 0.5127\n",
      "step 1/3 | epoch 20/50 | batch 51/60 | global_step 1191 | loss_total 0.2859\n",
      "step 1/3 | epoch 20/50 | batch 52/60 | global_step 1192 | loss_total 0.7325\n",
      "step 1/3 | epoch 20/50 | batch 53/60 | global_step 1193 | loss_total 0.3189\n",
      "step 1/3 | epoch 20/50 | batch 54/60 | global_step 1194 | loss_total 0.2420\n",
      "step 1/3 | epoch 20/50 | batch 55/60 | global_step 1195 | loss_total 0.4819\n",
      "step 1/3 | epoch 20/50 | batch 56/60 | global_step 1196 | loss_total 0.2812\n",
      "step 1/3 | epoch 20/50 | batch 57/60 | global_step 1197 | loss_total 0.3622\n",
      "step 1/3 | epoch 20/50 | batch 58/60 | global_step 1198 | loss_total 0.4240\n",
      "step 1/3 | epoch 20/50 | batch 59/60 | global_step 1199 | loss_total 0.3594\n",
      "step 1/3 | epoch 20/50 | batch 60/60 | global_step 1200 | loss_total 0.3098\n",
      "[epoch done] step 1/3 epoch 20/50 | train_total=0.4557 val_total=0.2268\n",
      "step 1/3 | epoch 21/50 | batch 1/60 | global_step 1201 | loss_total 0.3688\n",
      "step 1/3 | epoch 21/50 | batch 2/60 | global_step 1202 | loss_total 0.6004\n",
      "step 1/3 | epoch 21/50 | batch 3/60 | global_step 1203 | loss_total 0.5485\n",
      "step 1/3 | epoch 21/50 | batch 4/60 | global_step 1204 | loss_total 0.3814\n",
      "step 1/3 | epoch 21/50 | batch 5/60 | global_step 1205 | loss_total 0.2493\n",
      "step 1/3 | epoch 21/50 | batch 6/60 | global_step 1206 | loss_total 0.2579\n",
      "step 1/3 | epoch 21/50 | batch 7/60 | global_step 1207 | loss_total 0.6183\n",
      "step 1/3 | epoch 21/50 | batch 8/60 | global_step 1208 | loss_total 0.8385\n",
      "step 1/3 | epoch 21/50 | batch 9/60 | global_step 1209 | loss_total 0.2474\n",
      "step 1/3 | epoch 21/50 | batch 10/60 | global_step 1210 | loss_total 0.2519\n",
      "step 1/3 | epoch 21/50 | batch 11/60 | global_step 1211 | loss_total 0.3874\n",
      "step 1/3 | epoch 21/50 | batch 12/60 | global_step 1212 | loss_total 0.3208\n",
      "step 1/3 | epoch 21/50 | batch 13/60 | global_step 1213 | loss_total 0.2966\n",
      "step 1/3 | epoch 21/50 | batch 14/60 | global_step 1214 | loss_total 0.6338\n",
      "step 1/3 | epoch 21/50 | batch 15/60 | global_step 1215 | loss_total 0.3785\n",
      "step 1/3 | epoch 21/50 | batch 16/60 | global_step 1216 | loss_total 0.3502\n",
      "step 1/3 | epoch 21/50 | batch 17/60 | global_step 1217 | loss_total 0.2547\n",
      "step 1/3 | epoch 21/50 | batch 18/60 | global_step 1218 | loss_total 0.5085\n",
      "step 1/3 | epoch 21/50 | batch 19/60 | global_step 1219 | loss_total 0.6675\n",
      "step 1/3 | epoch 21/50 | batch 20/60 | global_step 1220 | loss_total 0.2856\n",
      "step 1/3 | epoch 21/50 | batch 21/60 | global_step 1221 | loss_total 0.2502\n",
      "step 1/3 | epoch 21/50 | batch 22/60 | global_step 1222 | loss_total 0.2604\n",
      "step 1/3 | epoch 21/50 | batch 23/60 | global_step 1223 | loss_total 0.7508\n",
      "step 1/3 | epoch 21/50 | batch 24/60 | global_step 1224 | loss_total 0.2641\n",
      "step 1/3 | epoch 21/50 | batch 25/60 | global_step 1225 | loss_total 0.8901\n",
      "step 1/3 | epoch 21/50 | batch 26/60 | global_step 1226 | loss_total 0.2564\n",
      "step 1/3 | epoch 21/50 | batch 27/60 | global_step 1227 | loss_total 0.3023\n",
      "step 1/3 | epoch 21/50 | batch 28/60 | global_step 1228 | loss_total 0.2901\n",
      "step 1/3 | epoch 21/50 | batch 29/60 | global_step 1229 | loss_total 0.3229\n",
      "step 1/3 | epoch 21/50 | batch 30/60 | global_step 1230 | loss_total 0.9195\n",
      "step 1/3 | epoch 21/50 | batch 31/60 | global_step 1231 | loss_total 0.4199\n",
      "step 1/3 | epoch 21/50 | batch 32/60 | global_step 1232 | loss_total 0.2713\n",
      "step 1/3 | epoch 21/50 | batch 33/60 | global_step 1233 | loss_total 0.3858\n",
      "step 1/3 | epoch 21/50 | batch 34/60 | global_step 1234 | loss_total 0.2720\n",
      "step 1/3 | epoch 21/50 | batch 35/60 | global_step 1235 | loss_total 0.2473\n",
      "step 1/3 | epoch 21/50 | batch 36/60 | global_step 1236 | loss_total 0.2511\n",
      "step 1/3 | epoch 21/50 | batch 37/60 | global_step 1237 | loss_total 0.2553\n",
      "step 1/3 | epoch 21/50 | batch 38/60 | global_step 1238 | loss_total 0.3193\n",
      "step 1/3 | epoch 21/50 | batch 39/60 | global_step 1239 | loss_total 0.2448\n",
      "step 1/3 | epoch 21/50 | batch 40/60 | global_step 1240 | loss_total 0.4406\n",
      "step 1/3 | epoch 21/50 | batch 41/60 | global_step 1241 | loss_total 0.2577\n",
      "step 1/3 | epoch 21/50 | batch 42/60 | global_step 1242 | loss_total 0.7156\n",
      "step 1/3 | epoch 21/50 | batch 43/60 | global_step 1243 | loss_total 0.2448\n",
      "step 1/3 | epoch 21/50 | batch 44/60 | global_step 1244 | loss_total 0.3512\n",
      "step 1/3 | epoch 21/50 | batch 45/60 | global_step 1245 | loss_total 0.2677\n",
      "step 1/3 | epoch 21/50 | batch 46/60 | global_step 1246 | loss_total 0.7883\n",
      "step 1/3 | epoch 21/50 | batch 47/60 | global_step 1247 | loss_total 0.5567\n",
      "step 1/3 | epoch 21/50 | batch 48/60 | global_step 1248 | loss_total 0.5393\n",
      "step 1/3 | epoch 21/50 | batch 49/60 | global_step 1249 | loss_total 0.2737\n",
      "step 1/3 | epoch 21/50 | batch 50/60 | global_step 1250 | loss_total 0.3776\n",
      "step 1/3 | epoch 21/50 | batch 51/60 | global_step 1251 | loss_total 0.3669\n",
      "step 1/3 | epoch 21/50 | batch 52/60 | global_step 1252 | loss_total 0.3441\n",
      "step 1/3 | epoch 21/50 | batch 53/60 | global_step 1253 | loss_total 0.2341\n",
      "step 1/3 | epoch 21/50 | batch 54/60 | global_step 1254 | loss_total 0.2420\n",
      "step 1/3 | epoch 21/50 | batch 55/60 | global_step 1255 | loss_total 0.6877\n",
      "step 1/3 | epoch 21/50 | batch 56/60 | global_step 1256 | loss_total 0.3211\n",
      "step 1/3 | epoch 21/50 | batch 57/60 | global_step 1257 | loss_total 0.4900\n",
      "step 1/3 | epoch 21/50 | batch 58/60 | global_step 1258 | loss_total 0.2440\n",
      "step 1/3 | epoch 21/50 | batch 59/60 | global_step 1259 | loss_total 0.6767\n",
      "step 1/3 | epoch 21/50 | batch 60/60 | global_step 1260 | loss_total 0.2369\n",
      "[epoch done] step 1/3 epoch 21/50 | train_total=0.4046 val_total=0.2104\n",
      "step 1/3 | epoch 22/50 | batch 1/60 | global_step 1261 | loss_total 0.6294\n",
      "step 1/3 | epoch 22/50 | batch 2/60 | global_step 1262 | loss_total 0.3992\n",
      "step 1/3 | epoch 22/50 | batch 3/60 | global_step 1263 | loss_total 0.2906\n",
      "step 1/3 | epoch 22/50 | batch 4/60 | global_step 1264 | loss_total 0.3316\n",
      "step 1/3 | epoch 22/50 | batch 5/60 | global_step 1265 | loss_total 0.4447\n",
      "step 1/3 | epoch 22/50 | batch 6/60 | global_step 1266 | loss_total 0.2777\n",
      "step 1/3 | epoch 22/50 | batch 7/60 | global_step 1267 | loss_total 0.2449\n",
      "step 1/3 | epoch 22/50 | batch 8/60 | global_step 1268 | loss_total 0.2612\n",
      "step 1/3 | epoch 22/50 | batch 9/60 | global_step 1269 | loss_total 0.3472\n",
      "step 1/3 | epoch 22/50 | batch 10/60 | global_step 1270 | loss_total 0.2723\n",
      "step 1/3 | epoch 22/50 | batch 11/60 | global_step 1271 | loss_total 0.3955\n",
      "step 1/3 | epoch 22/50 | batch 12/60 | global_step 1272 | loss_total 0.3770\n",
      "step 1/3 | epoch 22/50 | batch 13/60 | global_step 1273 | loss_total 1.1664\n",
      "step 1/3 | epoch 22/50 | batch 14/60 | global_step 1274 | loss_total 0.2522\n",
      "step 1/3 | epoch 22/50 | batch 15/60 | global_step 1275 | loss_total 1.3832\n",
      "step 1/3 | epoch 22/50 | batch 16/60 | global_step 1276 | loss_total 0.2651\n",
      "step 1/3 | epoch 22/50 | batch 17/60 | global_step 1277 | loss_total 1.0235\n",
      "step 1/3 | epoch 22/50 | batch 18/60 | global_step 1278 | loss_total 0.9068\n",
      "step 1/3 | epoch 22/50 | batch 19/60 | global_step 1279 | loss_total 0.2805\n",
      "step 1/3 | epoch 22/50 | batch 20/60 | global_step 1280 | loss_total 0.2617\n",
      "step 1/3 | epoch 22/50 | batch 21/60 | global_step 1281 | loss_total 0.2555\n",
      "step 1/3 | epoch 22/50 | batch 22/60 | global_step 1282 | loss_total 0.6888\n",
      "step 1/3 | epoch 22/50 | batch 23/60 | global_step 1283 | loss_total 0.2670\n",
      "step 1/3 | epoch 22/50 | batch 24/60 | global_step 1284 | loss_total 0.7027\n",
      "step 1/3 | epoch 22/50 | batch 25/60 | global_step 1285 | loss_total 0.2602\n",
      "step 1/3 | epoch 22/50 | batch 26/60 | global_step 1286 | loss_total 0.6728\n",
      "step 1/3 | epoch 22/50 | batch 27/60 | global_step 1287 | loss_total 0.2621\n",
      "step 1/3 | epoch 22/50 | batch 28/60 | global_step 1288 | loss_total 0.2764\n",
      "step 1/3 | epoch 22/50 | batch 29/60 | global_step 1289 | loss_total 0.3537\n",
      "step 1/3 | epoch 22/50 | batch 30/60 | global_step 1290 | loss_total 0.2624\n",
      "step 1/3 | epoch 22/50 | batch 31/60 | global_step 1291 | loss_total 0.9015\n",
      "step 1/3 | epoch 22/50 | batch 32/60 | global_step 1292 | loss_total 0.4603\n",
      "step 1/3 | epoch 22/50 | batch 33/60 | global_step 1293 | loss_total 0.3468\n",
      "step 1/3 | epoch 22/50 | batch 34/60 | global_step 1294 | loss_total 0.2763\n",
      "step 1/3 | epoch 22/50 | batch 35/60 | global_step 1295 | loss_total 0.2313\n",
      "step 1/3 | epoch 22/50 | batch 36/60 | global_step 1296 | loss_total 0.2247\n",
      "step 1/3 | epoch 22/50 | batch 37/60 | global_step 1297 | loss_total 0.2217\n",
      "step 1/3 | epoch 22/50 | batch 38/60 | global_step 1298 | loss_total 1.0263\n",
      "step 1/3 | epoch 22/50 | batch 39/60 | global_step 1299 | loss_total 0.4785\n",
      "step 1/3 | epoch 22/50 | batch 40/60 | global_step 1300 | loss_total 0.2553\n",
      "step 1/3 | epoch 22/50 | batch 41/60 | global_step 1301 | loss_total 0.3494\n",
      "step 1/3 | epoch 22/50 | batch 42/60 | global_step 1302 | loss_total 0.2336\n",
      "step 1/3 | epoch 22/50 | batch 43/60 | global_step 1303 | loss_total 0.2543\n",
      "step 1/3 | epoch 22/50 | batch 44/60 | global_step 1304 | loss_total 0.9676\n",
      "step 1/3 | epoch 22/50 | batch 45/60 | global_step 1305 | loss_total 0.2388\n",
      "step 1/3 | epoch 22/50 | batch 46/60 | global_step 1306 | loss_total 0.2552\n",
      "step 1/3 | epoch 22/50 | batch 47/60 | global_step 1307 | loss_total 0.7475\n",
      "step 1/3 | epoch 22/50 | batch 48/60 | global_step 1308 | loss_total 0.2648\n",
      "step 1/3 | epoch 22/50 | batch 49/60 | global_step 1309 | loss_total 0.3019\n",
      "step 1/3 | epoch 22/50 | batch 50/60 | global_step 1310 | loss_total 0.2344\n",
      "step 1/3 | epoch 22/50 | batch 51/60 | global_step 1311 | loss_total 0.2355\n",
      "step 1/3 | epoch 22/50 | batch 52/60 | global_step 1312 | loss_total 0.2333\n",
      "step 1/3 | epoch 22/50 | batch 53/60 | global_step 1313 | loss_total 0.2785\n",
      "step 1/3 | epoch 22/50 | batch 54/60 | global_step 1314 | loss_total 0.3339\n",
      "step 1/3 | epoch 22/50 | batch 55/60 | global_step 1315 | loss_total 0.8167\n",
      "step 1/3 | epoch 22/50 | batch 56/60 | global_step 1316 | loss_total 0.4126\n",
      "step 1/3 | epoch 22/50 | batch 57/60 | global_step 1317 | loss_total 0.2167\n",
      "step 1/3 | epoch 22/50 | batch 58/60 | global_step 1318 | loss_total 0.2388\n",
      "step 1/3 | epoch 22/50 | batch 59/60 | global_step 1319 | loss_total 1.0778\n",
      "step 1/3 | epoch 22/50 | batch 60/60 | global_step 1320 | loss_total 0.9914\n",
      "[epoch done] step 1/3 epoch 22/50 | train_total=0.4486 val_total=0.2503\n",
      "step 1/3 | epoch 23/50 | batch 1/60 | global_step 1321 | loss_total 0.2852\n",
      "step 1/3 | epoch 23/50 | batch 2/60 | global_step 1322 | loss_total 0.2361\n",
      "step 1/3 | epoch 23/50 | batch 3/60 | global_step 1323 | loss_total 0.3203\n",
      "step 1/3 | epoch 23/50 | batch 4/60 | global_step 1324 | loss_total 0.2195\n",
      "step 1/3 | epoch 23/50 | batch 5/60 | global_step 1325 | loss_total 0.9699\n",
      "step 1/3 | epoch 23/50 | batch 6/60 | global_step 1326 | loss_total 0.6245\n",
      "step 1/3 | epoch 23/50 | batch 7/60 | global_step 1327 | loss_total 0.2784\n",
      "step 1/3 | epoch 23/50 | batch 8/60 | global_step 1328 | loss_total 0.5340\n",
      "step 1/3 | epoch 23/50 | batch 9/60 | global_step 1329 | loss_total 0.6379\n",
      "step 1/3 | epoch 23/50 | batch 10/60 | global_step 1330 | loss_total 0.3440\n",
      "step 1/3 | epoch 23/50 | batch 11/60 | global_step 1331 | loss_total 0.2356\n",
      "step 1/3 | epoch 23/50 | batch 12/60 | global_step 1332 | loss_total 0.3308\n",
      "step 1/3 | epoch 23/50 | batch 13/60 | global_step 1333 | loss_total 0.5124\n",
      "step 1/3 | epoch 23/50 | batch 14/60 | global_step 1334 | loss_total 0.9896\n",
      "step 1/3 | epoch 23/50 | batch 15/60 | global_step 1335 | loss_total 0.2998\n",
      "step 1/3 | epoch 23/50 | batch 16/60 | global_step 1336 | loss_total 0.5904\n",
      "step 1/3 | epoch 23/50 | batch 17/60 | global_step 1337 | loss_total 0.2535\n",
      "step 1/3 | epoch 23/50 | batch 18/60 | global_step 1338 | loss_total 0.4079\n",
      "step 1/3 | epoch 23/50 | batch 19/60 | global_step 1339 | loss_total 0.2881\n",
      "step 1/3 | epoch 23/50 | batch 20/60 | global_step 1340 | loss_total 0.5337\n",
      "step 1/3 | epoch 23/50 | batch 21/60 | global_step 1341 | loss_total 0.3307\n",
      "step 1/3 | epoch 23/50 | batch 22/60 | global_step 1342 | loss_total 0.2502\n",
      "step 1/3 | epoch 23/50 | batch 23/60 | global_step 1343 | loss_total 0.3068\n",
      "step 1/3 | epoch 23/50 | batch 24/60 | global_step 1344 | loss_total 0.3656\n",
      "step 1/3 | epoch 23/50 | batch 25/60 | global_step 1345 | loss_total 0.2723\n",
      "step 1/3 | epoch 23/50 | batch 26/60 | global_step 1346 | loss_total 0.2500\n",
      "step 1/3 | epoch 23/50 | batch 27/60 | global_step 1347 | loss_total 0.9480\n",
      "step 1/3 | epoch 23/50 | batch 28/60 | global_step 1348 | loss_total 0.8709\n",
      "step 1/3 | epoch 23/50 | batch 29/60 | global_step 1349 | loss_total 0.2615\n",
      "step 1/3 | epoch 23/50 | batch 30/60 | global_step 1350 | loss_total 1.3389\n",
      "step 1/3 | epoch 23/50 | batch 31/60 | global_step 1351 | loss_total 0.3485\n",
      "step 1/3 | epoch 23/50 | batch 32/60 | global_step 1352 | loss_total 0.2275\n",
      "step 1/3 | epoch 23/50 | batch 33/60 | global_step 1353 | loss_total 0.2236\n",
      "step 1/3 | epoch 23/50 | batch 34/60 | global_step 1354 | loss_total 0.3311\n",
      "step 1/3 | epoch 23/50 | batch 35/60 | global_step 1355 | loss_total 0.3640\n",
      "step 1/3 | epoch 23/50 | batch 36/60 | global_step 1356 | loss_total 0.3638\n",
      "step 1/3 | epoch 23/50 | batch 37/60 | global_step 1357 | loss_total 0.3078\n",
      "step 1/3 | epoch 23/50 | batch 38/60 | global_step 1358 | loss_total 0.2316\n",
      "step 1/3 | epoch 23/50 | batch 39/60 | global_step 1359 | loss_total 0.4223\n",
      "step 1/3 | epoch 23/50 | batch 40/60 | global_step 1360 | loss_total 0.4246\n",
      "step 1/3 | epoch 23/50 | batch 41/60 | global_step 1361 | loss_total 0.2539\n",
      "step 1/3 | epoch 23/50 | batch 42/60 | global_step 1362 | loss_total 0.3528\n",
      "step 1/3 | epoch 23/50 | batch 43/60 | global_step 1363 | loss_total 0.6015\n",
      "step 1/3 | epoch 23/50 | batch 44/60 | global_step 1364 | loss_total 0.9160\n",
      "step 1/3 | epoch 23/50 | batch 45/60 | global_step 1365 | loss_total 0.2524\n",
      "step 1/3 | epoch 23/50 | batch 46/60 | global_step 1366 | loss_total 0.2855\n",
      "step 1/3 | epoch 23/50 | batch 47/60 | global_step 1367 | loss_total 0.3841\n",
      "step 1/3 | epoch 23/50 | batch 48/60 | global_step 1368 | loss_total 0.2843\n",
      "step 1/3 | epoch 23/50 | batch 49/60 | global_step 1369 | loss_total 0.4276\n",
      "step 1/3 | epoch 23/50 | batch 50/60 | global_step 1370 | loss_total 0.5288\n",
      "step 1/3 | epoch 23/50 | batch 51/60 | global_step 1371 | loss_total 0.2372\n",
      "step 1/3 | epoch 23/50 | batch 52/60 | global_step 1372 | loss_total 0.4816\n",
      "step 1/3 | epoch 23/50 | batch 53/60 | global_step 1373 | loss_total 0.2461\n",
      "step 1/3 | epoch 23/50 | batch 54/60 | global_step 1374 | loss_total 0.2623\n",
      "step 1/3 | epoch 23/50 | batch 55/60 | global_step 1375 | loss_total 0.2544\n",
      "step 1/3 | epoch 23/50 | batch 56/60 | global_step 1376 | loss_total 0.2456\n",
      "step 1/3 | epoch 23/50 | batch 57/60 | global_step 1377 | loss_total 0.2601\n",
      "step 1/3 | epoch 23/50 | batch 58/60 | global_step 1378 | loss_total 0.2387\n",
      "step 1/3 | epoch 23/50 | batch 59/60 | global_step 1379 | loss_total 0.2405\n",
      "step 1/3 | epoch 23/50 | batch 60/60 | global_step 1380 | loss_total 1.4128\n",
      "[epoch done] step 1/3 epoch 23/50 | train_total=0.4250 val_total=0.2051\n",
      "step 1/3 | epoch 24/50 | batch 1/60 | global_step 1381 | loss_total 0.2583\n",
      "step 1/3 | epoch 24/50 | batch 2/60 | global_step 1382 | loss_total 0.2418\n",
      "step 1/3 | epoch 24/50 | batch 3/60 | global_step 1383 | loss_total 0.2825\n",
      "step 1/3 | epoch 24/50 | batch 4/60 | global_step 1384 | loss_total 0.2905\n",
      "step 1/3 | epoch 24/50 | batch 5/60 | global_step 1385 | loss_total 0.6350\n",
      "step 1/3 | epoch 24/50 | batch 6/60 | global_step 1386 | loss_total 0.3975\n",
      "step 1/3 | epoch 24/50 | batch 7/60 | global_step 1387 | loss_total 0.2636\n",
      "step 1/3 | epoch 24/50 | batch 8/60 | global_step 1388 | loss_total 0.3621\n",
      "step 1/3 | epoch 24/50 | batch 9/60 | global_step 1389 | loss_total 0.4472\n",
      "step 1/3 | epoch 24/50 | batch 10/60 | global_step 1390 | loss_total 0.2461\n",
      "step 1/3 | epoch 24/50 | batch 11/60 | global_step 1391 | loss_total 0.2637\n",
      "step 1/3 | epoch 24/50 | batch 12/60 | global_step 1392 | loss_total 0.2366\n",
      "step 1/3 | epoch 24/50 | batch 13/60 | global_step 1393 | loss_total 0.2437\n",
      "step 1/3 | epoch 24/50 | batch 14/60 | global_step 1394 | loss_total 0.2374\n",
      "step 1/3 | epoch 24/50 | batch 15/60 | global_step 1395 | loss_total 0.9423\n",
      "step 1/3 | epoch 24/50 | batch 16/60 | global_step 1396 | loss_total 0.2549\n",
      "step 1/3 | epoch 24/50 | batch 17/60 | global_step 1397 | loss_total 0.2691\n",
      "step 1/3 | epoch 24/50 | batch 18/60 | global_step 1398 | loss_total 0.4783\n",
      "step 1/3 | epoch 24/50 | batch 19/60 | global_step 1399 | loss_total 0.2757\n",
      "step 1/3 | epoch 24/50 | batch 20/60 | global_step 1400 | loss_total 0.2341\n",
      "step 1/3 | epoch 24/50 | batch 21/60 | global_step 1401 | loss_total 0.8230\n",
      "step 1/3 | epoch 24/50 | batch 22/60 | global_step 1402 | loss_total 0.5980\n",
      "step 1/3 | epoch 24/50 | batch 23/60 | global_step 1403 | loss_total 0.2365\n",
      "step 1/3 | epoch 24/50 | batch 24/60 | global_step 1404 | loss_total 0.2230\n",
      "step 1/3 | epoch 24/50 | batch 25/60 | global_step 1405 | loss_total 0.2256\n",
      "step 1/3 | epoch 24/50 | batch 26/60 | global_step 1406 | loss_total 0.2238\n",
      "step 1/3 | epoch 24/50 | batch 27/60 | global_step 1407 | loss_total 0.2565\n",
      "step 1/3 | epoch 24/50 | batch 28/60 | global_step 1408 | loss_total 0.2393\n",
      "step 1/3 | epoch 24/50 | batch 29/60 | global_step 1409 | loss_total 0.2273\n",
      "step 1/3 | epoch 24/50 | batch 30/60 | global_step 1410 | loss_total 0.3655\n",
      "step 1/3 | epoch 24/50 | batch 31/60 | global_step 1411 | loss_total 0.5550\n",
      "step 1/3 | epoch 24/50 | batch 32/60 | global_step 1412 | loss_total 0.3585\n",
      "step 1/3 | epoch 24/50 | batch 33/60 | global_step 1413 | loss_total 0.2902\n",
      "step 1/3 | epoch 24/50 | batch 34/60 | global_step 1414 | loss_total 0.3258\n",
      "step 1/3 | epoch 24/50 | batch 35/60 | global_step 1415 | loss_total 0.2385\n",
      "step 1/3 | epoch 24/50 | batch 36/60 | global_step 1416 | loss_total 0.2150\n",
      "step 1/3 | epoch 24/50 | batch 37/60 | global_step 1417 | loss_total 1.2980\n",
      "step 1/3 | epoch 24/50 | batch 38/60 | global_step 1418 | loss_total 0.2822\n",
      "step 1/3 | epoch 24/50 | batch 39/60 | global_step 1419 | loss_total 0.2159\n",
      "step 1/3 | epoch 24/50 | batch 40/60 | global_step 1420 | loss_total 0.2153\n",
      "step 1/3 | epoch 24/50 | batch 41/60 | global_step 1421 | loss_total 0.2395\n",
      "step 1/3 | epoch 24/50 | batch 42/60 | global_step 1422 | loss_total 0.2202\n",
      "step 1/3 | epoch 24/50 | batch 43/60 | global_step 1423 | loss_total 0.2443\n",
      "step 1/3 | epoch 24/50 | batch 44/60 | global_step 1424 | loss_total 0.2629\n",
      "step 1/3 | epoch 24/50 | batch 45/60 | global_step 1425 | loss_total 0.7567\n",
      "step 1/3 | epoch 24/50 | batch 46/60 | global_step 1426 | loss_total 0.2146\n",
      "step 1/3 | epoch 24/50 | batch 47/60 | global_step 1427 | loss_total 0.9992\n",
      "step 1/3 | epoch 24/50 | batch 48/60 | global_step 1428 | loss_total 0.5925\n",
      "step 1/3 | epoch 24/50 | batch 49/60 | global_step 1429 | loss_total 0.5753\n",
      "step 1/3 | epoch 24/50 | batch 50/60 | global_step 1430 | loss_total 0.2214\n",
      "step 1/3 | epoch 24/50 | batch 51/60 | global_step 1431 | loss_total 0.3508\n",
      "step 1/3 | epoch 24/50 | batch 52/60 | global_step 1432 | loss_total 1.0576\n",
      "step 1/3 | epoch 24/50 | batch 53/60 | global_step 1433 | loss_total 0.2183\n",
      "step 1/3 | epoch 24/50 | batch 54/60 | global_step 1434 | loss_total 0.2344\n",
      "step 1/3 | epoch 24/50 | batch 55/60 | global_step 1435 | loss_total 0.3844\n",
      "step 1/3 | epoch 24/50 | batch 56/60 | global_step 1436 | loss_total 0.2182\n",
      "step 1/3 | epoch 24/50 | batch 57/60 | global_step 1437 | loss_total 1.6503\n",
      "step 1/3 | epoch 24/50 | batch 58/60 | global_step 1438 | loss_total 0.2124\n",
      "step 1/3 | epoch 24/50 | batch 59/60 | global_step 1439 | loss_total 0.2190\n",
      "step 1/3 | epoch 24/50 | batch 60/60 | global_step 1440 | loss_total 0.2225\n",
      "[epoch done] step 1/3 epoch 24/50 | train_total=0.3895 val_total=0.2090\n",
      "step 1/3 | epoch 25/50 | batch 1/60 | global_step 1441 | loss_total 0.2163\n",
      "step 1/3 | epoch 25/50 | batch 2/60 | global_step 1442 | loss_total 0.2214\n",
      "step 1/3 | epoch 25/50 | batch 3/60 | global_step 1443 | loss_total 0.2138\n",
      "step 1/3 | epoch 25/50 | batch 4/60 | global_step 1444 | loss_total 0.2361\n",
      "step 1/3 | epoch 25/50 | batch 5/60 | global_step 1445 | loss_total 0.6248\n",
      "step 1/3 | epoch 25/50 | batch 6/60 | global_step 1446 | loss_total 0.2198\n",
      "step 1/3 | epoch 25/50 | batch 7/60 | global_step 1447 | loss_total 0.3309\n",
      "step 1/3 | epoch 25/50 | batch 8/60 | global_step 1448 | loss_total 0.2375\n",
      "step 1/3 | epoch 25/50 | batch 9/60 | global_step 1449 | loss_total 0.4346\n",
      "step 1/3 | epoch 25/50 | batch 10/60 | global_step 1450 | loss_total 0.2375\n",
      "step 1/3 | epoch 25/50 | batch 11/60 | global_step 1451 | loss_total 0.3011\n",
      "step 1/3 | epoch 25/50 | batch 12/60 | global_step 1452 | loss_total 0.2159\n",
      "step 1/3 | epoch 25/50 | batch 13/60 | global_step 1453 | loss_total 1.1374\n",
      "step 1/3 | epoch 25/50 | batch 14/60 | global_step 1454 | loss_total 0.2381\n",
      "step 1/3 | epoch 25/50 | batch 15/60 | global_step 1455 | loss_total 0.8883\n",
      "step 1/3 | epoch 25/50 | batch 16/60 | global_step 1456 | loss_total 0.2100\n",
      "step 1/3 | epoch 25/50 | batch 17/60 | global_step 1457 | loss_total 0.2127\n",
      "step 1/3 | epoch 25/50 | batch 18/60 | global_step 1458 | loss_total 0.2344\n",
      "step 1/3 | epoch 25/50 | batch 19/60 | global_step 1459 | loss_total 0.3344\n",
      "step 1/3 | epoch 25/50 | batch 20/60 | global_step 1460 | loss_total 0.3327\n",
      "step 1/3 | epoch 25/50 | batch 21/60 | global_step 1461 | loss_total 0.2353\n",
      "step 1/3 | epoch 25/50 | batch 22/60 | global_step 1462 | loss_total 1.2005\n",
      "step 1/3 | epoch 25/50 | batch 23/60 | global_step 1463 | loss_total 1.0656\n",
      "step 1/3 | epoch 25/50 | batch 24/60 | global_step 1464 | loss_total 0.5177\n",
      "step 1/3 | epoch 25/50 | batch 25/60 | global_step 1465 | loss_total 1.2299\n",
      "step 1/3 | epoch 25/50 | batch 26/60 | global_step 1466 | loss_total 1.1502\n",
      "step 1/3 | epoch 25/50 | batch 27/60 | global_step 1467 | loss_total 0.4208\n",
      "step 1/3 | epoch 25/50 | batch 28/60 | global_step 1468 | loss_total 0.3491\n",
      "step 1/3 | epoch 25/50 | batch 29/60 | global_step 1469 | loss_total 0.5707\n",
      "step 1/3 | epoch 25/50 | batch 30/60 | global_step 1470 | loss_total 0.2871\n",
      "step 1/3 | epoch 25/50 | batch 31/60 | global_step 1471 | loss_total 0.3711\n",
      "step 1/3 | epoch 25/50 | batch 32/60 | global_step 1472 | loss_total 0.3557\n",
      "step 1/3 | epoch 25/50 | batch 33/60 | global_step 1473 | loss_total 0.4532\n",
      "step 1/3 | epoch 25/50 | batch 34/60 | global_step 1474 | loss_total 0.2505\n",
      "step 1/3 | epoch 25/50 | batch 35/60 | global_step 1475 | loss_total 0.2613\n",
      "step 1/3 | epoch 25/50 | batch 36/60 | global_step 1476 | loss_total 0.6084\n",
      "step 1/3 | epoch 25/50 | batch 37/60 | global_step 1477 | loss_total 0.2474\n",
      "step 1/3 | epoch 25/50 | batch 38/60 | global_step 1478 | loss_total 0.3547\n",
      "step 1/3 | epoch 25/50 | batch 39/60 | global_step 1479 | loss_total 0.4620\n",
      "step 1/3 | epoch 25/50 | batch 40/60 | global_step 1480 | loss_total 0.4365\n",
      "step 1/3 | epoch 25/50 | batch 41/60 | global_step 1481 | loss_total 0.4771\n",
      "step 1/3 | epoch 25/50 | batch 42/60 | global_step 1482 | loss_total 0.5381\n",
      "step 1/3 | epoch 25/50 | batch 43/60 | global_step 1483 | loss_total 0.2463\n",
      "step 1/3 | epoch 25/50 | batch 44/60 | global_step 1484 | loss_total 0.2522\n",
      "step 1/3 | epoch 25/50 | batch 45/60 | global_step 1485 | loss_total 0.2438\n",
      "step 1/3 | epoch 25/50 | batch 46/60 | global_step 1486 | loss_total 0.2454\n",
      "step 1/3 | epoch 25/50 | batch 47/60 | global_step 1487 | loss_total 0.2469\n",
      "step 1/3 | epoch 25/50 | batch 48/60 | global_step 1488 | loss_total 0.2884\n",
      "step 1/3 | epoch 25/50 | batch 49/60 | global_step 1489 | loss_total 0.2382\n",
      "step 1/3 | epoch 25/50 | batch 50/60 | global_step 1490 | loss_total 1.3332\n",
      "step 1/3 | epoch 25/50 | batch 51/60 | global_step 1491 | loss_total 0.3168\n",
      "step 1/3 | epoch 25/50 | batch 52/60 | global_step 1492 | loss_total 0.5765\n",
      "step 1/3 | epoch 25/50 | batch 53/60 | global_step 1493 | loss_total 0.2371\n",
      "step 1/3 | epoch 25/50 | batch 54/60 | global_step 1494 | loss_total 0.3969\n",
      "step 1/3 | epoch 25/50 | batch 55/60 | global_step 1495 | loss_total 0.2311\n",
      "step 1/3 | epoch 25/50 | batch 56/60 | global_step 1496 | loss_total 0.2430\n",
      "step 1/3 | epoch 25/50 | batch 57/60 | global_step 1497 | loss_total 0.2848\n",
      "step 1/3 | epoch 25/50 | batch 58/60 | global_step 1498 | loss_total 0.4306\n",
      "step 1/3 | epoch 25/50 | batch 59/60 | global_step 1499 | loss_total 0.2640\n",
      "step 1/3 | epoch 25/50 | batch 60/60 | global_step 1500 | loss_total 0.2531\n",
      "[epoch done] step 1/3 epoch 25/50 | train_total=0.4208 val_total=0.2434\n",
      "step 1/3 | epoch 26/50 | batch 1/60 | global_step 1501 | loss_total 0.4898\n",
      "step 1/3 | epoch 26/50 | batch 2/60 | global_step 1502 | loss_total 0.2522\n",
      "step 1/3 | epoch 26/50 | batch 3/60 | global_step 1503 | loss_total 0.4608\n",
      "step 1/3 | epoch 26/50 | batch 4/60 | global_step 1504 | loss_total 0.7391\n",
      "step 1/3 | epoch 26/50 | batch 5/60 | global_step 1505 | loss_total 0.9151\n",
      "step 1/3 | epoch 26/50 | batch 6/60 | global_step 1506 | loss_total 0.4129\n",
      "step 1/3 | epoch 26/50 | batch 7/60 | global_step 1507 | loss_total 0.3581\n",
      "step 1/3 | epoch 26/50 | batch 8/60 | global_step 1508 | loss_total 0.2679\n",
      "step 1/3 | epoch 26/50 | batch 9/60 | global_step 1509 | loss_total 0.2352\n",
      "step 1/3 | epoch 26/50 | batch 10/60 | global_step 1510 | loss_total 0.2831\n",
      "step 1/3 | epoch 26/50 | batch 11/60 | global_step 1511 | loss_total 0.2272\n",
      "step 1/3 | epoch 26/50 | batch 12/60 | global_step 1512 | loss_total 0.4062\n",
      "step 1/3 | epoch 26/50 | batch 13/60 | global_step 1513 | loss_total 0.7137\n",
      "step 1/3 | epoch 26/50 | batch 14/60 | global_step 1514 | loss_total 0.5055\n",
      "step 1/3 | epoch 26/50 | batch 15/60 | global_step 1515 | loss_total 0.2571\n",
      "step 1/3 | epoch 26/50 | batch 16/60 | global_step 1516 | loss_total 1.5576\n",
      "step 1/3 | epoch 26/50 | batch 17/60 | global_step 1517 | loss_total 0.2321\n",
      "step 1/3 | epoch 26/50 | batch 18/60 | global_step 1518 | loss_total 0.5737\n",
      "step 1/3 | epoch 26/50 | batch 19/60 | global_step 1519 | loss_total 0.3132\n",
      "step 1/3 | epoch 26/50 | batch 20/60 | global_step 1520 | loss_total 0.2354\n",
      "step 1/3 | epoch 26/50 | batch 21/60 | global_step 1521 | loss_total 0.2544\n",
      "step 1/3 | epoch 26/50 | batch 22/60 | global_step 1522 | loss_total 0.2602\n",
      "step 1/3 | epoch 26/50 | batch 23/60 | global_step 1523 | loss_total 0.2359\n",
      "step 1/3 | epoch 26/50 | batch 24/60 | global_step 1524 | loss_total 0.2302\n",
      "step 1/3 | epoch 26/50 | batch 25/60 | global_step 1525 | loss_total 0.2877\n",
      "step 1/3 | epoch 26/50 | batch 26/60 | global_step 1526 | loss_total 0.2281\n",
      "step 1/3 | epoch 26/50 | batch 27/60 | global_step 1527 | loss_total 0.3991\n",
      "step 1/3 | epoch 26/50 | batch 28/60 | global_step 1528 | loss_total 0.5038\n",
      "step 1/3 | epoch 26/50 | batch 29/60 | global_step 1529 | loss_total 0.2277\n",
      "step 1/3 | epoch 26/50 | batch 30/60 | global_step 1530 | loss_total 0.2334\n",
      "step 1/3 | epoch 26/50 | batch 31/60 | global_step 1531 | loss_total 0.2255\n",
      "step 1/3 | epoch 26/50 | batch 32/60 | global_step 1532 | loss_total 0.2441\n",
      "step 1/3 | epoch 26/50 | batch 33/60 | global_step 1533 | loss_total 0.7641\n",
      "step 1/3 | epoch 26/50 | batch 34/60 | global_step 1534 | loss_total 0.3406\n",
      "step 1/3 | epoch 26/50 | batch 35/60 | global_step 1535 | loss_total 0.5100\n",
      "step 1/3 | epoch 26/50 | batch 36/60 | global_step 1536 | loss_total 0.2536\n",
      "step 1/3 | epoch 26/50 | batch 37/60 | global_step 1537 | loss_total 0.7052\n",
      "step 1/3 | epoch 26/50 | batch 38/60 | global_step 1538 | loss_total 0.2396\n",
      "step 1/3 | epoch 26/50 | batch 39/60 | global_step 1539 | loss_total 0.2447\n",
      "step 1/3 | epoch 26/50 | batch 40/60 | global_step 1540 | loss_total 0.4128\n",
      "step 1/3 | epoch 26/50 | batch 41/60 | global_step 1541 | loss_total 0.3223\n",
      "step 1/3 | epoch 26/50 | batch 42/60 | global_step 1542 | loss_total 0.3123\n",
      "step 1/3 | epoch 26/50 | batch 43/60 | global_step 1543 | loss_total 0.2320\n",
      "step 1/3 | epoch 26/50 | batch 44/60 | global_step 1544 | loss_total 0.2134\n",
      "step 1/3 | epoch 26/50 | batch 45/60 | global_step 1545 | loss_total 0.2458\n",
      "step 1/3 | epoch 26/50 | batch 46/60 | global_step 1546 | loss_total 0.2523\n",
      "step 1/3 | epoch 26/50 | batch 47/60 | global_step 1547 | loss_total 0.2188\n",
      "step 1/3 | epoch 26/50 | batch 48/60 | global_step 1548 | loss_total 0.4289\n",
      "step 1/3 | epoch 26/50 | batch 49/60 | global_step 1549 | loss_total 0.2538\n",
      "step 1/3 | epoch 26/50 | batch 50/60 | global_step 1550 | loss_total 0.2431\n",
      "step 1/3 | epoch 26/50 | batch 51/60 | global_step 1551 | loss_total 0.2829\n",
      "step 1/3 | epoch 26/50 | batch 52/60 | global_step 1552 | loss_total 0.6100\n",
      "step 1/3 | epoch 26/50 | batch 53/60 | global_step 1553 | loss_total 0.2561\n",
      "step 1/3 | epoch 26/50 | batch 54/60 | global_step 1554 | loss_total 0.2255\n",
      "step 1/3 | epoch 26/50 | batch 55/60 | global_step 1555 | loss_total 0.7728\n",
      "step 1/3 | epoch 26/50 | batch 56/60 | global_step 1556 | loss_total 0.2127\n",
      "step 1/3 | epoch 26/50 | batch 57/60 | global_step 1557 | loss_total 0.2387\n",
      "step 1/3 | epoch 26/50 | batch 58/60 | global_step 1558 | loss_total 0.2259\n",
      "step 1/3 | epoch 26/50 | batch 59/60 | global_step 1559 | loss_total 0.2294\n",
      "step 1/3 | epoch 26/50 | batch 60/60 | global_step 1560 | loss_total 0.9132\n",
      "[epoch done] step 1/3 epoch 26/50 | train_total=0.3821 val_total=0.2068\n",
      "step 1/3 | epoch 27/50 | batch 1/60 | global_step 1561 | loss_total 0.2230\n",
      "step 1/3 | epoch 27/50 | batch 2/60 | global_step 1562 | loss_total 0.2168\n",
      "step 1/3 | epoch 27/50 | batch 3/60 | global_step 1563 | loss_total 0.2226\n",
      "step 1/3 | epoch 27/50 | batch 4/60 | global_step 1564 | loss_total 0.2550\n",
      "step 1/3 | epoch 27/50 | batch 5/60 | global_step 1565 | loss_total 0.2277\n",
      "step 1/3 | epoch 27/50 | batch 6/60 | global_step 1566 | loss_total 0.6976\n",
      "step 1/3 | epoch 27/50 | batch 7/60 | global_step 1567 | loss_total 0.2212\n",
      "step 1/3 | epoch 27/50 | batch 8/60 | global_step 1568 | loss_total 0.2238\n",
      "step 1/3 | epoch 27/50 | batch 9/60 | global_step 1569 | loss_total 0.3529\n",
      "step 1/3 | epoch 27/50 | batch 10/60 | global_step 1570 | loss_total 0.2189\n",
      "step 1/3 | epoch 27/50 | batch 11/60 | global_step 1571 | loss_total 0.2698\n",
      "step 1/3 | epoch 27/50 | batch 12/60 | global_step 1572 | loss_total 0.2462\n",
      "step 1/3 | epoch 27/50 | batch 13/60 | global_step 1573 | loss_total 0.2215\n",
      "step 1/3 | epoch 27/50 | batch 14/60 | global_step 1574 | loss_total 0.3531\n",
      "step 1/3 | epoch 27/50 | batch 15/60 | global_step 1575 | loss_total 0.2231\n",
      "step 1/3 | epoch 27/50 | batch 16/60 | global_step 1576 | loss_total 0.2188\n",
      "step 1/3 | epoch 27/50 | batch 17/60 | global_step 1577 | loss_total 0.5864\n",
      "step 1/3 | epoch 27/50 | batch 18/60 | global_step 1578 | loss_total 0.2181\n",
      "step 1/3 | epoch 27/50 | batch 19/60 | global_step 1579 | loss_total 0.2251\n",
      "step 1/3 | epoch 27/50 | batch 20/60 | global_step 1580 | loss_total 0.5877\n",
      "step 1/3 | epoch 27/50 | batch 21/60 | global_step 1581 | loss_total 0.7245\n",
      "step 1/3 | epoch 27/50 | batch 22/60 | global_step 1582 | loss_total 0.2202\n",
      "step 1/3 | epoch 27/50 | batch 23/60 | global_step 1583 | loss_total 0.2298\n",
      "step 1/3 | epoch 27/50 | batch 24/60 | global_step 1584 | loss_total 0.2793\n",
      "step 1/3 | epoch 27/50 | batch 25/60 | global_step 1585 | loss_total 0.7730\n",
      "step 1/3 | epoch 27/50 | batch 26/60 | global_step 1586 | loss_total 0.2765\n",
      "step 1/3 | epoch 27/50 | batch 27/60 | global_step 1587 | loss_total 0.2299\n",
      "step 1/3 | epoch 27/50 | batch 28/60 | global_step 1588 | loss_total 0.2442\n",
      "step 1/3 | epoch 27/50 | batch 29/60 | global_step 1589 | loss_total 0.2242\n",
      "step 1/3 | epoch 27/50 | batch 30/60 | global_step 1590 | loss_total 0.4395\n",
      "step 1/3 | epoch 27/50 | batch 31/60 | global_step 1591 | loss_total 0.2287\n",
      "step 1/3 | epoch 27/50 | batch 32/60 | global_step 1592 | loss_total 0.4328\n",
      "step 1/3 | epoch 27/50 | batch 33/60 | global_step 1593 | loss_total 0.2514\n",
      "step 1/3 | epoch 27/50 | batch 34/60 | global_step 1594 | loss_total 0.3190\n",
      "step 1/3 | epoch 27/50 | batch 35/60 | global_step 1595 | loss_total 0.7086\n",
      "step 1/3 | epoch 27/50 | batch 36/60 | global_step 1596 | loss_total 0.5775\n",
      "step 1/3 | epoch 27/50 | batch 37/60 | global_step 1597 | loss_total 0.2237\n",
      "step 1/3 | epoch 27/50 | batch 38/60 | global_step 1598 | loss_total 0.2413\n",
      "step 1/3 | epoch 27/50 | batch 39/60 | global_step 1599 | loss_total 0.2172\n",
      "step 1/3 | epoch 27/50 | batch 40/60 | global_step 1600 | loss_total 0.3758\n",
      "step 1/3 | epoch 27/50 | batch 41/60 | global_step 1601 | loss_total 0.2146\n",
      "step 1/3 | epoch 27/50 | batch 42/60 | global_step 1602 | loss_total 0.2229\n",
      "step 1/3 | epoch 27/50 | batch 43/60 | global_step 1603 | loss_total 0.5168\n",
      "step 1/3 | epoch 27/50 | batch 44/60 | global_step 1604 | loss_total 0.2200\n",
      "step 1/3 | epoch 27/50 | batch 45/60 | global_step 1605 | loss_total 0.2318\n",
      "step 1/3 | epoch 27/50 | batch 46/60 | global_step 1606 | loss_total 0.2223\n",
      "step 1/3 | epoch 27/50 | batch 47/60 | global_step 1607 | loss_total 0.2249\n",
      "step 1/3 | epoch 27/50 | batch 48/60 | global_step 1608 | loss_total 0.2341\n",
      "step 1/3 | epoch 27/50 | batch 49/60 | global_step 1609 | loss_total 0.3923\n",
      "step 1/3 | epoch 27/50 | batch 50/60 | global_step 1610 | loss_total 0.2571\n",
      "step 1/3 | epoch 27/50 | batch 51/60 | global_step 1611 | loss_total 0.8552\n",
      "step 1/3 | epoch 27/50 | batch 52/60 | global_step 1612 | loss_total 0.2124\n",
      "step 1/3 | epoch 27/50 | batch 53/60 | global_step 1613 | loss_total 0.7253\n",
      "step 1/3 | epoch 27/50 | batch 54/60 | global_step 1614 | loss_total 0.2398\n",
      "step 1/3 | epoch 27/50 | batch 55/60 | global_step 1615 | loss_total 0.2185\n",
      "step 1/3 | epoch 27/50 | batch 56/60 | global_step 1616 | loss_total 0.3727\n",
      "step 1/3 | epoch 27/50 | batch 57/60 | global_step 1617 | loss_total 1.3167\n",
      "step 1/3 | epoch 27/50 | batch 58/60 | global_step 1618 | loss_total 0.4377\n",
      "step 1/3 | epoch 27/50 | batch 59/60 | global_step 1619 | loss_total 0.9147\n",
      "step 1/3 | epoch 27/50 | batch 60/60 | global_step 1620 | loss_total 0.2135\n",
      "[epoch done] step 1/3 epoch 27/50 | train_total=0.3578 val_total=0.1860\n",
      "step 1/3 | epoch 28/50 | batch 1/60 | global_step 1621 | loss_total 0.2502\n",
      "step 1/3 | epoch 28/50 | batch 2/60 | global_step 1622 | loss_total 0.2243\n",
      "step 1/3 | epoch 28/50 | batch 3/60 | global_step 1623 | loss_total 0.2202\n",
      "step 1/3 | epoch 28/50 | batch 4/60 | global_step 1624 | loss_total 0.2253\n",
      "step 1/3 | epoch 28/50 | batch 5/60 | global_step 1625 | loss_total 0.7917\n",
      "step 1/3 | epoch 28/50 | batch 6/60 | global_step 1626 | loss_total 0.2235\n",
      "step 1/3 | epoch 28/50 | batch 7/60 | global_step 1627 | loss_total 0.2267\n",
      "step 1/3 | epoch 28/50 | batch 8/60 | global_step 1628 | loss_total 0.2293\n",
      "step 1/3 | epoch 28/50 | batch 9/60 | global_step 1629 | loss_total 0.6552\n",
      "step 1/3 | epoch 28/50 | batch 10/60 | global_step 1630 | loss_total 0.2244\n",
      "step 1/3 | epoch 28/50 | batch 11/60 | global_step 1631 | loss_total 0.3074\n",
      "step 1/3 | epoch 28/50 | batch 12/60 | global_step 1632 | loss_total 0.2250\n",
      "step 1/3 | epoch 28/50 | batch 13/60 | global_step 1633 | loss_total 0.2281\n",
      "step 1/3 | epoch 28/50 | batch 14/60 | global_step 1634 | loss_total 0.2268\n",
      "step 1/3 | epoch 28/50 | batch 15/60 | global_step 1635 | loss_total 0.2093\n",
      "step 1/3 | epoch 28/50 | batch 16/60 | global_step 1636 | loss_total 0.4331\n",
      "step 1/3 | epoch 28/50 | batch 17/60 | global_step 1637 | loss_total 0.1916\n",
      "step 1/3 | epoch 28/50 | batch 18/60 | global_step 1638 | loss_total 0.2030\n",
      "step 1/3 | epoch 28/50 | batch 19/60 | global_step 1639 | loss_total 0.2660\n",
      "step 1/3 | epoch 28/50 | batch 20/60 | global_step 1640 | loss_total 0.2594\n",
      "step 1/3 | epoch 28/50 | batch 21/60 | global_step 1641 | loss_total 0.4154\n",
      "step 1/3 | epoch 28/50 | batch 22/60 | global_step 1642 | loss_total 0.2230\n",
      "step 1/3 | epoch 28/50 | batch 23/60 | global_step 1643 | loss_total 0.2857\n",
      "step 1/3 | epoch 28/50 | batch 24/60 | global_step 1644 | loss_total 0.8196\n",
      "step 1/3 | epoch 28/50 | batch 25/60 | global_step 1645 | loss_total 0.2399\n",
      "step 1/3 | epoch 28/50 | batch 26/60 | global_step 1646 | loss_total 0.2617\n",
      "step 1/3 | epoch 28/50 | batch 27/60 | global_step 1647 | loss_total 0.2531\n",
      "step 1/3 | epoch 28/50 | batch 28/60 | global_step 1648 | loss_total 0.3414\n",
      "step 1/3 | epoch 28/50 | batch 29/60 | global_step 1649 | loss_total 0.5109\n",
      "step 1/3 | epoch 28/50 | batch 30/60 | global_step 1650 | loss_total 0.9016\n",
      "step 1/3 | epoch 28/50 | batch 31/60 | global_step 1651 | loss_total 0.3102\n",
      "step 1/3 | epoch 28/50 | batch 32/60 | global_step 1652 | loss_total 0.2322\n",
      "step 1/3 | epoch 28/50 | batch 33/60 | global_step 1653 | loss_total 0.3531\n",
      "step 1/3 | epoch 28/50 | batch 34/60 | global_step 1654 | loss_total 0.3448\n",
      "step 1/3 | epoch 28/50 | batch 35/60 | global_step 1655 | loss_total 0.2311\n",
      "step 1/3 | epoch 28/50 | batch 36/60 | global_step 1656 | loss_total 0.5685\n",
      "step 1/3 | epoch 28/50 | batch 37/60 | global_step 1657 | loss_total 0.2225\n",
      "step 1/3 | epoch 28/50 | batch 38/60 | global_step 1658 | loss_total 0.7140\n",
      "step 1/3 | epoch 28/50 | batch 39/60 | global_step 1659 | loss_total 0.2227\n",
      "step 1/3 | epoch 28/50 | batch 40/60 | global_step 1660 | loss_total 0.2231\n",
      "step 1/3 | epoch 28/50 | batch 41/60 | global_step 1661 | loss_total 0.3076\n",
      "step 1/3 | epoch 28/50 | batch 42/60 | global_step 1662 | loss_total 0.2182\n",
      "step 1/3 | epoch 28/50 | batch 43/60 | global_step 1663 | loss_total 1.5252\n",
      "step 1/3 | epoch 28/50 | batch 44/60 | global_step 1664 | loss_total 0.4521\n",
      "step 1/3 | epoch 28/50 | batch 45/60 | global_step 1665 | loss_total 0.7019\n",
      "step 1/3 | epoch 28/50 | batch 46/60 | global_step 1666 | loss_total 0.2595\n",
      "step 1/3 | epoch 28/50 | batch 47/60 | global_step 1667 | loss_total 0.2234\n",
      "step 1/3 | epoch 28/50 | batch 48/60 | global_step 1668 | loss_total 0.2257\n",
      "step 1/3 | epoch 28/50 | batch 49/60 | global_step 1669 | loss_total 0.5990\n",
      "step 1/3 | epoch 28/50 | batch 50/60 | global_step 1670 | loss_total 0.6995\n",
      "step 1/3 | epoch 28/50 | batch 51/60 | global_step 1671 | loss_total 0.4278\n",
      "step 1/3 | epoch 28/50 | batch 52/60 | global_step 1672 | loss_total 0.2370\n",
      "step 1/3 | epoch 28/50 | batch 53/60 | global_step 1673 | loss_total 0.3418\n",
      "step 1/3 | epoch 28/50 | batch 54/60 | global_step 1674 | loss_total 0.2083\n",
      "step 1/3 | epoch 28/50 | batch 55/60 | global_step 1675 | loss_total 0.3430\n",
      "step 1/3 | epoch 28/50 | batch 56/60 | global_step 1676 | loss_total 0.3184\n",
      "step 1/3 | epoch 28/50 | batch 57/60 | global_step 1677 | loss_total 0.2047\n",
      "step 1/3 | epoch 28/50 | batch 58/60 | global_step 1678 | loss_total 0.2040\n",
      "step 1/3 | epoch 28/50 | batch 59/60 | global_step 1679 | loss_total 1.3189\n",
      "step 1/3 | epoch 28/50 | batch 60/60 | global_step 1680 | loss_total 0.2544\n",
      "[epoch done] step 1/3 epoch 28/50 | train_total=0.3761 val_total=0.2518\n",
      "step 1/3 | epoch 29/50 | batch 1/60 | global_step 1681 | loss_total 0.7162\n",
      "step 1/3 | epoch 29/50 | batch 2/60 | global_step 1682 | loss_total 0.2002\n",
      "step 1/3 | epoch 29/50 | batch 3/60 | global_step 1683 | loss_total 0.2274\n",
      "step 1/3 | epoch 29/50 | batch 4/60 | global_step 1684 | loss_total 0.2296\n",
      "step 1/3 | epoch 29/50 | batch 5/60 | global_step 1685 | loss_total 0.9744\n",
      "step 1/3 | epoch 29/50 | batch 6/60 | global_step 1686 | loss_total 0.2838\n",
      "step 1/3 | epoch 29/50 | batch 7/60 | global_step 1687 | loss_total 0.2499\n",
      "step 1/3 | epoch 29/50 | batch 8/60 | global_step 1688 | loss_total 0.2604\n",
      "step 1/3 | epoch 29/50 | batch 9/60 | global_step 1689 | loss_total 0.2939\n",
      "step 1/3 | epoch 29/50 | batch 10/60 | global_step 1690 | loss_total 0.3313\n",
      "step 1/3 | epoch 29/50 | batch 11/60 | global_step 1691 | loss_total 1.2389\n",
      "step 1/3 | epoch 29/50 | batch 12/60 | global_step 1692 | loss_total 0.6848\n",
      "step 1/3 | epoch 29/50 | batch 13/60 | global_step 1693 | loss_total 0.2207\n",
      "step 1/3 | epoch 29/50 | batch 14/60 | global_step 1694 | loss_total 0.2328\n",
      "step 1/3 | epoch 29/50 | batch 15/60 | global_step 1695 | loss_total 0.2441\n",
      "step 1/3 | epoch 29/50 | batch 16/60 | global_step 1696 | loss_total 0.2049\n",
      "step 1/3 | epoch 29/50 | batch 17/60 | global_step 1697 | loss_total 0.3859\n",
      "step 1/3 | epoch 29/50 | batch 18/60 | global_step 1698 | loss_total 0.3905\n",
      "step 1/3 | epoch 29/50 | batch 19/60 | global_step 1699 | loss_total 0.4010\n",
      "step 1/3 | epoch 29/50 | batch 20/60 | global_step 1700 | loss_total 1.3401\n",
      "step 1/3 | epoch 29/50 | batch 21/60 | global_step 1701 | loss_total 0.2244\n",
      "step 1/3 | epoch 29/50 | batch 22/60 | global_step 1702 | loss_total 0.2354\n",
      "step 1/3 | epoch 29/50 | batch 23/60 | global_step 1703 | loss_total 0.2240\n",
      "step 1/3 | epoch 29/50 | batch 24/60 | global_step 1704 | loss_total 0.3155\n",
      "step 1/3 | epoch 29/50 | batch 25/60 | global_step 1705 | loss_total 0.2439\n",
      "step 1/3 | epoch 29/50 | batch 26/60 | global_step 1706 | loss_total 0.7322\n",
      "step 1/3 | epoch 29/50 | batch 27/60 | global_step 1707 | loss_total 0.2532\n",
      "step 1/3 | epoch 29/50 | batch 28/60 | global_step 1708 | loss_total 0.7265\n",
      "step 1/3 | epoch 29/50 | batch 29/60 | global_step 1709 | loss_total 0.7138\n",
      "step 1/3 | epoch 29/50 | batch 30/60 | global_step 1710 | loss_total 0.2208\n",
      "step 1/3 | epoch 29/50 | batch 31/60 | global_step 1711 | loss_total 0.2171\n",
      "step 1/3 | epoch 29/50 | batch 32/60 | global_step 1712 | loss_total 0.3228\n",
      "step 1/3 | epoch 29/50 | batch 33/60 | global_step 1713 | loss_total 0.2449\n",
      "step 1/3 | epoch 29/50 | batch 34/60 | global_step 1714 | loss_total 0.3530\n",
      "step 1/3 | epoch 29/50 | batch 35/60 | global_step 1715 | loss_total 0.2242\n",
      "step 1/3 | epoch 29/50 | batch 36/60 | global_step 1716 | loss_total 0.2291\n",
      "step 1/3 | epoch 29/50 | batch 37/60 | global_step 1717 | loss_total 0.6102\n",
      "step 1/3 | epoch 29/50 | batch 38/60 | global_step 1718 | loss_total 0.6502\n",
      "step 1/3 | epoch 29/50 | batch 39/60 | global_step 1719 | loss_total 0.2233\n",
      "step 1/3 | epoch 29/50 | batch 40/60 | global_step 1720 | loss_total 0.2221\n",
      "step 1/3 | epoch 29/50 | batch 41/60 | global_step 1721 | loss_total 0.6214\n",
      "step 1/3 | epoch 29/50 | batch 42/60 | global_step 1722 | loss_total 0.2210\n",
      "step 1/3 | epoch 29/50 | batch 43/60 | global_step 1723 | loss_total 0.7272\n",
      "step 1/3 | epoch 29/50 | batch 44/60 | global_step 1724 | loss_total 0.2306\n",
      "step 1/3 | epoch 29/50 | batch 45/60 | global_step 1725 | loss_total 0.2187\n",
      "step 1/3 | epoch 29/50 | batch 46/60 | global_step 1726 | loss_total 0.5598\n",
      "step 1/3 | epoch 29/50 | batch 47/60 | global_step 1727 | loss_total 0.2704\n",
      "step 1/3 | epoch 29/50 | batch 48/60 | global_step 1728 | loss_total 0.2301\n",
      "step 1/3 | epoch 29/50 | batch 49/60 | global_step 1729 | loss_total 0.2279\n",
      "step 1/3 | epoch 29/50 | batch 50/60 | global_step 1730 | loss_total 0.6613\n",
      "step 1/3 | epoch 29/50 | batch 51/60 | global_step 1731 | loss_total 0.7013\n",
      "step 1/3 | epoch 29/50 | batch 52/60 | global_step 1732 | loss_total 0.2836\n",
      "step 1/3 | epoch 29/50 | batch 53/60 | global_step 1733 | loss_total 0.2301\n",
      "step 1/3 | epoch 29/50 | batch 54/60 | global_step 1734 | loss_total 0.2457\n",
      "step 1/3 | epoch 29/50 | batch 55/60 | global_step 1735 | loss_total 0.7553\n",
      "step 1/3 | epoch 29/50 | batch 56/60 | global_step 1736 | loss_total 0.2864\n",
      "step 1/3 | epoch 29/50 | batch 57/60 | global_step 1737 | loss_total 0.2332\n",
      "step 1/3 | epoch 29/50 | batch 58/60 | global_step 1738 | loss_total 0.5598\n",
      "step 1/3 | epoch 29/50 | batch 59/60 | global_step 1739 | loss_total 0.7541\n",
      "step 1/3 | epoch 29/50 | batch 60/60 | global_step 1740 | loss_total 0.3535\n",
      "[epoch done] step 1/3 epoch 29/50 | train_total=0.4111 val_total=0.2223\n",
      "step 1/3 | epoch 30/50 | batch 1/60 | global_step 1741 | loss_total 0.2274\n",
      "step 1/3 | epoch 30/50 | batch 2/60 | global_step 1742 | loss_total 0.9765\n",
      "step 1/3 | epoch 30/50 | batch 3/60 | global_step 1743 | loss_total 0.6065\n",
      "step 1/3 | epoch 30/50 | batch 4/60 | global_step 1744 | loss_total 0.7965\n",
      "step 1/3 | epoch 30/50 | batch 5/60 | global_step 1745 | loss_total 0.2429\n",
      "step 1/3 | epoch 30/50 | batch 6/60 | global_step 1746 | loss_total 0.2569\n",
      "step 1/3 | epoch 30/50 | batch 7/60 | global_step 1747 | loss_total 0.4358\n",
      "step 1/3 | epoch 30/50 | batch 8/60 | global_step 1748 | loss_total 1.1478\n",
      "step 1/3 | epoch 30/50 | batch 9/60 | global_step 1749 | loss_total 0.2481\n",
      "step 1/3 | epoch 30/50 | batch 10/60 | global_step 1750 | loss_total 0.2314\n",
      "step 1/3 | epoch 30/50 | batch 11/60 | global_step 1751 | loss_total 0.2370\n",
      "step 1/3 | epoch 30/50 | batch 12/60 | global_step 1752 | loss_total 0.2266\n",
      "step 1/3 | epoch 30/50 | batch 13/60 | global_step 1753 | loss_total 0.6307\n",
      "step 1/3 | epoch 30/50 | batch 14/60 | global_step 1754 | loss_total 0.2297\n",
      "step 1/3 | epoch 30/50 | batch 15/60 | global_step 1755 | loss_total 0.2997\n",
      "step 1/3 | epoch 30/50 | batch 16/60 | global_step 1756 | loss_total 0.2433\n",
      "step 1/3 | epoch 30/50 | batch 17/60 | global_step 1757 | loss_total 0.4305\n",
      "step 1/3 | epoch 30/50 | batch 18/60 | global_step 1758 | loss_total 0.4362\n",
      "step 1/3 | epoch 30/50 | batch 19/60 | global_step 1759 | loss_total 0.3071\n",
      "step 1/3 | epoch 30/50 | batch 20/60 | global_step 1760 | loss_total 0.2463\n",
      "step 1/3 | epoch 30/50 | batch 21/60 | global_step 1761 | loss_total 0.2739\n",
      "step 1/3 | epoch 30/50 | batch 22/60 | global_step 1762 | loss_total 0.2414\n",
      "step 1/3 | epoch 30/50 | batch 23/60 | global_step 1763 | loss_total 0.2202\n",
      "step 1/3 | epoch 30/50 | batch 24/60 | global_step 1764 | loss_total 0.3385\n",
      "step 1/3 | epoch 30/50 | batch 25/60 | global_step 1765 | loss_total 1.0337\n",
      "step 1/3 | epoch 30/50 | batch 26/60 | global_step 1766 | loss_total 0.3948\n",
      "step 1/3 | epoch 30/50 | batch 27/60 | global_step 1767 | loss_total 0.3947\n",
      "step 1/3 | epoch 30/50 | batch 28/60 | global_step 1768 | loss_total 0.2245\n",
      "step 1/3 | epoch 30/50 | batch 29/60 | global_step 1769 | loss_total 0.3317\n",
      "step 1/3 | epoch 30/50 | batch 30/60 | global_step 1770 | loss_total 0.2087\n",
      "step 1/3 | epoch 30/50 | batch 31/60 | global_step 1771 | loss_total 1.0335\n",
      "step 1/3 | epoch 30/50 | batch 32/60 | global_step 1772 | loss_total 0.4572\n",
      "step 1/3 | epoch 30/50 | batch 33/60 | global_step 1773 | loss_total 0.2166\n",
      "step 1/3 | epoch 30/50 | batch 34/60 | global_step 1774 | loss_total 0.2374\n",
      "step 1/3 | epoch 30/50 | batch 35/60 | global_step 1775 | loss_total 0.2585\n",
      "step 1/3 | epoch 30/50 | batch 36/60 | global_step 1776 | loss_total 0.2198\n",
      "step 1/3 | epoch 30/50 | batch 37/60 | global_step 1777 | loss_total 0.2667\n",
      "step 1/3 | epoch 30/50 | batch 38/60 | global_step 1778 | loss_total 0.5496\n",
      "step 1/3 | epoch 30/50 | batch 39/60 | global_step 1779 | loss_total 0.2635\n",
      "step 1/3 | epoch 30/50 | batch 40/60 | global_step 1780 | loss_total 0.2621\n",
      "step 1/3 | epoch 30/50 | batch 41/60 | global_step 1781 | loss_total 0.5806\n",
      "step 1/3 | epoch 30/50 | batch 42/60 | global_step 1782 | loss_total 0.2353\n",
      "step 1/3 | epoch 30/50 | batch 43/60 | global_step 1783 | loss_total 0.2769\n",
      "step 1/3 | epoch 30/50 | batch 44/60 | global_step 1784 | loss_total 0.2385\n",
      "step 1/3 | epoch 30/50 | batch 45/60 | global_step 1785 | loss_total 0.5872\n",
      "step 1/3 | epoch 30/50 | batch 46/60 | global_step 1786 | loss_total 0.2400\n",
      "step 1/3 | epoch 30/50 | batch 47/60 | global_step 1787 | loss_total 0.5130\n",
      "step 1/3 | epoch 30/50 | batch 48/60 | global_step 1788 | loss_total 0.2195\n",
      "step 1/3 | epoch 30/50 | batch 49/60 | global_step 1789 | loss_total 0.3235\n",
      "step 1/3 | epoch 30/50 | batch 50/60 | global_step 1790 | loss_total 0.2936\n",
      "step 1/3 | epoch 30/50 | batch 51/60 | global_step 1791 | loss_total 0.7546\n",
      "step 1/3 | epoch 30/50 | batch 52/60 | global_step 1792 | loss_total 0.7366\n",
      "step 1/3 | epoch 30/50 | batch 53/60 | global_step 1793 | loss_total 0.2202\n",
      "step 1/3 | epoch 30/50 | batch 54/60 | global_step 1794 | loss_total 0.7546\n",
      "step 1/3 | epoch 30/50 | batch 55/60 | global_step 1795 | loss_total 0.2380\n",
      "step 1/3 | epoch 30/50 | batch 56/60 | global_step 1796 | loss_total 0.4940\n",
      "step 1/3 | epoch 30/50 | batch 57/60 | global_step 1797 | loss_total 0.2847\n",
      "step 1/3 | epoch 30/50 | batch 58/60 | global_step 1798 | loss_total 0.3948\n",
      "step 1/3 | epoch 30/50 | batch 59/60 | global_step 1799 | loss_total 0.8423\n",
      "step 1/3 | epoch 30/50 | batch 60/60 | global_step 1800 | loss_total 0.2580\n",
      "[epoch done] step 1/3 epoch 30/50 | train_total=0.4062 val_total=0.2490\n",
      "step 1/3 | epoch 31/50 | batch 1/60 | global_step 1801 | loss_total 0.2454\n",
      "step 1/3 | epoch 31/50 | batch 2/60 | global_step 1802 | loss_total 0.2931\n",
      "step 1/3 | epoch 31/50 | batch 3/60 | global_step 1803 | loss_total 0.6871\n",
      "step 1/3 | epoch 31/50 | batch 4/60 | global_step 1804 | loss_total 0.6965\n",
      "step 1/3 | epoch 31/50 | batch 5/60 | global_step 1805 | loss_total 0.3038\n",
      "step 1/3 | epoch 31/50 | batch 6/60 | global_step 1806 | loss_total 0.2478\n",
      "step 1/3 | epoch 31/50 | batch 7/60 | global_step 1807 | loss_total 0.2371\n",
      "step 1/3 | epoch 31/50 | batch 8/60 | global_step 1808 | loss_total 0.7795\n",
      "step 1/3 | epoch 31/50 | batch 9/60 | global_step 1809 | loss_total 0.3051\n",
      "step 1/3 | epoch 31/50 | batch 10/60 | global_step 1810 | loss_total 0.2267\n",
      "step 1/3 | epoch 31/50 | batch 11/60 | global_step 1811 | loss_total 0.2575\n",
      "step 1/3 | epoch 31/50 | batch 12/60 | global_step 1812 | loss_total 0.3137\n",
      "step 1/3 | epoch 31/50 | batch 13/60 | global_step 1813 | loss_total 0.2247\n",
      "step 1/3 | epoch 31/50 | batch 14/60 | global_step 1814 | loss_total 0.3229\n",
      "step 1/3 | epoch 31/50 | batch 15/60 | global_step 1815 | loss_total 0.2306\n",
      "step 1/3 | epoch 31/50 | batch 16/60 | global_step 1816 | loss_total 0.8705\n",
      "step 1/3 | epoch 31/50 | batch 17/60 | global_step 1817 | loss_total 0.3725\n",
      "step 1/3 | epoch 31/50 | batch 18/60 | global_step 1818 | loss_total 0.4560\n",
      "step 1/3 | epoch 31/50 | batch 19/60 | global_step 1819 | loss_total 0.3295\n",
      "step 1/3 | epoch 31/50 | batch 20/60 | global_step 1820 | loss_total 0.3206\n",
      "step 1/3 | epoch 31/50 | batch 21/60 | global_step 1821 | loss_total 0.2305\n",
      "step 1/3 | epoch 31/50 | batch 22/60 | global_step 1822 | loss_total 0.2976\n",
      "step 1/3 | epoch 31/50 | batch 23/60 | global_step 1823 | loss_total 0.2327\n",
      "step 1/3 | epoch 31/50 | batch 24/60 | global_step 1824 | loss_total 0.2303\n",
      "step 1/3 | epoch 31/50 | batch 25/60 | global_step 1825 | loss_total 0.2284\n",
      "step 1/3 | epoch 31/50 | batch 26/60 | global_step 1826 | loss_total 0.2260\n",
      "step 1/3 | epoch 31/50 | batch 27/60 | global_step 1827 | loss_total 0.2367\n",
      "step 1/3 | epoch 31/50 | batch 28/60 | global_step 1828 | loss_total 0.3001\n",
      "step 1/3 | epoch 31/50 | batch 29/60 | global_step 1829 | loss_total 0.2275\n",
      "step 1/3 | epoch 31/50 | batch 30/60 | global_step 1830 | loss_total 0.2272\n",
      "step 1/3 | epoch 31/50 | batch 31/60 | global_step 1831 | loss_total 0.2300\n",
      "step 1/3 | epoch 31/50 | batch 32/60 | global_step 1832 | loss_total 0.2931\n",
      "step 1/3 | epoch 31/50 | batch 33/60 | global_step 1833 | loss_total 0.2523\n",
      "step 1/3 | epoch 31/50 | batch 34/60 | global_step 1834 | loss_total 0.5482\n",
      "step 1/3 | epoch 31/50 | batch 35/60 | global_step 1835 | loss_total 0.2282\n",
      "step 1/3 | epoch 31/50 | batch 36/60 | global_step 1836 | loss_total 0.6559\n",
      "step 1/3 | epoch 31/50 | batch 37/60 | global_step 1837 | loss_total 0.2460\n",
      "step 1/3 | epoch 31/50 | batch 38/60 | global_step 1838 | loss_total 0.8154\n",
      "step 1/3 | epoch 31/50 | batch 39/60 | global_step 1839 | loss_total 0.2501\n",
      "step 1/3 | epoch 31/50 | batch 40/60 | global_step 1840 | loss_total 0.2433\n",
      "step 1/3 | epoch 31/50 | batch 41/60 | global_step 1841 | loss_total 0.2272\n",
      "step 1/3 | epoch 31/50 | batch 42/60 | global_step 1842 | loss_total 0.2402\n",
      "step 1/3 | epoch 31/50 | batch 43/60 | global_step 1843 | loss_total 0.6419\n",
      "step 1/3 | epoch 31/50 | batch 44/60 | global_step 1844 | loss_total 0.3562\n",
      "step 1/3 | epoch 31/50 | batch 45/60 | global_step 1845 | loss_total 0.2608\n",
      "step 1/3 | epoch 31/50 | batch 46/60 | global_step 1846 | loss_total 0.2173\n",
      "step 1/3 | epoch 31/50 | batch 47/60 | global_step 1847 | loss_total 0.3251\n",
      "step 1/3 | epoch 31/50 | batch 48/60 | global_step 1848 | loss_total 0.7363\n",
      "step 1/3 | epoch 31/50 | batch 49/60 | global_step 1849 | loss_total 0.4425\n",
      "step 1/3 | epoch 31/50 | batch 50/60 | global_step 1850 | loss_total 0.3143\n",
      "step 1/3 | epoch 31/50 | batch 51/60 | global_step 1851 | loss_total 0.2484\n",
      "step 1/3 | epoch 31/50 | batch 52/60 | global_step 1852 | loss_total 0.4688\n",
      "step 1/3 | epoch 31/50 | batch 53/60 | global_step 1853 | loss_total 0.6462\n",
      "step 1/3 | epoch 31/50 | batch 54/60 | global_step 1854 | loss_total 0.2362\n",
      "step 1/3 | epoch 31/50 | batch 55/60 | global_step 1855 | loss_total 0.5190\n",
      "step 1/3 | epoch 31/50 | batch 56/60 | global_step 1856 | loss_total 0.3770\n",
      "step 1/3 | epoch 31/50 | batch 57/60 | global_step 1857 | loss_total 0.2276\n",
      "step 1/3 | epoch 31/50 | batch 58/60 | global_step 1858 | loss_total 0.2665\n",
      "step 1/3 | epoch 31/50 | batch 59/60 | global_step 1859 | loss_total 0.2324\n",
      "step 1/3 | epoch 31/50 | batch 60/60 | global_step 1860 | loss_total 0.4535\n",
      "[epoch done] step 1/3 epoch 31/50 | train_total=0.3560 val_total=0.2316\n",
      "step 1/3 | epoch 32/50 | batch 1/60 | global_step 1861 | loss_total 0.6114\n",
      "step 1/3 | epoch 32/50 | batch 2/60 | global_step 1862 | loss_total 0.2211\n",
      "step 1/3 | epoch 32/50 | batch 3/60 | global_step 1863 | loss_total 1.2123\n",
      "step 1/3 | epoch 32/50 | batch 4/60 | global_step 1864 | loss_total 0.2208\n",
      "step 1/3 | epoch 32/50 | batch 5/60 | global_step 1865 | loss_total 0.9449\n",
      "step 1/3 | epoch 32/50 | batch 6/60 | global_step 1866 | loss_total 0.2211\n",
      "step 1/3 | epoch 32/50 | batch 7/60 | global_step 1867 | loss_total 0.6003\n",
      "step 1/3 | epoch 32/50 | batch 8/60 | global_step 1868 | loss_total 1.5139\n",
      "step 1/3 | epoch 32/50 | batch 9/60 | global_step 1869 | loss_total 0.3200\n",
      "step 1/3 | epoch 32/50 | batch 10/60 | global_step 1870 | loss_total 0.2063\n",
      "step 1/3 | epoch 32/50 | batch 11/60 | global_step 1871 | loss_total 0.2140\n",
      "step 1/3 | epoch 32/50 | batch 12/60 | global_step 1872 | loss_total 0.6184\n",
      "step 1/3 | epoch 32/50 | batch 13/60 | global_step 1873 | loss_total 0.2972\n",
      "step 1/3 | epoch 32/50 | batch 14/60 | global_step 1874 | loss_total 0.5975\n",
      "step 1/3 | epoch 32/50 | batch 15/60 | global_step 1875 | loss_total 0.6797\n",
      "step 1/3 | epoch 32/50 | batch 16/60 | global_step 1876 | loss_total 0.2123\n",
      "step 1/3 | epoch 32/50 | batch 17/60 | global_step 1877 | loss_total 0.2284\n",
      "step 1/3 | epoch 32/50 | batch 18/60 | global_step 1878 | loss_total 0.2026\n",
      "step 1/3 | epoch 32/50 | batch 19/60 | global_step 1879 | loss_total 0.4041\n",
      "step 1/3 | epoch 32/50 | batch 20/60 | global_step 1880 | loss_total 0.2351\n",
      "step 1/3 | epoch 32/50 | batch 21/60 | global_step 1881 | loss_total 0.3402\n",
      "step 1/3 | epoch 32/50 | batch 22/60 | global_step 1882 | loss_total 0.2338\n",
      "step 1/3 | epoch 32/50 | batch 23/60 | global_step 1883 | loss_total 0.4144\n",
      "step 1/3 | epoch 32/50 | batch 24/60 | global_step 1884 | loss_total 0.1997\n",
      "step 1/3 | epoch 32/50 | batch 25/60 | global_step 1885 | loss_total 0.2525\n",
      "step 1/3 | epoch 32/50 | batch 26/60 | global_step 1886 | loss_total 0.2041\n",
      "step 1/3 | epoch 32/50 | batch 27/60 | global_step 1887 | loss_total 0.2075\n",
      "step 1/3 | epoch 32/50 | batch 28/60 | global_step 1888 | loss_total 0.4706\n",
      "step 1/3 | epoch 32/50 | batch 29/60 | global_step 1889 | loss_total 0.4732\n",
      "step 1/3 | epoch 32/50 | batch 30/60 | global_step 1890 | loss_total 0.2570\n",
      "step 1/3 | epoch 32/50 | batch 31/60 | global_step 1891 | loss_total 0.2082\n",
      "step 1/3 | epoch 32/50 | batch 32/60 | global_step 1892 | loss_total 0.2017\n",
      "step 1/3 | epoch 32/50 | batch 33/60 | global_step 1893 | loss_total 0.2604\n",
      "step 1/3 | epoch 32/50 | batch 34/60 | global_step 1894 | loss_total 0.2424\n",
      "step 1/3 | epoch 32/50 | batch 35/60 | global_step 1895 | loss_total 0.3154\n",
      "step 1/3 | epoch 32/50 | batch 36/60 | global_step 1896 | loss_total 0.2290\n",
      "step 1/3 | epoch 32/50 | batch 37/60 | global_step 1897 | loss_total 0.2060\n",
      "step 1/3 | epoch 32/50 | batch 38/60 | global_step 1898 | loss_total 0.2322\n",
      "step 1/3 | epoch 32/50 | batch 39/60 | global_step 1899 | loss_total 0.3360\n",
      "step 1/3 | epoch 32/50 | batch 40/60 | global_step 1900 | loss_total 0.2457\n",
      "step 1/3 | epoch 32/50 | batch 41/60 | global_step 1901 | loss_total 0.3196\n",
      "step 1/3 | epoch 32/50 | batch 42/60 | global_step 1902 | loss_total 0.3033\n",
      "step 1/3 | epoch 32/50 | batch 43/60 | global_step 1903 | loss_total 0.2350\n",
      "step 1/3 | epoch 32/50 | batch 44/60 | global_step 1904 | loss_total 0.3048\n",
      "step 1/3 | epoch 32/50 | batch 45/60 | global_step 1905 | loss_total 0.2028\n",
      "step 1/3 | epoch 32/50 | batch 46/60 | global_step 1906 | loss_total 0.3480\n",
      "step 1/3 | epoch 32/50 | batch 47/60 | global_step 1907 | loss_total 0.2273\n",
      "step 1/3 | epoch 32/50 | batch 48/60 | global_step 1908 | loss_total 0.2623\n",
      "step 1/3 | epoch 32/50 | batch 49/60 | global_step 1909 | loss_total 0.5241\n",
      "step 1/3 | epoch 32/50 | batch 50/60 | global_step 1910 | loss_total 0.2505\n",
      "step 1/3 | epoch 32/50 | batch 51/60 | global_step 1911 | loss_total 0.3080\n",
      "step 1/3 | epoch 32/50 | batch 52/60 | global_step 1912 | loss_total 0.4437\n",
      "step 1/3 | epoch 32/50 | batch 53/60 | global_step 1913 | loss_total 0.2565\n",
      "step 1/3 | epoch 32/50 | batch 54/60 | global_step 1914 | loss_total 0.2282\n",
      "step 1/3 | epoch 32/50 | batch 55/60 | global_step 1915 | loss_total 0.3290\n",
      "step 1/3 | epoch 32/50 | batch 56/60 | global_step 1916 | loss_total 0.3170\n",
      "step 1/3 | epoch 32/50 | batch 57/60 | global_step 1917 | loss_total 0.2332\n",
      "step 1/3 | epoch 32/50 | batch 58/60 | global_step 1918 | loss_total 0.2275\n",
      "step 1/3 | epoch 32/50 | batch 59/60 | global_step 1919 | loss_total 0.2201\n",
      "step 1/3 | epoch 32/50 | batch 60/60 | global_step 1920 | loss_total 0.2291\n",
      "[epoch done] step 1/3 epoch 32/50 | train_total=0.3510 val_total=0.2030\n",
      "step 1/3 | epoch 33/50 | batch 1/60 | global_step 1921 | loss_total 0.2205\n",
      "step 1/3 | epoch 33/50 | batch 2/60 | global_step 1922 | loss_total 0.2208\n",
      "step 1/3 | epoch 33/50 | batch 3/60 | global_step 1923 | loss_total 0.2896\n",
      "step 1/3 | epoch 33/50 | batch 4/60 | global_step 1924 | loss_total 0.4329\n",
      "step 1/3 | epoch 33/50 | batch 5/60 | global_step 1925 | loss_total 2.0838\n",
      "step 1/3 | epoch 33/50 | batch 6/60 | global_step 1926 | loss_total 0.2321\n",
      "step 1/3 | epoch 33/50 | batch 7/60 | global_step 1927 | loss_total 0.2196\n",
      "step 1/3 | epoch 33/50 | batch 8/60 | global_step 1928 | loss_total 1.2472\n",
      "step 1/3 | epoch 33/50 | batch 9/60 | global_step 1929 | loss_total 0.2307\n",
      "step 1/3 | epoch 33/50 | batch 10/60 | global_step 1930 | loss_total 1.9657\n",
      "step 1/3 | epoch 33/50 | batch 11/60 | global_step 1931 | loss_total 0.8790\n",
      "step 1/3 | epoch 33/50 | batch 12/60 | global_step 1932 | loss_total 0.2425\n",
      "step 1/3 | epoch 33/50 | batch 13/60 | global_step 1933 | loss_total 0.6629\n",
      "step 1/3 | epoch 33/50 | batch 14/60 | global_step 1934 | loss_total 0.4026\n",
      "step 1/3 | epoch 33/50 | batch 15/60 | global_step 1935 | loss_total 0.2631\n",
      "step 1/3 | epoch 33/50 | batch 16/60 | global_step 1936 | loss_total 0.2275\n",
      "step 1/3 | epoch 33/50 | batch 17/60 | global_step 1937 | loss_total 0.2633\n",
      "step 1/3 | epoch 33/50 | batch 18/60 | global_step 1938 | loss_total 0.2266\n",
      "step 1/3 | epoch 33/50 | batch 19/60 | global_step 1939 | loss_total 0.2999\n",
      "step 1/3 | epoch 33/50 | batch 20/60 | global_step 1940 | loss_total 0.2374\n",
      "step 1/3 | epoch 33/50 | batch 21/60 | global_step 1941 | loss_total 0.2195\n",
      "step 1/3 | epoch 33/50 | batch 22/60 | global_step 1942 | loss_total 0.2199\n",
      "step 1/3 | epoch 33/50 | batch 23/60 | global_step 1943 | loss_total 0.2333\n",
      "step 1/3 | epoch 33/50 | batch 24/60 | global_step 1944 | loss_total 0.2172\n",
      "step 1/3 | epoch 33/50 | batch 25/60 | global_step 1945 | loss_total 0.2210\n",
      "step 1/3 | epoch 33/50 | batch 26/60 | global_step 1946 | loss_total 0.2934\n",
      "step 1/3 | epoch 33/50 | batch 27/60 | global_step 1947 | loss_total 0.8297\n",
      "step 1/3 | epoch 33/50 | batch 28/60 | global_step 1948 | loss_total 0.3971\n",
      "step 1/3 | epoch 33/50 | batch 29/60 | global_step 1949 | loss_total 0.4043\n",
      "step 1/3 | epoch 33/50 | batch 30/60 | global_step 1950 | loss_total 0.6401\n",
      "step 1/3 | epoch 33/50 | batch 31/60 | global_step 1951 | loss_total 0.7342\n",
      "step 1/3 | epoch 33/50 | batch 32/60 | global_step 1952 | loss_total 0.2252\n",
      "step 1/3 | epoch 33/50 | batch 33/60 | global_step 1953 | loss_total 0.2301\n",
      "step 1/3 | epoch 33/50 | batch 34/60 | global_step 1954 | loss_total 0.2404\n",
      "step 1/3 | epoch 33/50 | batch 35/60 | global_step 1955 | loss_total 0.2277\n",
      "step 1/3 | epoch 33/50 | batch 36/60 | global_step 1956 | loss_total 0.2241\n",
      "step 1/3 | epoch 33/50 | batch 37/60 | global_step 1957 | loss_total 0.2290\n",
      "step 1/3 | epoch 33/50 | batch 38/60 | global_step 1958 | loss_total 0.6696\n",
      "step 1/3 | epoch 33/50 | batch 39/60 | global_step 1959 | loss_total 0.2224\n",
      "step 1/3 | epoch 33/50 | batch 40/60 | global_step 1960 | loss_total 0.2202\n",
      "step 1/3 | epoch 33/50 | batch 41/60 | global_step 1961 | loss_total 0.2121\n",
      "step 1/3 | epoch 33/50 | batch 42/60 | global_step 1962 | loss_total 0.2119\n",
      "step 1/3 | epoch 33/50 | batch 43/60 | global_step 1963 | loss_total 0.2335\n",
      "step 1/3 | epoch 33/50 | batch 44/60 | global_step 1964 | loss_total 0.2324\n",
      "step 1/3 | epoch 33/50 | batch 45/60 | global_step 1965 | loss_total 0.6746\n",
      "step 1/3 | epoch 33/50 | batch 46/60 | global_step 1966 | loss_total 0.2074\n",
      "step 1/3 | epoch 33/50 | batch 47/60 | global_step 1967 | loss_total 0.2435\n",
      "step 1/3 | epoch 33/50 | batch 48/60 | global_step 1968 | loss_total 0.2026\n",
      "step 1/3 | epoch 33/50 | batch 49/60 | global_step 1969 | loss_total 0.2032\n",
      "step 1/3 | epoch 33/50 | batch 50/60 | global_step 1970 | loss_total 0.3014\n",
      "step 1/3 | epoch 33/50 | batch 51/60 | global_step 1971 | loss_total 0.2528\n",
      "step 1/3 | epoch 33/50 | batch 52/60 | global_step 1972 | loss_total 0.2444\n",
      "step 1/3 | epoch 33/50 | batch 53/60 | global_step 1973 | loss_total 0.2309\n",
      "step 1/3 | epoch 33/50 | batch 54/60 | global_step 1974 | loss_total 0.2539\n",
      "step 1/3 | epoch 33/50 | batch 55/60 | global_step 1975 | loss_total 0.2008\n",
      "step 1/3 | epoch 33/50 | batch 56/60 | global_step 1976 | loss_total 0.2232\n",
      "step 1/3 | epoch 33/50 | batch 57/60 | global_step 1977 | loss_total 0.2940\n",
      "step 1/3 | epoch 33/50 | batch 58/60 | global_step 1978 | loss_total 0.2504\n",
      "step 1/3 | epoch 33/50 | batch 59/60 | global_step 1979 | loss_total 0.2104\n",
      "step 1/3 | epoch 33/50 | batch 60/60 | global_step 1980 | loss_total 0.7366\n",
      "[epoch done] step 1/3 epoch 33/50 | train_total=0.3894 val_total=0.2623\n",
      "step 1/3 | epoch 34/50 | batch 1/60 | global_step 1981 | loss_total 0.2047\n",
      "step 1/3 | epoch 34/50 | batch 2/60 | global_step 1982 | loss_total 0.2057\n",
      "step 1/3 | epoch 34/50 | batch 3/60 | global_step 1983 | loss_total 0.3414\n",
      "step 1/3 | epoch 34/50 | batch 4/60 | global_step 1984 | loss_total 0.2250\n",
      "step 1/3 | epoch 34/50 | batch 5/60 | global_step 1985 | loss_total 0.2243\n",
      "step 1/3 | epoch 34/50 | batch 6/60 | global_step 1986 | loss_total 0.2427\n",
      "step 1/3 | epoch 34/50 | batch 7/60 | global_step 1987 | loss_total 0.3244\n",
      "step 1/3 | epoch 34/50 | batch 8/60 | global_step 1988 | loss_total 0.2240\n",
      "step 1/3 | epoch 34/50 | batch 9/60 | global_step 1989 | loss_total 0.2534\n",
      "step 1/3 | epoch 34/50 | batch 10/60 | global_step 1990 | loss_total 0.2247\n",
      "step 1/3 | epoch 34/50 | batch 11/60 | global_step 1991 | loss_total 0.2493\n",
      "step 1/3 | epoch 34/50 | batch 12/60 | global_step 1992 | loss_total 0.3108\n",
      "step 1/3 | epoch 34/50 | batch 13/60 | global_step 1993 | loss_total 0.2134\n",
      "step 1/3 | epoch 34/50 | batch 14/60 | global_step 1994 | loss_total 0.2251\n",
      "step 1/3 | epoch 34/50 | batch 15/60 | global_step 1995 | loss_total 0.2143\n",
      "step 1/3 | epoch 34/50 | batch 16/60 | global_step 1996 | loss_total 0.3208\n",
      "step 1/3 | epoch 34/50 | batch 17/60 | global_step 1997 | loss_total 0.6122\n",
      "step 1/3 | epoch 34/50 | batch 18/60 | global_step 1998 | loss_total 0.2590\n",
      "step 1/3 | epoch 34/50 | batch 19/60 | global_step 1999 | loss_total 0.2907\n",
      "step 1/3 | epoch 34/50 | batch 20/60 | global_step 2000 | loss_total 0.2250\n",
      "step 1/3 | epoch 34/50 | batch 21/60 | global_step 2001 | loss_total 0.2259\n",
      "step 1/3 | epoch 34/50 | batch 22/60 | global_step 2002 | loss_total 0.2184\n",
      "step 1/3 | epoch 34/50 | batch 23/60 | global_step 2003 | loss_total 0.2188\n",
      "step 1/3 | epoch 34/50 | batch 24/60 | global_step 2004 | loss_total 0.2229\n",
      "step 1/3 | epoch 34/50 | batch 25/60 | global_step 2005 | loss_total 0.7570\n",
      "step 1/3 | epoch 34/50 | batch 26/60 | global_step 2006 | loss_total 0.6070\n",
      "step 1/3 | epoch 34/50 | batch 27/60 | global_step 2007 | loss_total 0.5240\n",
      "step 1/3 | epoch 34/50 | batch 28/60 | global_step 2008 | loss_total 0.2296\n",
      "step 1/3 | epoch 34/50 | batch 29/60 | global_step 2009 | loss_total 0.2170\n",
      "step 1/3 | epoch 34/50 | batch 30/60 | global_step 2010 | loss_total 0.3178\n",
      "step 1/3 | epoch 34/50 | batch 31/60 | global_step 2011 | loss_total 0.2249\n",
      "step 1/3 | epoch 34/50 | batch 32/60 | global_step 2012 | loss_total 0.5125\n",
      "step 1/3 | epoch 34/50 | batch 33/60 | global_step 2013 | loss_total 0.3828\n",
      "step 1/3 | epoch 34/50 | batch 34/60 | global_step 2014 | loss_total 0.2628\n",
      "step 1/3 | epoch 34/50 | batch 35/60 | global_step 2015 | loss_total 0.4811\n",
      "step 1/3 | epoch 34/50 | batch 36/60 | global_step 2016 | loss_total 0.3730\n",
      "step 1/3 | epoch 34/50 | batch 37/60 | global_step 2017 | loss_total 0.2740\n",
      "step 1/3 | epoch 34/50 | batch 38/60 | global_step 2018 | loss_total 0.3799\n",
      "step 1/3 | epoch 34/50 | batch 39/60 | global_step 2019 | loss_total 0.2349\n",
      "step 1/3 | epoch 34/50 | batch 40/60 | global_step 2020 | loss_total 0.3507\n",
      "step 1/3 | epoch 34/50 | batch 41/60 | global_step 2021 | loss_total 0.8241\n",
      "step 1/3 | epoch 34/50 | batch 42/60 | global_step 2022 | loss_total 0.2375\n",
      "step 1/3 | epoch 34/50 | batch 43/60 | global_step 2023 | loss_total 0.5705\n",
      "step 1/3 | epoch 34/50 | batch 44/60 | global_step 2024 | loss_total 0.8290\n",
      "step 1/3 | epoch 34/50 | batch 45/60 | global_step 2025 | loss_total 0.2165\n",
      "step 1/3 | epoch 34/50 | batch 46/60 | global_step 2026 | loss_total 0.7065\n",
      "step 1/3 | epoch 34/50 | batch 47/60 | global_step 2027 | loss_total 0.2164\n",
      "step 1/3 | epoch 34/50 | batch 48/60 | global_step 2028 | loss_total 0.2298\n",
      "step 1/3 | epoch 34/50 | batch 49/60 | global_step 2029 | loss_total 0.2149\n",
      "step 1/3 | epoch 34/50 | batch 50/60 | global_step 2030 | loss_total 0.2169\n",
      "step 1/3 | epoch 34/50 | batch 51/60 | global_step 2031 | loss_total 0.2216\n",
      "step 1/3 | epoch 34/50 | batch 52/60 | global_step 2032 | loss_total 0.7805\n",
      "step 1/3 | epoch 34/50 | batch 53/60 | global_step 2033 | loss_total 1.3266\n",
      "step 1/3 | epoch 34/50 | batch 54/60 | global_step 2034 | loss_total 0.2315\n",
      "step 1/3 | epoch 34/50 | batch 55/60 | global_step 2035 | loss_total 0.2328\n",
      "step 1/3 | epoch 34/50 | batch 56/60 | global_step 2036 | loss_total 0.3126\n",
      "step 1/3 | epoch 34/50 | batch 57/60 | global_step 2037 | loss_total 0.2329\n",
      "step 1/3 | epoch 34/50 | batch 58/60 | global_step 2038 | loss_total 0.2147\n",
      "step 1/3 | epoch 34/50 | batch 59/60 | global_step 2039 | loss_total 0.2213\n",
      "step 1/3 | epoch 34/50 | batch 60/60 | global_step 2040 | loss_total 0.2314\n",
      "[epoch done] step 1/3 epoch 34/50 | train_total=0.3446 val_total=0.2038\n",
      "step 1/3 | epoch 35/50 | batch 1/60 | global_step 2041 | loss_total 0.2177\n",
      "step 1/3 | epoch 35/50 | batch 2/60 | global_step 2042 | loss_total 0.2248\n",
      "step 1/3 | epoch 35/50 | batch 3/60 | global_step 2043 | loss_total 0.2206\n",
      "step 1/3 | epoch 35/50 | batch 4/60 | global_step 2044 | loss_total 0.2090\n",
      "step 1/3 | epoch 35/50 | batch 5/60 | global_step 2045 | loss_total 0.2067\n",
      "step 1/3 | epoch 35/50 | batch 6/60 | global_step 2046 | loss_total 0.2106\n",
      "step 1/3 | epoch 35/50 | batch 7/60 | global_step 2047 | loss_total 0.2221\n",
      "step 1/3 | epoch 35/50 | batch 8/60 | global_step 2048 | loss_total 0.7263\n",
      "step 1/3 | epoch 35/50 | batch 9/60 | global_step 2049 | loss_total 0.2426\n",
      "step 1/3 | epoch 35/50 | batch 10/60 | global_step 2050 | loss_total 0.4351\n",
      "step 1/3 | epoch 35/50 | batch 11/60 | global_step 2051 | loss_total 0.2196\n",
      "step 1/3 | epoch 35/50 | batch 12/60 | global_step 2052 | loss_total 0.2669\n",
      "step 1/3 | epoch 35/50 | batch 13/60 | global_step 2053 | loss_total 0.2208\n",
      "step 1/3 | epoch 35/50 | batch 14/60 | global_step 2054 | loss_total 0.1773\n",
      "step 1/3 | epoch 35/50 | batch 15/60 | global_step 2055 | loss_total 0.2234\n",
      "step 1/3 | epoch 35/50 | batch 16/60 | global_step 2056 | loss_total 0.2212\n",
      "step 1/3 | epoch 35/50 | batch 17/60 | global_step 2057 | loss_total 0.2009\n",
      "step 1/3 | epoch 35/50 | batch 18/60 | global_step 2058 | loss_total 0.2489\n",
      "step 1/3 | epoch 35/50 | batch 19/60 | global_step 2059 | loss_total 0.7430\n",
      "step 1/3 | epoch 35/50 | batch 20/60 | global_step 2060 | loss_total 0.5342\n",
      "step 1/3 | epoch 35/50 | batch 21/60 | global_step 2061 | loss_total 0.1998\n",
      "step 1/3 | epoch 35/50 | batch 22/60 | global_step 2062 | loss_total 0.2212\n",
      "step 1/3 | epoch 35/50 | batch 23/60 | global_step 2063 | loss_total 0.2026\n",
      "step 1/3 | epoch 35/50 | batch 24/60 | global_step 2064 | loss_total 0.2869\n",
      "step 1/3 | epoch 35/50 | batch 25/60 | global_step 2065 | loss_total 0.2014\n",
      "step 1/3 | epoch 35/50 | batch 26/60 | global_step 2066 | loss_total 0.2586\n",
      "step 1/3 | epoch 35/50 | batch 27/60 | global_step 2067 | loss_total 0.2019\n",
      "step 1/3 | epoch 35/50 | batch 28/60 | global_step 2068 | loss_total 0.3255\n",
      "step 1/3 | epoch 35/50 | batch 29/60 | global_step 2069 | loss_total 0.1972\n",
      "step 1/3 | epoch 35/50 | batch 30/60 | global_step 2070 | loss_total 0.2759\n",
      "step 1/3 | epoch 35/50 | batch 31/60 | global_step 2071 | loss_total 0.2475\n",
      "step 1/3 | epoch 35/50 | batch 32/60 | global_step 2072 | loss_total 0.2217\n",
      "step 1/3 | epoch 35/50 | batch 33/60 | global_step 2073 | loss_total 0.2212\n",
      "step 1/3 | epoch 35/50 | batch 34/60 | global_step 2074 | loss_total 0.2228\n",
      "step 1/3 | epoch 35/50 | batch 35/60 | global_step 2075 | loss_total 0.2198\n",
      "step 1/3 | epoch 35/50 | batch 36/60 | global_step 2076 | loss_total 0.6137\n",
      "step 1/3 | epoch 35/50 | batch 37/60 | global_step 2077 | loss_total 0.2328\n",
      "step 1/3 | epoch 35/50 | batch 38/60 | global_step 2078 | loss_total 0.2197\n",
      "step 1/3 | epoch 35/50 | batch 39/60 | global_step 2079 | loss_total 0.2213\n",
      "step 1/3 | epoch 35/50 | batch 40/60 | global_step 2080 | loss_total 0.2169\n",
      "step 1/3 | epoch 35/50 | batch 41/60 | global_step 2081 | loss_total 0.2281\n",
      "step 1/3 | epoch 35/50 | batch 42/60 | global_step 2082 | loss_total 0.6484\n",
      "step 1/3 | epoch 35/50 | batch 43/60 | global_step 2083 | loss_total 0.2171\n",
      "step 1/3 | epoch 35/50 | batch 44/60 | global_step 2084 | loss_total 0.2165\n",
      "step 1/3 | epoch 35/50 | batch 45/60 | global_step 2085 | loss_total 0.7440\n",
      "step 1/3 | epoch 35/50 | batch 46/60 | global_step 2086 | loss_total 0.2170\n",
      "step 1/3 | epoch 35/50 | batch 47/60 | global_step 2087 | loss_total 0.2169\n",
      "step 1/3 | epoch 35/50 | batch 48/60 | global_step 2088 | loss_total 0.2164\n",
      "step 1/3 | epoch 35/50 | batch 49/60 | global_step 2089 | loss_total 0.2135\n",
      "step 1/3 | epoch 35/50 | batch 50/60 | global_step 2090 | loss_total 0.2844\n",
      "step 1/3 | epoch 35/50 | batch 51/60 | global_step 2091 | loss_total 0.2401\n",
      "step 1/3 | epoch 35/50 | batch 52/60 | global_step 2092 | loss_total 0.2073\n",
      "step 1/3 | epoch 35/50 | batch 53/60 | global_step 2093 | loss_total 0.2762\n",
      "step 1/3 | epoch 35/50 | batch 54/60 | global_step 2094 | loss_total 0.2179\n",
      "step 1/3 | epoch 35/50 | batch 55/60 | global_step 2095 | loss_total 0.2157\n",
      "step 1/3 | epoch 35/50 | batch 56/60 | global_step 2096 | loss_total 0.2345\n",
      "step 1/3 | epoch 35/50 | batch 57/60 | global_step 2097 | loss_total 0.2341\n",
      "step 1/3 | epoch 35/50 | batch 58/60 | global_step 2098 | loss_total 0.2970\n",
      "step 1/3 | epoch 35/50 | batch 59/60 | global_step 2099 | loss_total 0.5728\n",
      "step 1/3 | epoch 35/50 | batch 60/60 | global_step 2100 | loss_total 0.2496\n",
      "[epoch done] step 1/3 epoch 35/50 | train_total=0.2821 val_total=0.2403\n",
      "step 1/3 | epoch 36/50 | batch 1/60 | global_step 2101 | loss_total 0.2217\n",
      "step 1/3 | epoch 36/50 | batch 2/60 | global_step 2102 | loss_total 0.2031\n",
      "step 1/3 | epoch 36/50 | batch 3/60 | global_step 2103 | loss_total 0.2104\n",
      "step 1/3 | epoch 36/50 | batch 4/60 | global_step 2104 | loss_total 0.2159\n",
      "step 1/3 | epoch 36/50 | batch 5/60 | global_step 2105 | loss_total 0.2321\n",
      "step 1/3 | epoch 36/50 | batch 6/60 | global_step 2106 | loss_total 0.6933\n",
      "step 1/3 | epoch 36/50 | batch 7/60 | global_step 2107 | loss_total 0.2055\n",
      "step 1/3 | epoch 36/50 | batch 8/60 | global_step 2108 | loss_total 0.2376\n",
      "step 1/3 | epoch 36/50 | batch 9/60 | global_step 2109 | loss_total 0.2618\n",
      "step 1/3 | epoch 36/50 | batch 10/60 | global_step 2110 | loss_total 0.2186\n",
      "step 1/3 | epoch 36/50 | batch 11/60 | global_step 2111 | loss_total 0.2629\n",
      "step 1/3 | epoch 36/50 | batch 12/60 | global_step 2112 | loss_total 0.2645\n",
      "step 1/3 | epoch 36/50 | batch 13/60 | global_step 2113 | loss_total 0.2196\n",
      "step 1/3 | epoch 36/50 | batch 14/60 | global_step 2114 | loss_total 0.2355\n",
      "step 1/3 | epoch 36/50 | batch 15/60 | global_step 2115 | loss_total 0.2077\n",
      "step 1/3 | epoch 36/50 | batch 16/60 | global_step 2116 | loss_total 0.2095\n",
      "step 1/3 | epoch 36/50 | batch 17/60 | global_step 2117 | loss_total 0.2254\n",
      "step 1/3 | epoch 36/50 | batch 18/60 | global_step 2118 | loss_total 1.1676\n",
      "step 1/3 | epoch 36/50 | batch 19/60 | global_step 2119 | loss_total 0.2064\n",
      "step 1/3 | epoch 36/50 | batch 20/60 | global_step 2120 | loss_total 0.2312\n",
      "step 1/3 | epoch 36/50 | batch 21/60 | global_step 2121 | loss_total 1.7088\n",
      "step 1/3 | epoch 36/50 | batch 22/60 | global_step 2122 | loss_total 0.2205\n",
      "step 1/3 | epoch 36/50 | batch 23/60 | global_step 2123 | loss_total 0.2187\n",
      "step 1/3 | epoch 36/50 | batch 24/60 | global_step 2124 | loss_total 0.3075\n",
      "step 1/3 | epoch 36/50 | batch 25/60 | global_step 2125 | loss_total 0.2381\n",
      "step 1/3 | epoch 36/50 | batch 26/60 | global_step 2126 | loss_total 0.2735\n",
      "step 1/3 | epoch 36/50 | batch 27/60 | global_step 2127 | loss_total 0.2039\n",
      "step 1/3 | epoch 36/50 | batch 28/60 | global_step 2128 | loss_total 0.3507\n",
      "step 1/3 | epoch 36/50 | batch 29/60 | global_step 2129 | loss_total 1.7362\n",
      "step 1/3 | epoch 36/50 | batch 30/60 | global_step 2130 | loss_total 0.2178\n",
      "step 1/3 | epoch 36/50 | batch 31/60 | global_step 2131 | loss_total 0.2798\n",
      "step 1/3 | epoch 36/50 | batch 32/60 | global_step 2132 | loss_total 0.2183\n",
      "step 1/3 | epoch 36/50 | batch 33/60 | global_step 2133 | loss_total 0.2186\n",
      "step 1/3 | epoch 36/50 | batch 34/60 | global_step 2134 | loss_total 0.6168\n",
      "step 1/3 | epoch 36/50 | batch 35/60 | global_step 2135 | loss_total 0.2325\n",
      "step 1/3 | epoch 36/50 | batch 36/60 | global_step 2136 | loss_total 0.2169\n",
      "step 1/3 | epoch 36/50 | batch 37/60 | global_step 2137 | loss_total 0.2355\n",
      "step 1/3 | epoch 36/50 | batch 38/60 | global_step 2138 | loss_total 0.7455\n",
      "step 1/3 | epoch 36/50 | batch 39/60 | global_step 2139 | loss_total 0.2160\n",
      "step 1/3 | epoch 36/50 | batch 40/60 | global_step 2140 | loss_total 0.2178\n",
      "step 1/3 | epoch 36/50 | batch 41/60 | global_step 2141 | loss_total 0.2282\n",
      "step 1/3 | epoch 36/50 | batch 42/60 | global_step 2142 | loss_total 0.2199\n",
      "step 1/3 | epoch 36/50 | batch 43/60 | global_step 2143 | loss_total 0.2103\n",
      "step 1/3 | epoch 36/50 | batch 44/60 | global_step 2144 | loss_total 0.2120\n",
      "step 1/3 | epoch 36/50 | batch 45/60 | global_step 2145 | loss_total 0.2662\n",
      "step 1/3 | epoch 36/50 | batch 46/60 | global_step 2146 | loss_total 0.2183\n",
      "step 1/3 | epoch 36/50 | batch 47/60 | global_step 2147 | loss_total 0.2227\n",
      "step 1/3 | epoch 36/50 | batch 48/60 | global_step 2148 | loss_total 0.2195\n",
      "step 1/3 | epoch 36/50 | batch 49/60 | global_step 2149 | loss_total 0.4541\n",
      "step 1/3 | epoch 36/50 | batch 50/60 | global_step 2150 | loss_total 1.5951\n",
      "step 1/3 | epoch 36/50 | batch 51/60 | global_step 2151 | loss_total 0.2220\n",
      "step 1/3 | epoch 36/50 | batch 52/60 | global_step 2152 | loss_total 0.2103\n",
      "step 1/3 | epoch 36/50 | batch 53/60 | global_step 2153 | loss_total 0.2190\n",
      "step 1/3 | epoch 36/50 | batch 54/60 | global_step 2154 | loss_total 0.2176\n",
      "step 1/3 | epoch 36/50 | batch 55/60 | global_step 2155 | loss_total 0.2252\n",
      "step 1/3 | epoch 36/50 | batch 56/60 | global_step 2156 | loss_total 0.2271\n",
      "step 1/3 | epoch 36/50 | batch 57/60 | global_step 2157 | loss_total 0.2495\n",
      "step 1/3 | epoch 36/50 | batch 58/60 | global_step 2158 | loss_total 0.2316\n",
      "step 1/3 | epoch 36/50 | batch 59/60 | global_step 2159 | loss_total 0.2975\n",
      "step 1/3 | epoch 36/50 | batch 60/60 | global_step 2160 | loss_total 0.1984\n",
      "[epoch done] step 1/3 epoch 36/50 | train_total=0.3458 val_total=0.2500\n",
      "step 1/3 | epoch 37/50 | batch 1/60 | global_step 2161 | loss_total 0.9469\n",
      "step 1/3 | epoch 37/50 | batch 2/60 | global_step 2162 | loss_total 0.1970\n",
      "step 1/3 | epoch 37/50 | batch 3/60 | global_step 2163 | loss_total 0.1966\n",
      "step 1/3 | epoch 37/50 | batch 4/60 | global_step 2164 | loss_total 0.2194\n",
      "step 1/3 | epoch 37/50 | batch 5/60 | global_step 2165 | loss_total 0.2175\n",
      "step 1/3 | epoch 37/50 | batch 6/60 | global_step 2166 | loss_total 0.2179\n",
      "step 1/3 | epoch 37/50 | batch 7/60 | global_step 2167 | loss_total 0.6435\n",
      "step 1/3 | epoch 37/50 | batch 8/60 | global_step 2168 | loss_total 0.1915\n",
      "step 1/3 | epoch 37/50 | batch 9/60 | global_step 2169 | loss_total 0.2591\n",
      "step 1/3 | epoch 37/50 | batch 10/60 | global_step 2170 | loss_total 0.2224\n",
      "step 1/3 | epoch 37/50 | batch 11/60 | global_step 2171 | loss_total 0.2460\n",
      "step 1/3 | epoch 37/50 | batch 12/60 | global_step 2172 | loss_total 0.2449\n",
      "step 1/3 | epoch 37/50 | batch 13/60 | global_step 2173 | loss_total 0.2192\n",
      "step 1/3 | epoch 37/50 | batch 14/60 | global_step 2174 | loss_total 0.2055\n",
      "step 1/3 | epoch 37/50 | batch 15/60 | global_step 2175 | loss_total 0.2651\n",
      "step 1/3 | epoch 37/50 | batch 16/60 | global_step 2176 | loss_total 0.9010\n",
      "step 1/3 | epoch 37/50 | batch 17/60 | global_step 2177 | loss_total 0.2181\n",
      "step 1/3 | epoch 37/50 | batch 18/60 | global_step 2178 | loss_total 0.2174\n",
      "step 1/3 | epoch 37/50 | batch 19/60 | global_step 2179 | loss_total 0.1994\n",
      "step 1/3 | epoch 37/50 | batch 20/60 | global_step 2180 | loss_total 0.8420\n",
      "step 1/3 | epoch 37/50 | batch 21/60 | global_step 2181 | loss_total 0.2235\n",
      "step 1/3 | epoch 37/50 | batch 22/60 | global_step 2182 | loss_total 0.2153\n",
      "step 1/3 | epoch 37/50 | batch 23/60 | global_step 2183 | loss_total 0.2274\n",
      "step 1/3 | epoch 37/50 | batch 24/60 | global_step 2184 | loss_total 0.2145\n",
      "step 1/3 | epoch 37/50 | batch 25/60 | global_step 2185 | loss_total 0.4532\n",
      "step 1/3 | epoch 37/50 | batch 26/60 | global_step 2186 | loss_total 0.2245\n",
      "step 1/3 | epoch 37/50 | batch 27/60 | global_step 2187 | loss_total 1.5221\n",
      "step 1/3 | epoch 37/50 | batch 28/60 | global_step 2188 | loss_total 0.2299\n",
      "step 1/3 | epoch 37/50 | batch 29/60 | global_step 2189 | loss_total 1.4058\n",
      "step 1/3 | epoch 37/50 | batch 30/60 | global_step 2190 | loss_total 0.9371\n",
      "step 1/3 | epoch 37/50 | batch 31/60 | global_step 2191 | loss_total 0.2143\n",
      "step 1/3 | epoch 37/50 | batch 32/60 | global_step 2192 | loss_total 0.6270\n",
      "step 1/3 | epoch 37/50 | batch 33/60 | global_step 2193 | loss_total 0.2244\n",
      "step 1/3 | epoch 37/50 | batch 34/60 | global_step 2194 | loss_total 0.3314\n",
      "step 1/3 | epoch 37/50 | batch 35/60 | global_step 2195 | loss_total 0.2153\n",
      "step 1/3 | epoch 37/50 | batch 36/60 | global_step 2196 | loss_total 0.2247\n",
      "step 1/3 | epoch 37/50 | batch 37/60 | global_step 2197 | loss_total 0.2163\n",
      "step 1/3 | epoch 37/50 | batch 38/60 | global_step 2198 | loss_total 0.2270\n",
      "step 1/3 | epoch 37/50 | batch 39/60 | global_step 2199 | loss_total 0.2177\n",
      "step 1/3 | epoch 37/50 | batch 40/60 | global_step 2200 | loss_total 0.3311\n",
      "step 1/3 | epoch 37/50 | batch 41/60 | global_step 2201 | loss_total 0.2154\n",
      "step 1/3 | epoch 37/50 | batch 42/60 | global_step 2202 | loss_total 0.2193\n",
      "step 1/3 | epoch 37/50 | batch 43/60 | global_step 2203 | loss_total 0.2262\n",
      "step 1/3 | epoch 37/50 | batch 44/60 | global_step 2204 | loss_total 0.3810\n",
      "step 1/3 | epoch 37/50 | batch 45/60 | global_step 2205 | loss_total 0.2159\n",
      "step 1/3 | epoch 37/50 | batch 46/60 | global_step 2206 | loss_total 0.2179\n",
      "step 1/3 | epoch 37/50 | batch 47/60 | global_step 2207 | loss_total 0.2177\n",
      "step 1/3 | epoch 37/50 | batch 48/60 | global_step 2208 | loss_total 0.2120\n",
      "step 1/3 | epoch 37/50 | batch 49/60 | global_step 2209 | loss_total 0.2574\n",
      "step 1/3 | epoch 37/50 | batch 50/60 | global_step 2210 | loss_total 0.2188\n",
      "step 1/3 | epoch 37/50 | batch 51/60 | global_step 2211 | loss_total 1.0035\n",
      "step 1/3 | epoch 37/50 | batch 52/60 | global_step 2212 | loss_total 0.2266\n",
      "step 1/3 | epoch 37/50 | batch 53/60 | global_step 2213 | loss_total 0.7385\n",
      "step 1/3 | epoch 37/50 | batch 54/60 | global_step 2214 | loss_total 0.2307\n",
      "step 1/3 | epoch 37/50 | batch 55/60 | global_step 2215 | loss_total 0.2290\n",
      "step 1/3 | epoch 37/50 | batch 56/60 | global_step 2216 | loss_total 1.0760\n",
      "step 1/3 | epoch 37/50 | batch 57/60 | global_step 2217 | loss_total 0.5252\n",
      "step 1/3 | epoch 37/50 | batch 58/60 | global_step 2218 | loss_total 0.7127\n",
      "step 1/3 | epoch 37/50 | batch 59/60 | global_step 2219 | loss_total 0.2165\n",
      "step 1/3 | epoch 37/50 | batch 60/60 | global_step 2220 | loss_total 0.2214\n",
      "[epoch done] step 1/3 epoch 37/50 | train_total=0.3819 val_total=0.2340\n",
      "step 1/3 | epoch 38/50 | batch 1/60 | global_step 2221 | loss_total 0.2169\n",
      "step 1/3 | epoch 38/50 | batch 2/60 | global_step 2222 | loss_total 0.2639\n",
      "step 1/3 | epoch 38/50 | batch 3/60 | global_step 2223 | loss_total 0.2092\n",
      "step 1/3 | epoch 38/50 | batch 4/60 | global_step 2224 | loss_total 0.2699\n",
      "step 1/3 | epoch 38/50 | batch 5/60 | global_step 2225 | loss_total 0.2341\n",
      "step 1/3 | epoch 38/50 | batch 6/60 | global_step 2226 | loss_total 0.6001\n",
      "step 1/3 | epoch 38/50 | batch 7/60 | global_step 2227 | loss_total 0.2161\n",
      "step 1/3 | epoch 38/50 | batch 8/60 | global_step 2228 | loss_total 0.2149\n",
      "step 1/3 | epoch 38/50 | batch 9/60 | global_step 2229 | loss_total 0.2154\n",
      "step 1/3 | epoch 38/50 | batch 10/60 | global_step 2230 | loss_total 0.7075\n",
      "step 1/3 | epoch 38/50 | batch 11/60 | global_step 2231 | loss_total 0.2218\n",
      "step 1/3 | epoch 38/50 | batch 12/60 | global_step 2232 | loss_total 0.3546\n",
      "step 1/3 | epoch 38/50 | batch 13/60 | global_step 2233 | loss_total 0.3232\n",
      "step 1/3 | epoch 38/50 | batch 14/60 | global_step 2234 | loss_total 0.2299\n",
      "step 1/3 | epoch 38/50 | batch 15/60 | global_step 2235 | loss_total 0.4138\n",
      "step 1/3 | epoch 38/50 | batch 16/60 | global_step 2236 | loss_total 0.2745\n",
      "step 1/3 | epoch 38/50 | batch 17/60 | global_step 2237 | loss_total 1.1977\n",
      "step 1/3 | epoch 38/50 | batch 18/60 | global_step 2238 | loss_total 1.0513\n",
      "step 1/3 | epoch 38/50 | batch 19/60 | global_step 2239 | loss_total 0.2634\n",
      "step 1/3 | epoch 38/50 | batch 20/60 | global_step 2240 | loss_total 0.2178\n",
      "step 1/3 | epoch 38/50 | batch 21/60 | global_step 2241 | loss_total 0.2271\n",
      "step 1/3 | epoch 38/50 | batch 22/60 | global_step 2242 | loss_total 0.2188\n",
      "step 1/3 | epoch 38/50 | batch 23/60 | global_step 2243 | loss_total 0.2095\n",
      "step 1/3 | epoch 38/50 | batch 24/60 | global_step 2244 | loss_total 0.9633\n",
      "step 1/3 | epoch 38/50 | batch 25/60 | global_step 2245 | loss_total 0.2028\n",
      "step 1/3 | epoch 38/50 | batch 26/60 | global_step 2246 | loss_total 0.5214\n",
      "step 1/3 | epoch 38/50 | batch 27/60 | global_step 2247 | loss_total 0.8510\n",
      "step 1/3 | epoch 38/50 | batch 28/60 | global_step 2248 | loss_total 0.7689\n",
      "step 1/3 | epoch 38/50 | batch 29/60 | global_step 2249 | loss_total 0.4675\n",
      "step 1/3 | epoch 38/50 | batch 30/60 | global_step 2250 | loss_total 0.3837\n",
      "step 1/3 | epoch 38/50 | batch 31/60 | global_step 2251 | loss_total 0.2459\n",
      "step 1/3 | epoch 38/50 | batch 32/60 | global_step 2252 | loss_total 0.2566\n",
      "step 1/3 | epoch 38/50 | batch 33/60 | global_step 2253 | loss_total 0.2500\n",
      "step 1/3 | epoch 38/50 | batch 34/60 | global_step 2254 | loss_total 0.3019\n",
      "step 1/3 | epoch 38/50 | batch 35/60 | global_step 2255 | loss_total 1.2110\n",
      "step 1/3 | epoch 38/50 | batch 36/60 | global_step 2256 | loss_total 0.2173\n",
      "step 1/3 | epoch 38/50 | batch 37/60 | global_step 2257 | loss_total 0.3085\n",
      "step 1/3 | epoch 38/50 | batch 38/60 | global_step 2258 | loss_total 0.2762\n",
      "step 1/3 | epoch 38/50 | batch 39/60 | global_step 2259 | loss_total 0.8246\n",
      "step 1/3 | epoch 38/50 | batch 40/60 | global_step 2260 | loss_total 0.2277\n",
      "step 1/3 | epoch 38/50 | batch 41/60 | global_step 2261 | loss_total 0.2298\n",
      "step 1/3 | epoch 38/50 | batch 42/60 | global_step 2262 | loss_total 0.2275\n",
      "step 1/3 | epoch 38/50 | batch 43/60 | global_step 2263 | loss_total 0.2215\n",
      "step 1/3 | epoch 38/50 | batch 44/60 | global_step 2264 | loss_total 0.2606\n",
      "step 1/3 | epoch 38/50 | batch 45/60 | global_step 2265 | loss_total 0.5254\n",
      "step 1/3 | epoch 38/50 | batch 46/60 | global_step 2266 | loss_total 0.2987\n",
      "step 1/3 | epoch 38/50 | batch 47/60 | global_step 2267 | loss_total 0.2331\n",
      "step 1/3 | epoch 38/50 | batch 48/60 | global_step 2268 | loss_total 0.2370\n",
      "step 1/3 | epoch 38/50 | batch 49/60 | global_step 2269 | loss_total 0.6776\n",
      "step 1/3 | epoch 38/50 | batch 50/60 | global_step 2270 | loss_total 0.5550\n",
      "step 1/3 | epoch 38/50 | batch 51/60 | global_step 2271 | loss_total 0.2197\n",
      "step 1/3 | epoch 38/50 | batch 52/60 | global_step 2272 | loss_total 0.7652\n",
      "step 1/3 | epoch 38/50 | batch 53/60 | global_step 2273 | loss_total 0.2075\n",
      "step 1/3 | epoch 38/50 | batch 54/60 | global_step 2274 | loss_total 0.1897\n",
      "step 1/3 | epoch 38/50 | batch 55/60 | global_step 2275 | loss_total 0.2127\n",
      "step 1/3 | epoch 38/50 | batch 56/60 | global_step 2276 | loss_total 0.2209\n",
      "step 1/3 | epoch 38/50 | batch 57/60 | global_step 2277 | loss_total 0.2419\n",
      "step 1/3 | epoch 38/50 | batch 58/60 | global_step 2278 | loss_total 0.2850\n",
      "step 1/3 | epoch 38/50 | batch 59/60 | global_step 2279 | loss_total 0.1976\n",
      "step 1/3 | epoch 38/50 | batch 60/60 | global_step 2280 | loss_total 0.2253\n",
      "[epoch done] step 1/3 epoch 38/50 | train_total=0.3810 val_total=0.1673\n",
      "step 1/3 | epoch 39/50 | batch 1/60 | global_step 2281 | loss_total 0.2253\n",
      "step 1/3 | epoch 39/50 | batch 2/60 | global_step 2282 | loss_total 1.6093\n",
      "step 1/3 | epoch 39/50 | batch 3/60 | global_step 2283 | loss_total 0.2500\n",
      "step 1/3 | epoch 39/50 | batch 4/60 | global_step 2284 | loss_total 0.2274\n",
      "step 1/3 | epoch 39/50 | batch 5/60 | global_step 2285 | loss_total 0.4370\n",
      "step 1/3 | epoch 39/50 | batch 6/60 | global_step 2286 | loss_total 0.2272\n",
      "step 1/3 | epoch 39/50 | batch 7/60 | global_step 2287 | loss_total 0.1988\n",
      "step 1/3 | epoch 39/50 | batch 8/60 | global_step 2288 | loss_total 0.4137\n",
      "step 1/3 | epoch 39/50 | batch 9/60 | global_step 2289 | loss_total 0.8954\n",
      "step 1/3 | epoch 39/50 | batch 10/60 | global_step 2290 | loss_total 0.2310\n",
      "step 1/3 | epoch 39/50 | batch 11/60 | global_step 2291 | loss_total 0.5356\n",
      "step 1/3 | epoch 39/50 | batch 12/60 | global_step 2292 | loss_total 0.2451\n",
      "step 1/3 | epoch 39/50 | batch 13/60 | global_step 2293 | loss_total 0.2373\n",
      "step 1/3 | epoch 39/50 | batch 14/60 | global_step 2294 | loss_total 0.2290\n",
      "step 1/3 | epoch 39/50 | batch 15/60 | global_step 2295 | loss_total 0.2415\n",
      "step 1/3 | epoch 39/50 | batch 16/60 | global_step 2296 | loss_total 0.5512\n",
      "step 1/3 | epoch 39/50 | batch 17/60 | global_step 2297 | loss_total 0.2105\n",
      "step 1/3 | epoch 39/50 | batch 18/60 | global_step 2298 | loss_total 0.2066\n",
      "step 1/3 | epoch 39/50 | batch 19/60 | global_step 2299 | loss_total 0.7583\n",
      "step 1/3 | epoch 39/50 | batch 20/60 | global_step 2300 | loss_total 0.2358\n",
      "step 1/3 | epoch 39/50 | batch 21/60 | global_step 2301 | loss_total 0.2102\n",
      "step 1/3 | epoch 39/50 | batch 22/60 | global_step 2302 | loss_total 0.6417\n",
      "step 1/3 | epoch 39/50 | batch 23/60 | global_step 2303 | loss_total 0.3132\n",
      "step 1/3 | epoch 39/50 | batch 24/60 | global_step 2304 | loss_total 0.2176\n",
      "step 1/3 | epoch 39/50 | batch 25/60 | global_step 2305 | loss_total 0.2347\n",
      "step 1/3 | epoch 39/50 | batch 26/60 | global_step 2306 | loss_total 0.2297\n",
      "step 1/3 | epoch 39/50 | batch 27/60 | global_step 2307 | loss_total 0.2266\n",
      "step 1/3 | epoch 39/50 | batch 28/60 | global_step 2308 | loss_total 0.2664\n",
      "step 1/3 | epoch 39/50 | batch 29/60 | global_step 2309 | loss_total 0.2300\n",
      "step 1/3 | epoch 39/50 | batch 30/60 | global_step 2310 | loss_total 0.2193\n",
      "step 1/3 | epoch 39/50 | batch 31/60 | global_step 2311 | loss_total 0.2528\n",
      "step 1/3 | epoch 39/50 | batch 32/60 | global_step 2312 | loss_total 0.4949\n",
      "step 1/3 | epoch 39/50 | batch 33/60 | global_step 2313 | loss_total 0.2288\n",
      "step 1/3 | epoch 39/50 | batch 34/60 | global_step 2314 | loss_total 0.5331\n",
      "step 1/3 | epoch 39/50 | batch 35/60 | global_step 2315 | loss_total 0.2340\n",
      "step 1/3 | epoch 39/50 | batch 36/60 | global_step 2316 | loss_total 0.2192\n",
      "step 1/3 | epoch 39/50 | batch 37/60 | global_step 2317 | loss_total 0.5120\n",
      "step 1/3 | epoch 39/50 | batch 38/60 | global_step 2318 | loss_total 0.2348\n",
      "step 1/3 | epoch 39/50 | batch 39/60 | global_step 2319 | loss_total 0.8829\n",
      "step 1/3 | epoch 39/50 | batch 40/60 | global_step 2320 | loss_total 0.4221\n",
      "step 1/3 | epoch 39/50 | batch 41/60 | global_step 2321 | loss_total 1.4692\n",
      "step 1/3 | epoch 39/50 | batch 42/60 | global_step 2322 | loss_total 0.9133\n",
      "step 1/3 | epoch 39/50 | batch 43/60 | global_step 2323 | loss_total 0.2315\n",
      "step 1/3 | epoch 39/50 | batch 44/60 | global_step 2324 | loss_total 0.2435\n",
      "step 1/3 | epoch 39/50 | batch 45/60 | global_step 2325 | loss_total 0.8293\n",
      "step 1/3 | epoch 39/50 | batch 46/60 | global_step 2326 | loss_total 0.2324\n",
      "step 1/3 | epoch 39/50 | batch 47/60 | global_step 2327 | loss_total 0.2230\n",
      "step 1/3 | epoch 39/50 | batch 48/60 | global_step 2328 | loss_total 0.2275\n",
      "step 1/3 | epoch 39/50 | batch 49/60 | global_step 2329 | loss_total 0.2182\n",
      "step 1/3 | epoch 39/50 | batch 50/60 | global_step 2330 | loss_total 0.5283\n",
      "step 1/3 | epoch 39/50 | batch 51/60 | global_step 2331 | loss_total 0.4811\n",
      "step 1/3 | epoch 39/50 | batch 52/60 | global_step 2332 | loss_total 0.2329\n",
      "step 1/3 | epoch 39/50 | batch 53/60 | global_step 2333 | loss_total 0.2196\n",
      "step 1/3 | epoch 39/50 | batch 54/60 | global_step 2334 | loss_total 0.2069\n",
      "step 1/3 | epoch 39/50 | batch 55/60 | global_step 2335 | loss_total 0.2197\n",
      "step 1/3 | epoch 39/50 | batch 56/60 | global_step 2336 | loss_total 0.2218\n",
      "step 1/3 | epoch 39/50 | batch 57/60 | global_step 2337 | loss_total 0.2464\n",
      "step 1/3 | epoch 39/50 | batch 58/60 | global_step 2338 | loss_total 0.2887\n",
      "step 1/3 | epoch 39/50 | batch 59/60 | global_step 2339 | loss_total 0.2019\n",
      "step 1/3 | epoch 39/50 | batch 60/60 | global_step 2340 | loss_total 0.2105\n",
      "[epoch done] step 1/3 epoch 39/50 | train_total=0.3769 val_total=0.1749\n",
      "step 1/3 | epoch 40/50 | batch 1/60 | global_step 2341 | loss_total 0.2294\n",
      "step 1/3 | epoch 40/50 | batch 2/60 | global_step 2342 | loss_total 0.2345\n",
      "step 1/3 | epoch 40/50 | batch 3/60 | global_step 2343 | loss_total 0.2117\n",
      "step 1/3 | epoch 40/50 | batch 4/60 | global_step 2344 | loss_total 0.9515\n",
      "step 1/3 | epoch 40/50 | batch 5/60 | global_step 2345 | loss_total 0.2235\n",
      "step 1/3 | epoch 40/50 | batch 6/60 | global_step 2346 | loss_total 0.2578\n",
      "step 1/3 | epoch 40/50 | batch 7/60 | global_step 2347 | loss_total 0.7894\n",
      "step 1/3 | epoch 40/50 | batch 8/60 | global_step 2348 | loss_total 0.2610\n",
      "step 1/3 | epoch 40/50 | batch 9/60 | global_step 2349 | loss_total 0.4797\n",
      "step 1/3 | epoch 40/50 | batch 10/60 | global_step 2350 | loss_total 0.4658\n",
      "step 1/3 | epoch 40/50 | batch 11/60 | global_step 2351 | loss_total 0.2199\n",
      "step 1/3 | epoch 40/50 | batch 12/60 | global_step 2352 | loss_total 0.2423\n",
      "step 1/3 | epoch 40/50 | batch 13/60 | global_step 2353 | loss_total 0.2715\n",
      "step 1/3 | epoch 40/50 | batch 14/60 | global_step 2354 | loss_total 0.2699\n",
      "step 1/3 | epoch 40/50 | batch 15/60 | global_step 2355 | loss_total 1.5252\n",
      "step 1/3 | epoch 40/50 | batch 16/60 | global_step 2356 | loss_total 0.2905\n",
      "step 1/3 | epoch 40/50 | batch 17/60 | global_step 2357 | loss_total 0.2407\n",
      "step 1/3 | epoch 40/50 | batch 18/60 | global_step 2358 | loss_total 0.2335\n",
      "step 1/3 | epoch 40/50 | batch 19/60 | global_step 2359 | loss_total 0.2765\n",
      "step 1/3 | epoch 40/50 | batch 20/60 | global_step 2360 | loss_total 0.3546\n",
      "step 1/3 | epoch 40/50 | batch 21/60 | global_step 2361 | loss_total 0.2202\n",
      "step 1/3 | epoch 40/50 | batch 22/60 | global_step 2362 | loss_total 0.8562\n",
      "step 1/3 | epoch 40/50 | batch 23/60 | global_step 2363 | loss_total 0.2321\n",
      "step 1/3 | epoch 40/50 | batch 24/60 | global_step 2364 | loss_total 0.6724\n",
      "step 1/3 | epoch 40/50 | batch 25/60 | global_step 2365 | loss_total 0.2134\n",
      "step 1/3 | epoch 40/50 | batch 26/60 | global_step 2366 | loss_total 0.2207\n",
      "step 1/3 | epoch 40/50 | batch 27/60 | global_step 2367 | loss_total 0.4103\n",
      "step 1/3 | epoch 40/50 | batch 28/60 | global_step 2368 | loss_total 0.3686\n",
      "step 1/3 | epoch 40/50 | batch 29/60 | global_step 2369 | loss_total 0.7389\n",
      "step 1/3 | epoch 40/50 | batch 30/60 | global_step 2370 | loss_total 0.7388\n",
      "step 1/3 | epoch 40/50 | batch 31/60 | global_step 2371 | loss_total 0.2153\n",
      "step 1/3 | epoch 40/50 | batch 32/60 | global_step 2372 | loss_total 0.3462\n",
      "step 1/3 | epoch 40/50 | batch 33/60 | global_step 2373 | loss_total 0.3276\n",
      "step 1/3 | epoch 40/50 | batch 34/60 | global_step 2374 | loss_total 0.2093\n",
      "step 1/3 | epoch 40/50 | batch 35/60 | global_step 2375 | loss_total 0.5904\n",
      "step 1/3 | epoch 40/50 | batch 36/60 | global_step 2376 | loss_total 0.3968\n",
      "step 1/3 | epoch 40/50 | batch 37/60 | global_step 2377 | loss_total 1.2825\n",
      "step 1/3 | epoch 40/50 | batch 38/60 | global_step 2378 | loss_total 0.2516\n",
      "step 1/3 | epoch 40/50 | batch 39/60 | global_step 2379 | loss_total 0.4450\n",
      "step 1/3 | epoch 40/50 | batch 40/60 | global_step 2380 | loss_total 0.2717\n",
      "step 1/3 | epoch 40/50 | batch 41/60 | global_step 2381 | loss_total 0.4031\n",
      "step 1/3 | epoch 40/50 | batch 42/60 | global_step 2382 | loss_total 1.2357\n",
      "step 1/3 | epoch 40/50 | batch 43/60 | global_step 2383 | loss_total 0.2195\n",
      "step 1/3 | epoch 40/50 | batch 44/60 | global_step 2384 | loss_total 0.5666\n",
      "step 1/3 | epoch 40/50 | batch 45/60 | global_step 2385 | loss_total 0.2194\n",
      "step 1/3 | epoch 40/50 | batch 46/60 | global_step 2386 | loss_total 0.3520\n",
      "step 1/3 | epoch 40/50 | batch 47/60 | global_step 2387 | loss_total 0.2216\n",
      "step 1/3 | epoch 40/50 | batch 48/60 | global_step 2388 | loss_total 0.2323\n",
      "step 1/3 | epoch 40/50 | batch 49/60 | global_step 2389 | loss_total 0.2171\n",
      "step 1/3 | epoch 40/50 | batch 50/60 | global_step 2390 | loss_total 0.2614\n",
      "step 1/3 | epoch 40/50 | batch 51/60 | global_step 2391 | loss_total 0.2359\n",
      "step 1/3 | epoch 40/50 | batch 52/60 | global_step 2392 | loss_total 0.2378\n",
      "step 1/3 | epoch 40/50 | batch 53/60 | global_step 2393 | loss_total 0.2142\n",
      "step 1/3 | epoch 40/50 | batch 54/60 | global_step 2394 | loss_total 0.7275\n",
      "step 1/3 | epoch 40/50 | batch 55/60 | global_step 2395 | loss_total 0.2153\n",
      "step 1/3 | epoch 40/50 | batch 56/60 | global_step 2396 | loss_total 0.2234\n",
      "step 1/3 | epoch 40/50 | batch 57/60 | global_step 2397 | loss_total 1.9200\n",
      "step 1/3 | epoch 40/50 | batch 58/60 | global_step 2398 | loss_total 0.7109\n",
      "step 1/3 | epoch 40/50 | batch 59/60 | global_step 2399 | loss_total 0.2064\n",
      "step 1/3 | epoch 40/50 | batch 60/60 | global_step 2400 | loss_total 0.2181\n",
      "[epoch done] step 1/3 epoch 40/50 | train_total=0.4313 val_total=0.2353\n",
      "step 1/3 | epoch 41/50 | batch 1/60 | global_step 2401 | loss_total 0.3923\n",
      "step 1/3 | epoch 41/50 | batch 2/60 | global_step 2402 | loss_total 1.8758\n",
      "step 1/3 | epoch 41/50 | batch 3/60 | global_step 2403 | loss_total 0.2185\n",
      "step 1/3 | epoch 41/50 | batch 4/60 | global_step 2404 | loss_total 0.3526\n",
      "step 1/3 | epoch 41/50 | batch 5/60 | global_step 2405 | loss_total 0.2194\n",
      "step 1/3 | epoch 41/50 | batch 6/60 | global_step 2406 | loss_total 0.5966\n",
      "step 1/3 | epoch 41/50 | batch 7/60 | global_step 2407 | loss_total 0.2711\n",
      "step 1/3 | epoch 41/50 | batch 8/60 | global_step 2408 | loss_total 0.2263\n",
      "step 1/3 | epoch 41/50 | batch 9/60 | global_step 2409 | loss_total 0.2254\n",
      "step 1/3 | epoch 41/50 | batch 10/60 | global_step 2410 | loss_total 0.2236\n",
      "step 1/3 | epoch 41/50 | batch 11/60 | global_step 2411 | loss_total 0.2059\n",
      "step 1/3 | epoch 41/50 | batch 12/60 | global_step 2412 | loss_total 0.2328\n",
      "step 1/3 | epoch 41/50 | batch 13/60 | global_step 2413 | loss_total 0.4098\n",
      "step 1/3 | epoch 41/50 | batch 14/60 | global_step 2414 | loss_total 0.2681\n",
      "step 1/3 | epoch 41/50 | batch 15/60 | global_step 2415 | loss_total 0.2002\n",
      "step 1/3 | epoch 41/50 | batch 16/60 | global_step 2416 | loss_total 0.2249\n",
      "step 1/3 | epoch 41/50 | batch 17/60 | global_step 2417 | loss_total 0.3000\n",
      "step 1/3 | epoch 41/50 | batch 18/60 | global_step 2418 | loss_total 0.2088\n",
      "step 1/3 | epoch 41/50 | batch 19/60 | global_step 2419 | loss_total 0.2267\n",
      "step 1/3 | epoch 41/50 | batch 20/60 | global_step 2420 | loss_total 0.2500\n",
      "step 1/3 | epoch 41/50 | batch 21/60 | global_step 2421 | loss_total 0.2493\n",
      "step 1/3 | epoch 41/50 | batch 22/60 | global_step 2422 | loss_total 0.2271\n",
      "step 1/3 | epoch 41/50 | batch 23/60 | global_step 2423 | loss_total 0.2623\n",
      "step 1/3 | epoch 41/50 | batch 24/60 | global_step 2424 | loss_total 0.2203\n",
      "step 1/3 | epoch 41/50 | batch 25/60 | global_step 2425 | loss_total 0.2000\n",
      "step 1/3 | epoch 41/50 | batch 26/60 | global_step 2426 | loss_total 0.7172\n",
      "step 1/3 | epoch 41/50 | batch 27/60 | global_step 2427 | loss_total 0.2234\n",
      "step 1/3 | epoch 41/50 | batch 28/60 | global_step 2428 | loss_total 0.2422\n",
      "step 1/3 | epoch 41/50 | batch 29/60 | global_step 2429 | loss_total 0.2923\n",
      "step 1/3 | epoch 41/50 | batch 30/60 | global_step 2430 | loss_total 0.2188\n",
      "step 1/3 | epoch 41/50 | batch 31/60 | global_step 2431 | loss_total 0.2088\n",
      "step 1/3 | epoch 41/50 | batch 32/60 | global_step 2432 | loss_total 0.3248\n",
      "step 1/3 | epoch 41/50 | batch 33/60 | global_step 2433 | loss_total 0.6295\n",
      "step 1/3 | epoch 41/50 | batch 34/60 | global_step 2434 | loss_total 0.9035\n",
      "step 1/3 | epoch 41/50 | batch 35/60 | global_step 2435 | loss_total 0.2296\n",
      "step 1/3 | epoch 41/50 | batch 36/60 | global_step 2436 | loss_total 0.8640\n",
      "step 1/3 | epoch 41/50 | batch 37/60 | global_step 2437 | loss_total 0.2217\n",
      "step 1/3 | epoch 41/50 | batch 38/60 | global_step 2438 | loss_total 0.8124\n",
      "step 1/3 | epoch 41/50 | batch 39/60 | global_step 2439 | loss_total 0.2300\n",
      "step 1/3 | epoch 41/50 | batch 40/60 | global_step 2440 | loss_total 0.2105\n",
      "step 1/3 | epoch 41/50 | batch 41/60 | global_step 2441 | loss_total 0.2215\n",
      "step 1/3 | epoch 41/50 | batch 42/60 | global_step 2442 | loss_total 0.2205\n",
      "step 1/3 | epoch 41/50 | batch 43/60 | global_step 2443 | loss_total 0.7660\n",
      "step 1/3 | epoch 41/50 | batch 44/60 | global_step 2444 | loss_total 0.2545\n",
      "step 1/3 | epoch 41/50 | batch 45/60 | global_step 2445 | loss_total 0.2137\n",
      "step 1/3 | epoch 41/50 | batch 46/60 | global_step 2446 | loss_total 0.2191\n",
      "step 1/3 | epoch 41/50 | batch 47/60 | global_step 2447 | loss_total 0.2193\n",
      "step 1/3 | epoch 41/50 | batch 48/60 | global_step 2448 | loss_total 0.2148\n",
      "step 1/3 | epoch 41/50 | batch 49/60 | global_step 2449 | loss_total 0.2347\n",
      "step 1/3 | epoch 41/50 | batch 50/60 | global_step 2450 | loss_total 0.7702\n",
      "step 1/3 | epoch 41/50 | batch 51/60 | global_step 2451 | loss_total 0.2344\n",
      "step 1/3 | epoch 41/50 | batch 52/60 | global_step 2452 | loss_total 0.2019\n",
      "step 1/3 | epoch 41/50 | batch 53/60 | global_step 2453 | loss_total 0.7503\n",
      "step 1/3 | epoch 41/50 | batch 54/60 | global_step 2454 | loss_total 0.4825\n",
      "step 1/3 | epoch 41/50 | batch 55/60 | global_step 2455 | loss_total 0.4053\n",
      "step 1/3 | epoch 41/50 | batch 56/60 | global_step 2456 | loss_total 0.2425\n",
      "step 1/3 | epoch 41/50 | batch 57/60 | global_step 2457 | loss_total 0.2422\n",
      "step 1/3 | epoch 41/50 | batch 58/60 | global_step 2458 | loss_total 0.2290\n",
      "step 1/3 | epoch 41/50 | batch 59/60 | global_step 2459 | loss_total 0.2293\n",
      "step 1/3 | epoch 41/50 | batch 60/60 | global_step 2460 | loss_total 0.8211\n",
      "[epoch done] step 1/3 epoch 41/50 | train_total=0.3632 val_total=0.1894\n",
      "step 1/3 | epoch 42/50 | batch 1/60 | global_step 2461 | loss_total 0.7851\n",
      "step 1/3 | epoch 42/50 | batch 2/60 | global_step 2462 | loss_total 0.2359\n",
      "step 1/3 | epoch 42/50 | batch 3/60 | global_step 2463 | loss_total 0.4021\n",
      "step 1/3 | epoch 42/50 | batch 4/60 | global_step 2464 | loss_total 0.4748\n",
      "step 1/3 | epoch 42/50 | batch 5/60 | global_step 2465 | loss_total 0.2612\n",
      "step 1/3 | epoch 42/50 | batch 6/60 | global_step 2466 | loss_total 0.2347\n",
      "step 1/3 | epoch 42/50 | batch 7/60 | global_step 2467 | loss_total 0.2228\n",
      "step 1/3 | epoch 42/50 | batch 8/60 | global_step 2468 | loss_total 0.2150\n",
      "step 1/3 | epoch 42/50 | batch 9/60 | global_step 2469 | loss_total 0.6384\n",
      "step 1/3 | epoch 42/50 | batch 10/60 | global_step 2470 | loss_total 0.2169\n",
      "step 1/3 | epoch 42/50 | batch 11/60 | global_step 2471 | loss_total 0.2319\n",
      "step 1/3 | epoch 42/50 | batch 12/60 | global_step 2472 | loss_total 0.2155\n",
      "step 1/3 | epoch 42/50 | batch 13/60 | global_step 2473 | loss_total 0.3358\n",
      "step 1/3 | epoch 42/50 | batch 14/60 | global_step 2474 | loss_total 0.5974\n",
      "step 1/3 | epoch 42/50 | batch 15/60 | global_step 2475 | loss_total 0.2128\n",
      "step 1/3 | epoch 42/50 | batch 16/60 | global_step 2476 | loss_total 0.2224\n",
      "step 1/3 | epoch 42/50 | batch 17/60 | global_step 2477 | loss_total 0.2150\n",
      "step 1/3 | epoch 42/50 | batch 18/60 | global_step 2478 | loss_total 0.7629\n",
      "step 1/3 | epoch 42/50 | batch 19/60 | global_step 2479 | loss_total 0.3397\n",
      "step 1/3 | epoch 42/50 | batch 20/60 | global_step 2480 | loss_total 0.6749\n",
      "step 1/3 | epoch 42/50 | batch 21/60 | global_step 2481 | loss_total 0.2184\n",
      "step 1/3 | epoch 42/50 | batch 22/60 | global_step 2482 | loss_total 0.3340\n",
      "step 1/3 | epoch 42/50 | batch 23/60 | global_step 2483 | loss_total 0.1997\n",
      "step 1/3 | epoch 42/50 | batch 24/60 | global_step 2484 | loss_total 0.2073\n",
      "step 1/3 | epoch 42/50 | batch 25/60 | global_step 2485 | loss_total 0.2213\n",
      "step 1/3 | epoch 42/50 | batch 26/60 | global_step 2486 | loss_total 0.2652\n",
      "step 1/3 | epoch 42/50 | batch 27/60 | global_step 2487 | loss_total 0.2183\n",
      "step 1/3 | epoch 42/50 | batch 28/60 | global_step 2488 | loss_total 0.2018\n",
      "step 1/3 | epoch 42/50 | batch 29/60 | global_step 2489 | loss_total 0.5193\n",
      "step 1/3 | epoch 42/50 | batch 30/60 | global_step 2490 | loss_total 0.2049\n",
      "step 1/3 | epoch 42/50 | batch 31/60 | global_step 2491 | loss_total 0.2218\n",
      "step 1/3 | epoch 42/50 | batch 32/60 | global_step 2492 | loss_total 0.2216\n",
      "step 1/3 | epoch 42/50 | batch 33/60 | global_step 2493 | loss_total 0.2040\n",
      "step 1/3 | epoch 42/50 | batch 34/60 | global_step 2494 | loss_total 0.1824\n",
      "step 1/3 | epoch 42/50 | batch 35/60 | global_step 2495 | loss_total 0.1922\n",
      "step 1/3 | epoch 42/50 | batch 36/60 | global_step 2496 | loss_total 0.3729\n",
      "step 1/3 | epoch 42/50 | batch 37/60 | global_step 2497 | loss_total 0.1890\n",
      "step 1/3 | epoch 42/50 | batch 38/60 | global_step 2498 | loss_total 0.8753\n",
      "step 1/3 | epoch 42/50 | batch 39/60 | global_step 2499 | loss_total 1.5176\n",
      "step 1/3 | epoch 42/50 | batch 40/60 | global_step 2500 | loss_total 0.1619\n",
      "step 1/3 | epoch 42/50 | batch 41/60 | global_step 2501 | loss_total 0.2810\n",
      "step 1/3 | epoch 42/50 | batch 42/60 | global_step 2502 | loss_total 1.5486\n",
      "step 1/3 | epoch 42/50 | batch 43/60 | global_step 2503 | loss_total 0.7926\n",
      "step 1/3 | epoch 42/50 | batch 44/60 | global_step 2504 | loss_total 0.1863\n",
      "step 1/3 | epoch 42/50 | batch 45/60 | global_step 2505 | loss_total 1.3546\n",
      "step 1/3 | epoch 42/50 | batch 46/60 | global_step 2506 | loss_total 0.2277\n",
      "step 1/3 | epoch 42/50 | batch 47/60 | global_step 2507 | loss_total 0.2337\n",
      "step 1/3 | epoch 42/50 | batch 48/60 | global_step 2508 | loss_total 0.6802\n",
      "step 1/3 | epoch 42/50 | batch 49/60 | global_step 2509 | loss_total 0.2452\n",
      "step 1/3 | epoch 42/50 | batch 50/60 | global_step 2510 | loss_total 0.2642\n",
      "step 1/3 | epoch 42/50 | batch 51/60 | global_step 2511 | loss_total 0.2258\n",
      "step 1/3 | epoch 42/50 | batch 52/60 | global_step 2512 | loss_total 0.2543\n",
      "step 1/3 | epoch 42/50 | batch 53/60 | global_step 2513 | loss_total 0.2610\n",
      "step 1/3 | epoch 42/50 | batch 54/60 | global_step 2514 | loss_total 0.2603\n",
      "step 1/3 | epoch 42/50 | batch 55/60 | global_step 2515 | loss_total 0.2399\n",
      "step 1/3 | epoch 42/50 | batch 56/60 | global_step 2516 | loss_total 0.2204\n",
      "step 1/3 | epoch 42/50 | batch 57/60 | global_step 2517 | loss_total 0.2594\n",
      "step 1/3 | epoch 42/50 | batch 58/60 | global_step 2518 | loss_total 0.3155\n",
      "step 1/3 | epoch 42/50 | batch 59/60 | global_step 2519 | loss_total 0.2180\n",
      "step 1/3 | epoch 42/50 | batch 60/60 | global_step 2520 | loss_total 0.2664\n",
      "[epoch done] step 1/3 epoch 42/50 | train_total=0.3760 val_total=0.2332\n",
      "step 1/3 | epoch 43/50 | batch 1/60 | global_step 2521 | loss_total 0.4558\n",
      "step 1/3 | epoch 43/50 | batch 2/60 | global_step 2522 | loss_total 0.2282\n",
      "step 1/3 | epoch 43/50 | batch 3/60 | global_step 2523 | loss_total 0.2161\n",
      "step 1/3 | epoch 43/50 | batch 4/60 | global_step 2524 | loss_total 0.2225\n",
      "step 1/3 | epoch 43/50 | batch 5/60 | global_step 2525 | loss_total 0.2263\n",
      "step 1/3 | epoch 43/50 | batch 6/60 | global_step 2526 | loss_total 0.2158\n",
      "step 1/3 | epoch 43/50 | batch 7/60 | global_step 2527 | loss_total 0.2224\n",
      "step 1/3 | epoch 43/50 | batch 8/60 | global_step 2528 | loss_total 0.3019\n",
      "step 1/3 | epoch 43/50 | batch 9/60 | global_step 2529 | loss_total 0.2334\n",
      "step 1/3 | epoch 43/50 | batch 10/60 | global_step 2530 | loss_total 0.2155\n",
      "step 1/3 | epoch 43/50 | batch 11/60 | global_step 2531 | loss_total 0.5926\n",
      "step 1/3 | epoch 43/50 | batch 12/60 | global_step 2532 | loss_total 0.2177\n",
      "step 1/3 | epoch 43/50 | batch 13/60 | global_step 2533 | loss_total 0.2176\n",
      "step 1/3 | epoch 43/50 | batch 14/60 | global_step 2534 | loss_total 0.2289\n",
      "step 1/3 | epoch 43/50 | batch 15/60 | global_step 2535 | loss_total 0.4548\n",
      "step 1/3 | epoch 43/50 | batch 16/60 | global_step 2536 | loss_total 0.2167\n",
      "step 1/3 | epoch 43/50 | batch 17/60 | global_step 2537 | loss_total 0.2155\n",
      "step 1/3 | epoch 43/50 | batch 18/60 | global_step 2538 | loss_total 0.2161\n",
      "step 1/3 | epoch 43/50 | batch 19/60 | global_step 2539 | loss_total 0.2221\n",
      "step 1/3 | epoch 43/50 | batch 20/60 | global_step 2540 | loss_total 1.2679\n",
      "step 1/3 | epoch 43/50 | batch 21/60 | global_step 2541 | loss_total 0.2135\n",
      "step 1/3 | epoch 43/50 | batch 22/60 | global_step 2542 | loss_total 0.2252\n",
      "step 1/3 | epoch 43/50 | batch 23/60 | global_step 2543 | loss_total 0.2249\n",
      "step 1/3 | epoch 43/50 | batch 24/60 | global_step 2544 | loss_total 0.2216\n",
      "step 1/3 | epoch 43/50 | batch 25/60 | global_step 2545 | loss_total 0.2219\n",
      "step 1/3 | epoch 43/50 | batch 26/60 | global_step 2546 | loss_total 0.2187\n",
      "step 1/3 | epoch 43/50 | batch 27/60 | global_step 2547 | loss_total 0.2184\n",
      "step 1/3 | epoch 43/50 | batch 28/60 | global_step 2548 | loss_total 0.6163\n",
      "step 1/3 | epoch 43/50 | batch 29/60 | global_step 2549 | loss_total 0.2163\n",
      "step 1/3 | epoch 43/50 | batch 30/60 | global_step 2550 | loss_total 0.2222\n",
      "step 1/3 | epoch 43/50 | batch 31/60 | global_step 2551 | loss_total 0.2847\n",
      "step 1/3 | epoch 43/50 | batch 32/60 | global_step 2552 | loss_total 0.7744\n",
      "step 1/3 | epoch 43/50 | batch 33/60 | global_step 2553 | loss_total 1.1669\n",
      "step 1/3 | epoch 43/50 | batch 34/60 | global_step 2554 | loss_total 0.2274\n",
      "step 1/3 | epoch 43/50 | batch 35/60 | global_step 2555 | loss_total 0.2210\n",
      "step 1/3 | epoch 43/50 | batch 36/60 | global_step 2556 | loss_total 0.2242\n",
      "step 1/3 | epoch 43/50 | batch 37/60 | global_step 2557 | loss_total 0.2174\n",
      "step 1/3 | epoch 43/50 | batch 38/60 | global_step 2558 | loss_total 0.2188\n",
      "step 1/3 | epoch 43/50 | batch 39/60 | global_step 2559 | loss_total 0.8531\n",
      "step 1/3 | epoch 43/50 | batch 40/60 | global_step 2560 | loss_total 0.2305\n",
      "step 1/3 | epoch 43/50 | batch 41/60 | global_step 2561 | loss_total 0.2629\n",
      "step 1/3 | epoch 43/50 | batch 42/60 | global_step 2562 | loss_total 0.2179\n",
      "step 1/3 | epoch 43/50 | batch 43/60 | global_step 2563 | loss_total 0.6607\n",
      "step 1/3 | epoch 43/50 | batch 44/60 | global_step 2564 | loss_total 0.5562\n",
      "step 1/3 | epoch 43/50 | batch 45/60 | global_step 2565 | loss_total 0.2182\n",
      "step 1/3 | epoch 43/50 | batch 46/60 | global_step 2566 | loss_total 0.2180\n",
      "step 1/3 | epoch 43/50 | batch 47/60 | global_step 2567 | loss_total 0.2220\n",
      "step 1/3 | epoch 43/50 | batch 48/60 | global_step 2568 | loss_total 0.7151\n",
      "step 1/3 | epoch 43/50 | batch 49/60 | global_step 2569 | loss_total 0.4696\n",
      "step 1/3 | epoch 43/50 | batch 50/60 | global_step 2570 | loss_total 0.2191\n",
      "step 1/3 | epoch 43/50 | batch 51/60 | global_step 2571 | loss_total 0.6787\n",
      "step 1/3 | epoch 43/50 | batch 52/60 | global_step 2572 | loss_total 0.2202\n",
      "step 1/3 | epoch 43/50 | batch 53/60 | global_step 2573 | loss_total 0.3458\n",
      "step 1/3 | epoch 43/50 | batch 54/60 | global_step 2574 | loss_total 1.1836\n",
      "step 1/3 | epoch 43/50 | batch 55/60 | global_step 2575 | loss_total 0.2226\n",
      "step 1/3 | epoch 43/50 | batch 56/60 | global_step 2576 | loss_total 0.2131\n",
      "step 1/3 | epoch 43/50 | batch 57/60 | global_step 2577 | loss_total 0.2224\n",
      "step 1/3 | epoch 43/50 | batch 58/60 | global_step 2578 | loss_total 0.2223\n",
      "step 1/3 | epoch 43/50 | batch 59/60 | global_step 2579 | loss_total 0.2444\n",
      "step 1/3 | epoch 43/50 | batch 60/60 | global_step 2580 | loss_total 0.2211\n",
      "[epoch done] step 1/3 epoch 43/50 | train_total=0.3490 val_total=0.2391\n",
      "step 1/3 | epoch 44/50 | batch 1/60 | global_step 2581 | loss_total 0.2198\n",
      "step 1/3 | epoch 44/50 | batch 2/60 | global_step 2582 | loss_total 0.2199\n",
      "step 1/3 | epoch 44/50 | batch 3/60 | global_step 2583 | loss_total 0.2205\n",
      "step 1/3 | epoch 44/50 | batch 4/60 | global_step 2584 | loss_total 0.2228\n",
      "step 1/3 | epoch 44/50 | batch 5/60 | global_step 2585 | loss_total 0.2090\n",
      "step 1/3 | epoch 44/50 | batch 6/60 | global_step 2586 | loss_total 0.2340\n",
      "step 1/3 | epoch 44/50 | batch 7/60 | global_step 2587 | loss_total 0.2189\n",
      "step 1/3 | epoch 44/50 | batch 8/60 | global_step 2588 | loss_total 0.2039\n",
      "step 1/3 | epoch 44/50 | batch 9/60 | global_step 2589 | loss_total 0.2298\n",
      "step 1/3 | epoch 44/50 | batch 10/60 | global_step 2590 | loss_total 0.2242\n",
      "step 1/3 | epoch 44/50 | batch 11/60 | global_step 2591 | loss_total 0.2038\n",
      "step 1/3 | epoch 44/50 | batch 12/60 | global_step 2592 | loss_total 0.7151\n",
      "step 1/3 | epoch 44/50 | batch 13/60 | global_step 2593 | loss_total 0.8721\n",
      "step 1/3 | epoch 44/50 | batch 14/60 | global_step 2594 | loss_total 0.2097\n",
      "step 1/3 | epoch 44/50 | batch 15/60 | global_step 2595 | loss_total 0.2349\n",
      "step 1/3 | epoch 44/50 | batch 16/60 | global_step 2596 | loss_total 0.2163\n",
      "step 1/3 | epoch 44/50 | batch 17/60 | global_step 2597 | loss_total 0.2116\n",
      "step 1/3 | epoch 44/50 | batch 18/60 | global_step 2598 | loss_total 1.4133\n",
      "step 1/3 | epoch 44/50 | batch 19/60 | global_step 2599 | loss_total 0.3275\n",
      "step 1/3 | epoch 44/50 | batch 20/60 | global_step 2600 | loss_total 0.1992\n",
      "step 1/3 | epoch 44/50 | batch 21/60 | global_step 2601 | loss_total 0.8050\n",
      "step 1/3 | epoch 44/50 | batch 22/60 | global_step 2602 | loss_total 0.7685\n",
      "step 1/3 | epoch 44/50 | batch 23/60 | global_step 2603 | loss_total 0.3511\n",
      "step 1/3 | epoch 44/50 | batch 24/60 | global_step 2604 | loss_total 0.2278\n",
      "step 1/3 | epoch 44/50 | batch 25/60 | global_step 2605 | loss_total 0.2392\n",
      "step 1/3 | epoch 44/50 | batch 26/60 | global_step 2606 | loss_total 0.6273\n",
      "step 1/3 | epoch 44/50 | batch 27/60 | global_step 2607 | loss_total 0.2608\n",
      "step 1/3 | epoch 44/50 | batch 28/60 | global_step 2608 | loss_total 0.2061\n",
      "step 1/3 | epoch 44/50 | batch 29/60 | global_step 2609 | loss_total 0.2878\n",
      "step 1/3 | epoch 44/50 | batch 30/60 | global_step 2610 | loss_total 0.2831\n",
      "step 1/3 | epoch 44/50 | batch 31/60 | global_step 2611 | loss_total 0.2062\n",
      "step 1/3 | epoch 44/50 | batch 32/60 | global_step 2612 | loss_total 0.2342\n",
      "step 1/3 | epoch 44/50 | batch 33/60 | global_step 2613 | loss_total 0.2245\n",
      "step 1/3 | epoch 44/50 | batch 34/60 | global_step 2614 | loss_total 0.2362\n",
      "step 1/3 | epoch 44/50 | batch 35/60 | global_step 2615 | loss_total 0.9510\n",
      "step 1/3 | epoch 44/50 | batch 36/60 | global_step 2616 | loss_total 0.2559\n",
      "step 1/3 | epoch 44/50 | batch 37/60 | global_step 2617 | loss_total 0.2502\n",
      "step 1/3 | epoch 44/50 | batch 38/60 | global_step 2618 | loss_total 0.2415\n",
      "step 1/3 | epoch 44/50 | batch 39/60 | global_step 2619 | loss_total 0.2089\n",
      "step 1/3 | epoch 44/50 | batch 40/60 | global_step 2620 | loss_total 0.3192\n",
      "step 1/3 | epoch 44/50 | batch 41/60 | global_step 2621 | loss_total 0.2052\n",
      "step 1/3 | epoch 44/50 | batch 42/60 | global_step 2622 | loss_total 1.3439\n",
      "step 1/3 | epoch 44/50 | batch 43/60 | global_step 2623 | loss_total 0.2175\n",
      "step 1/3 | epoch 44/50 | batch 44/60 | global_step 2624 | loss_total 0.2051\n",
      "step 1/3 | epoch 44/50 | batch 45/60 | global_step 2625 | loss_total 0.2257\n",
      "step 1/3 | epoch 44/50 | batch 46/60 | global_step 2626 | loss_total 1.0206\n",
      "step 1/3 | epoch 44/50 | batch 47/60 | global_step 2627 | loss_total 0.2346\n",
      "step 1/3 | epoch 44/50 | batch 48/60 | global_step 2628 | loss_total 0.2294\n",
      "step 1/3 | epoch 44/50 | batch 49/60 | global_step 2629 | loss_total 0.2240\n",
      "step 1/3 | epoch 44/50 | batch 50/60 | global_step 2630 | loss_total 0.2276\n",
      "step 1/3 | epoch 44/50 | batch 51/60 | global_step 2631 | loss_total 0.2792\n",
      "step 1/3 | epoch 44/50 | batch 52/60 | global_step 2632 | loss_total 0.5537\n",
      "step 1/3 | epoch 44/50 | batch 53/60 | global_step 2633 | loss_total 0.2253\n",
      "step 1/3 | epoch 44/50 | batch 54/60 | global_step 2634 | loss_total 0.2158\n",
      "step 1/3 | epoch 44/50 | batch 55/60 | global_step 2635 | loss_total 0.2873\n",
      "step 1/3 | epoch 44/50 | batch 56/60 | global_step 2636 | loss_total 0.2063\n",
      "step 1/3 | epoch 44/50 | batch 57/60 | global_step 2637 | loss_total 0.2084\n",
      "step 1/3 | epoch 44/50 | batch 58/60 | global_step 2638 | loss_total 0.2915\n",
      "step 1/3 | epoch 44/50 | batch 59/60 | global_step 2639 | loss_total 0.6393\n",
      "step 1/3 | epoch 44/50 | batch 60/60 | global_step 2640 | loss_total 1.0101\n",
      "[epoch done] step 1/3 epoch 44/50 | train_total=0.3674 val_total=0.2413\n",
      "step 1/3 | epoch 45/50 | batch 1/60 | global_step 2641 | loss_total 0.2468\n",
      "step 1/3 | epoch 45/50 | batch 2/60 | global_step 2642 | loss_total 0.2432\n",
      "step 1/3 | epoch 45/50 | batch 3/60 | global_step 2643 | loss_total 0.2230\n",
      "step 1/3 | epoch 45/50 | batch 4/60 | global_step 2644 | loss_total 0.2340\n",
      "step 1/3 | epoch 45/50 | batch 5/60 | global_step 2645 | loss_total 0.2223\n",
      "step 1/3 | epoch 45/50 | batch 6/60 | global_step 2646 | loss_total 0.2721\n",
      "step 1/3 | epoch 45/50 | batch 7/60 | global_step 2647 | loss_total 0.2172\n",
      "step 1/3 | epoch 45/50 | batch 8/60 | global_step 2648 | loss_total 0.3904\n",
      "step 1/3 | epoch 45/50 | batch 9/60 | global_step 2649 | loss_total 0.2178\n",
      "step 1/3 | epoch 45/50 | batch 10/60 | global_step 2650 | loss_total 1.2495\n",
      "step 1/3 | epoch 45/50 | batch 11/60 | global_step 2651 | loss_total 1.3743\n",
      "step 1/3 | epoch 45/50 | batch 12/60 | global_step 2652 | loss_total 0.2164\n",
      "step 1/3 | epoch 45/50 | batch 13/60 | global_step 2653 | loss_total 1.0510\n",
      "step 1/3 | epoch 45/50 | batch 14/60 | global_step 2654 | loss_total 0.2181\n",
      "step 1/3 | epoch 45/50 | batch 15/60 | global_step 2655 | loss_total 0.2169\n",
      "step 1/3 | epoch 45/50 | batch 16/60 | global_step 2656 | loss_total 0.2150\n",
      "step 1/3 | epoch 45/50 | batch 17/60 | global_step 2657 | loss_total 0.2199\n",
      "step 1/3 | epoch 45/50 | batch 18/60 | global_step 2658 | loss_total 0.2173\n",
      "step 1/3 | epoch 45/50 | batch 19/60 | global_step 2659 | loss_total 0.3508\n",
      "step 1/3 | epoch 45/50 | batch 20/60 | global_step 2660 | loss_total 0.2325\n",
      "step 1/3 | epoch 45/50 | batch 21/60 | global_step 2661 | loss_total 0.2169\n",
      "step 1/3 | epoch 45/50 | batch 22/60 | global_step 2662 | loss_total 0.2177\n",
      "step 1/3 | epoch 45/50 | batch 23/60 | global_step 2663 | loss_total 0.2318\n",
      "step 1/3 | epoch 45/50 | batch 24/60 | global_step 2664 | loss_total 0.2337\n",
      "step 1/3 | epoch 45/50 | batch 25/60 | global_step 2665 | loss_total 0.2231\n",
      "step 1/3 | epoch 45/50 | batch 26/60 | global_step 2666 | loss_total 0.3251\n",
      "step 1/3 | epoch 45/50 | batch 27/60 | global_step 2667 | loss_total 0.3254\n",
      "step 1/3 | epoch 45/50 | batch 28/60 | global_step 2668 | loss_total 0.2208\n",
      "step 1/3 | epoch 45/50 | batch 29/60 | global_step 2669 | loss_total 0.2753\n",
      "step 1/3 | epoch 45/50 | batch 30/60 | global_step 2670 | loss_total 0.2163\n",
      "step 1/3 | epoch 45/50 | batch 31/60 | global_step 2671 | loss_total 0.6179\n",
      "step 1/3 | epoch 45/50 | batch 32/60 | global_step 2672 | loss_total 0.2794\n",
      "step 1/3 | epoch 45/50 | batch 33/60 | global_step 2673 | loss_total 0.2267\n",
      "step 1/3 | epoch 45/50 | batch 34/60 | global_step 2674 | loss_total 0.2335\n",
      "step 1/3 | epoch 45/50 | batch 35/60 | global_step 2675 | loss_total 0.6665\n",
      "step 1/3 | epoch 45/50 | batch 36/60 | global_step 2676 | loss_total 0.2272\n",
      "step 1/3 | epoch 45/50 | batch 37/60 | global_step 2677 | loss_total 0.2091\n",
      "step 1/3 | epoch 45/50 | batch 38/60 | global_step 2678 | loss_total 0.2266\n",
      "step 1/3 | epoch 45/50 | batch 39/60 | global_step 2679 | loss_total 0.2168\n",
      "step 1/3 | epoch 45/50 | batch 40/60 | global_step 2680 | loss_total 0.2305\n",
      "step 1/3 | epoch 45/50 | batch 41/60 | global_step 2681 | loss_total 0.2145\n",
      "step 1/3 | epoch 45/50 | batch 42/60 | global_step 2682 | loss_total 0.2027\n",
      "step 1/3 | epoch 45/50 | batch 43/60 | global_step 2683 | loss_total 0.2178\n",
      "step 1/3 | epoch 45/50 | batch 44/60 | global_step 2684 | loss_total 0.2229\n",
      "step 1/3 | epoch 45/50 | batch 45/60 | global_step 2685 | loss_total 0.2204\n",
      "step 1/3 | epoch 45/50 | batch 46/60 | global_step 2686 | loss_total 0.7833\n",
      "step 1/3 | epoch 45/50 | batch 47/60 | global_step 2687 | loss_total 0.2208\n",
      "step 1/3 | epoch 45/50 | batch 48/60 | global_step 2688 | loss_total 0.1990\n",
      "step 1/3 | epoch 45/50 | batch 49/60 | global_step 2689 | loss_total 0.2191\n",
      "step 1/3 | epoch 45/50 | batch 50/60 | global_step 2690 | loss_total 0.5221\n",
      "step 1/3 | epoch 45/50 | batch 51/60 | global_step 2691 | loss_total 0.3976\n",
      "step 1/3 | epoch 45/50 | batch 52/60 | global_step 2692 | loss_total 0.4675\n",
      "step 1/3 | epoch 45/50 | batch 53/60 | global_step 2693 | loss_total 0.2400\n",
      "step 1/3 | epoch 45/50 | batch 54/60 | global_step 2694 | loss_total 0.2321\n",
      "step 1/3 | epoch 45/50 | batch 55/60 | global_step 2695 | loss_total 0.2128\n",
      "step 1/3 | epoch 45/50 | batch 56/60 | global_step 2696 | loss_total 0.2159\n",
      "step 1/3 | epoch 45/50 | batch 57/60 | global_step 2697 | loss_total 0.2726\n",
      "step 1/3 | epoch 45/50 | batch 58/60 | global_step 2698 | loss_total 0.2343\n",
      "step 1/3 | epoch 45/50 | batch 59/60 | global_step 2699 | loss_total 0.2005\n",
      "step 1/3 | epoch 45/50 | batch 60/60 | global_step 2700 | loss_total 0.2025\n",
      "[epoch done] step 1/3 epoch 45/50 | train_total=0.3191 val_total=0.2332\n",
      "step 1/3 | epoch 46/50 | batch 1/60 | global_step 2701 | loss_total 0.2124\n",
      "step 1/3 | epoch 46/50 | batch 2/60 | global_step 2702 | loss_total 0.2116\n",
      "step 1/3 | epoch 46/50 | batch 3/60 | global_step 2703 | loss_total 0.4056\n",
      "step 1/3 | epoch 46/50 | batch 4/60 | global_step 2704 | loss_total 0.2081\n",
      "step 1/3 | epoch 46/50 | batch 5/60 | global_step 2705 | loss_total 0.2126\n",
      "step 1/3 | epoch 46/50 | batch 6/60 | global_step 2706 | loss_total 0.4286\n",
      "step 1/3 | epoch 46/50 | batch 7/60 | global_step 2707 | loss_total 0.3300\n",
      "step 1/3 | epoch 46/50 | batch 8/60 | global_step 2708 | loss_total 0.2250\n",
      "step 1/3 | epoch 46/50 | batch 9/60 | global_step 2709 | loss_total 0.3042\n",
      "step 1/3 | epoch 46/50 | batch 10/60 | global_step 2710 | loss_total 0.2301\n",
      "step 1/3 | epoch 46/50 | batch 11/60 | global_step 2711 | loss_total 0.2169\n",
      "step 1/3 | epoch 46/50 | batch 12/60 | global_step 2712 | loss_total 0.2120\n",
      "step 1/3 | epoch 46/50 | batch 13/60 | global_step 2713 | loss_total 0.2197\n",
      "step 1/3 | epoch 46/50 | batch 14/60 | global_step 2714 | loss_total 0.2107\n",
      "step 1/3 | epoch 46/50 | batch 15/60 | global_step 2715 | loss_total 0.2252\n",
      "step 1/3 | epoch 46/50 | batch 16/60 | global_step 2716 | loss_total 0.3081\n",
      "step 1/3 | epoch 46/50 | batch 17/60 | global_step 2717 | loss_total 0.7523\n",
      "step 1/3 | epoch 46/50 | batch 18/60 | global_step 2718 | loss_total 0.2324\n",
      "step 1/3 | epoch 46/50 | batch 19/60 | global_step 2719 | loss_total 0.2126\n",
      "step 1/3 | epoch 46/50 | batch 20/60 | global_step 2720 | loss_total 0.2135\n",
      "step 1/3 | epoch 46/50 | batch 21/60 | global_step 2721 | loss_total 0.2125\n",
      "step 1/3 | epoch 46/50 | batch 22/60 | global_step 2722 | loss_total 0.5128\n",
      "step 1/3 | epoch 46/50 | batch 23/60 | global_step 2723 | loss_total 0.5796\n",
      "step 1/3 | epoch 46/50 | batch 24/60 | global_step 2724 | loss_total 0.8211\n",
      "step 1/3 | epoch 46/50 | batch 25/60 | global_step 2725 | loss_total 0.2089\n",
      "step 1/3 | epoch 46/50 | batch 26/60 | global_step 2726 | loss_total 0.2033\n",
      "step 1/3 | epoch 46/50 | batch 27/60 | global_step 2727 | loss_total 0.2022\n",
      "step 1/3 | epoch 46/50 | batch 28/60 | global_step 2728 | loss_total 0.2006\n",
      "step 1/3 | epoch 46/50 | batch 29/60 | global_step 2729 | loss_total 0.2311\n",
      "step 1/3 | epoch 46/50 | batch 30/60 | global_step 2730 | loss_total 0.1963\n",
      "step 1/3 | epoch 46/50 | batch 31/60 | global_step 2731 | loss_total 1.5834\n",
      "step 1/3 | epoch 46/50 | batch 32/60 | global_step 2732 | loss_total 0.2129\n",
      "step 1/3 | epoch 46/50 | batch 33/60 | global_step 2733 | loss_total 0.2423\n",
      "step 1/3 | epoch 46/50 | batch 34/60 | global_step 2734 | loss_total 0.1960\n",
      "step 1/3 | epoch 46/50 | batch 35/60 | global_step 2735 | loss_total 0.1937\n",
      "step 1/3 | epoch 46/50 | batch 36/60 | global_step 2736 | loss_total 0.1921\n",
      "step 1/3 | epoch 46/50 | batch 37/60 | global_step 2737 | loss_total 0.2394\n",
      "step 1/3 | epoch 46/50 | batch 38/60 | global_step 2738 | loss_total 0.1987\n",
      "step 1/3 | epoch 46/50 | batch 39/60 | global_step 2739 | loss_total 0.6179\n",
      "step 1/3 | epoch 46/50 | batch 40/60 | global_step 2740 | loss_total 0.2415\n",
      "step 1/3 | epoch 46/50 | batch 41/60 | global_step 2741 | loss_total 0.2199\n",
      "step 1/3 | epoch 46/50 | batch 42/60 | global_step 2742 | loss_total 0.2171\n",
      "step 1/3 | epoch 46/50 | batch 43/60 | global_step 2743 | loss_total 1.5000\n",
      "step 1/3 | epoch 46/50 | batch 44/60 | global_step 2744 | loss_total 0.2134\n",
      "step 1/3 | epoch 46/50 | batch 45/60 | global_step 2745 | loss_total 0.2171\n",
      "step 1/3 | epoch 46/50 | batch 46/60 | global_step 2746 | loss_total 0.2361\n",
      "step 1/3 | epoch 46/50 | batch 47/60 | global_step 2747 | loss_total 0.2666\n",
      "step 1/3 | epoch 46/50 | batch 48/60 | global_step 2748 | loss_total 0.2304\n",
      "step 1/3 | epoch 46/50 | batch 49/60 | global_step 2749 | loss_total 0.2204\n",
      "step 1/3 | epoch 46/50 | batch 50/60 | global_step 2750 | loss_total 1.0689\n",
      "step 1/3 | epoch 46/50 | batch 51/60 | global_step 2751 | loss_total 1.4392\n",
      "step 1/3 | epoch 46/50 | batch 52/60 | global_step 2752 | loss_total 0.2171\n",
      "step 1/3 | epoch 46/50 | batch 53/60 | global_step 2753 | loss_total 0.9614\n",
      "step 1/3 | epoch 46/50 | batch 54/60 | global_step 2754 | loss_total 0.2190\n",
      "step 1/3 | epoch 46/50 | batch 55/60 | global_step 2755 | loss_total 0.2347\n",
      "step 1/3 | epoch 46/50 | batch 56/60 | global_step 2756 | loss_total 0.2148\n",
      "step 1/3 | epoch 46/50 | batch 57/60 | global_step 2757 | loss_total 0.2158\n",
      "step 1/3 | epoch 46/50 | batch 58/60 | global_step 2758 | loss_total 0.7590\n",
      "step 1/3 | epoch 46/50 | batch 59/60 | global_step 2759 | loss_total 0.2188\n",
      "step 1/3 | epoch 46/50 | batch 60/60 | global_step 2760 | loss_total 0.8622\n",
      "[epoch done] step 1/3 epoch 46/50 | train_total=0.3765 val_total=0.2194\n",
      "step 1/3 | epoch 47/50 | batch 1/60 | global_step 2761 | loss_total 1.0280\n",
      "step 1/3 | epoch 47/50 | batch 2/60 | global_step 2762 | loss_total 0.2107\n",
      "step 1/3 | epoch 47/50 | batch 3/60 | global_step 2763 | loss_total 0.2150\n",
      "step 1/3 | epoch 47/50 | batch 4/60 | global_step 2764 | loss_total 0.2110\n",
      "step 1/3 | epoch 47/50 | batch 5/60 | global_step 2765 | loss_total 0.5976\n",
      "step 1/3 | epoch 47/50 | batch 6/60 | global_step 2766 | loss_total 0.2238\n",
      "step 1/3 | epoch 47/50 | batch 7/60 | global_step 2767 | loss_total 0.4214\n",
      "step 1/3 | epoch 47/50 | batch 8/60 | global_step 2768 | loss_total 0.2157\n",
      "step 1/3 | epoch 47/50 | batch 9/60 | global_step 2769 | loss_total 0.4686\n",
      "step 1/3 | epoch 47/50 | batch 10/60 | global_step 2770 | loss_total 0.2578\n",
      "step 1/3 | epoch 47/50 | batch 11/60 | global_step 2771 | loss_total 0.2229\n",
      "step 1/3 | epoch 47/50 | batch 12/60 | global_step 2772 | loss_total 0.2357\n",
      "step 1/3 | epoch 47/50 | batch 13/60 | global_step 2773 | loss_total 0.2583\n",
      "step 1/3 | epoch 47/50 | batch 14/60 | global_step 2774 | loss_total 0.2174\n",
      "step 1/3 | epoch 47/50 | batch 15/60 | global_step 2775 | loss_total 0.2167\n",
      "step 1/3 | epoch 47/50 | batch 16/60 | global_step 2776 | loss_total 0.4372\n",
      "step 1/3 | epoch 47/50 | batch 17/60 | global_step 2777 | loss_total 0.2390\n",
      "step 1/3 | epoch 47/50 | batch 18/60 | global_step 2778 | loss_total 0.2167\n",
      "step 1/3 | epoch 47/50 | batch 19/60 | global_step 2779 | loss_total 0.7735\n",
      "step 1/3 | epoch 47/50 | batch 20/60 | global_step 2780 | loss_total 0.5963\n",
      "step 1/3 | epoch 47/50 | batch 21/60 | global_step 2781 | loss_total 0.3061\n",
      "step 1/3 | epoch 47/50 | batch 22/60 | global_step 2782 | loss_total 0.2343\n",
      "step 1/3 | epoch 47/50 | batch 23/60 | global_step 2783 | loss_total 0.2199\n",
      "step 1/3 | epoch 47/50 | batch 24/60 | global_step 2784 | loss_total 0.2211\n",
      "step 1/3 | epoch 47/50 | batch 25/60 | global_step 2785 | loss_total 0.2430\n",
      "step 1/3 | epoch 47/50 | batch 26/60 | global_step 2786 | loss_total 0.2742\n",
      "step 1/3 | epoch 47/50 | batch 27/60 | global_step 2787 | loss_total 0.3336\n",
      "step 1/3 | epoch 47/50 | batch 28/60 | global_step 2788 | loss_total 0.2219\n",
      "step 1/3 | epoch 47/50 | batch 29/60 | global_step 2789 | loss_total 0.2174\n",
      "step 1/3 | epoch 47/50 | batch 30/60 | global_step 2790 | loss_total 0.2186\n",
      "step 1/3 | epoch 47/50 | batch 31/60 | global_step 2791 | loss_total 1.1157\n",
      "step 1/3 | epoch 47/50 | batch 32/60 | global_step 2792 | loss_total 0.2209\n",
      "step 1/3 | epoch 47/50 | batch 33/60 | global_step 2793 | loss_total 0.2230\n",
      "step 1/3 | epoch 47/50 | batch 34/60 | global_step 2794 | loss_total 0.2191\n",
      "step 1/3 | epoch 47/50 | batch 35/60 | global_step 2795 | loss_total 0.2232\n",
      "step 1/3 | epoch 47/50 | batch 36/60 | global_step 2796 | loss_total 0.4268\n",
      "step 1/3 | epoch 47/50 | batch 37/60 | global_step 2797 | loss_total 0.2287\n",
      "step 1/3 | epoch 47/50 | batch 38/60 | global_step 2798 | loss_total 0.2258\n",
      "step 1/3 | epoch 47/50 | batch 39/60 | global_step 2799 | loss_total 0.2350\n",
      "step 1/3 | epoch 47/50 | batch 40/60 | global_step 2800 | loss_total 0.2338\n",
      "step 1/3 | epoch 47/50 | batch 41/60 | global_step 2801 | loss_total 0.7848\n",
      "step 1/3 | epoch 47/50 | batch 42/60 | global_step 2802 | loss_total 0.2176\n",
      "step 1/3 | epoch 47/50 | batch 43/60 | global_step 2803 | loss_total 0.2604\n",
      "step 1/3 | epoch 47/50 | batch 44/60 | global_step 2804 | loss_total 0.3040\n",
      "step 1/3 | epoch 47/50 | batch 45/60 | global_step 2805 | loss_total 0.2190\n",
      "step 1/3 | epoch 47/50 | batch 46/60 | global_step 2806 | loss_total 0.2708\n",
      "step 1/3 | epoch 47/50 | batch 47/60 | global_step 2807 | loss_total 0.2229\n",
      "step 1/3 | epoch 47/50 | batch 48/60 | global_step 2808 | loss_total 1.1199\n",
      "step 1/3 | epoch 47/50 | batch 49/60 | global_step 2809 | loss_total 0.2741\n",
      "step 1/3 | epoch 47/50 | batch 50/60 | global_step 2810 | loss_total 0.2630\n",
      "step 1/3 | epoch 47/50 | batch 51/60 | global_step 2811 | loss_total 0.2317\n",
      "step 1/3 | epoch 47/50 | batch 52/60 | global_step 2812 | loss_total 0.2217\n",
      "step 1/3 | epoch 47/50 | batch 53/60 | global_step 2813 | loss_total 0.2225\n",
      "step 1/3 | epoch 47/50 | batch 54/60 | global_step 2814 | loss_total 0.3682\n",
      "step 1/3 | epoch 47/50 | batch 55/60 | global_step 2815 | loss_total 0.2194\n",
      "step 1/3 | epoch 47/50 | batch 56/60 | global_step 2816 | loss_total 0.8445\n",
      "step 1/3 | epoch 47/50 | batch 57/60 | global_step 2817 | loss_total 0.8023\n",
      "step 1/3 | epoch 47/50 | batch 58/60 | global_step 2818 | loss_total 0.2094\n",
      "step 1/3 | epoch 47/50 | batch 59/60 | global_step 2819 | loss_total 0.7218\n",
      "step 1/3 | epoch 47/50 | batch 60/60 | global_step 2820 | loss_total 0.6840\n",
      "[epoch done] step 1/3 epoch 47/50 | train_total=0.3595 val_total=0.2266\n",
      "step 1/3 | epoch 48/50 | batch 1/60 | global_step 2821 | loss_total 0.8712\n",
      "step 1/3 | epoch 48/50 | batch 2/60 | global_step 2822 | loss_total 0.2097\n",
      "step 1/3 | epoch 48/50 | batch 3/60 | global_step 2823 | loss_total 0.2168\n",
      "step 1/3 | epoch 48/50 | batch 4/60 | global_step 2824 | loss_total 0.2033\n",
      "step 1/3 | epoch 48/50 | batch 5/60 | global_step 2825 | loss_total 0.2039\n",
      "step 1/3 | epoch 48/50 | batch 6/60 | global_step 2826 | loss_total 0.2530\n",
      "step 1/3 | epoch 48/50 | batch 7/60 | global_step 2827 | loss_total 0.2175\n",
      "step 1/3 | epoch 48/50 | batch 8/60 | global_step 2828 | loss_total 0.2138\n",
      "step 1/3 | epoch 48/50 | batch 9/60 | global_step 2829 | loss_total 0.2382\n",
      "step 1/3 | epoch 48/50 | batch 10/60 | global_step 2830 | loss_total 0.2159\n",
      "step 1/3 | epoch 48/50 | batch 11/60 | global_step 2831 | loss_total 0.2848\n",
      "step 1/3 | epoch 48/50 | batch 12/60 | global_step 2832 | loss_total 0.2178\n",
      "step 1/3 | epoch 48/50 | batch 13/60 | global_step 2833 | loss_total 0.2375\n",
      "step 1/3 | epoch 48/50 | batch 14/60 | global_step 2834 | loss_total 0.2220\n",
      "step 1/3 | epoch 48/50 | batch 15/60 | global_step 2835 | loss_total 0.2635\n",
      "step 1/3 | epoch 48/50 | batch 16/60 | global_step 2836 | loss_total 0.2169\n",
      "step 1/3 | epoch 48/50 | batch 17/60 | global_step 2837 | loss_total 0.2691\n",
      "step 1/3 | epoch 48/50 | batch 18/60 | global_step 2838 | loss_total 0.2124\n",
      "step 1/3 | epoch 48/50 | batch 19/60 | global_step 2839 | loss_total 1.4686\n",
      "step 1/3 | epoch 48/50 | batch 20/60 | global_step 2840 | loss_total 0.1999\n",
      "step 1/3 | epoch 48/50 | batch 21/60 | global_step 2841 | loss_total 0.2228\n",
      "step 1/3 | epoch 48/50 | batch 22/60 | global_step 2842 | loss_total 0.7770\n",
      "step 1/3 | epoch 48/50 | batch 23/60 | global_step 2843 | loss_total 0.5115\n",
      "step 1/3 | epoch 48/50 | batch 24/60 | global_step 2844 | loss_total 0.2170\n",
      "step 1/3 | epoch 48/50 | batch 25/60 | global_step 2845 | loss_total 0.2343\n",
      "step 1/3 | epoch 48/50 | batch 26/60 | global_step 2846 | loss_total 0.2091\n",
      "step 1/3 | epoch 48/50 | batch 27/60 | global_step 2847 | loss_total 0.2287\n",
      "step 1/3 | epoch 48/50 | batch 28/60 | global_step 2848 | loss_total 0.4166\n",
      "step 1/3 | epoch 48/50 | batch 29/60 | global_step 2849 | loss_total 0.2134\n",
      "step 1/3 | epoch 48/50 | batch 30/60 | global_step 2850 | loss_total 0.2191\n",
      "step 1/3 | epoch 48/50 | batch 31/60 | global_step 2851 | loss_total 0.2263\n",
      "step 1/3 | epoch 48/50 | batch 32/60 | global_step 2852 | loss_total 0.2436\n",
      "step 1/3 | epoch 48/50 | batch 33/60 | global_step 2853 | loss_total 0.2181\n",
      "step 1/3 | epoch 48/50 | batch 34/60 | global_step 2854 | loss_total 0.2218\n",
      "step 1/3 | epoch 48/50 | batch 35/60 | global_step 2855 | loss_total 0.2156\n",
      "step 1/3 | epoch 48/50 | batch 36/60 | global_step 2856 | loss_total 0.2100\n",
      "step 1/3 | epoch 48/50 | batch 37/60 | global_step 2857 | loss_total 0.2262\n",
      "step 1/3 | epoch 48/50 | batch 38/60 | global_step 2858 | loss_total 0.9805\n",
      "step 1/3 | epoch 48/50 | batch 39/60 | global_step 2859 | loss_total 0.2102\n",
      "step 1/3 | epoch 48/50 | batch 40/60 | global_step 2860 | loss_total 0.2098\n",
      "step 1/3 | epoch 48/50 | batch 41/60 | global_step 2861 | loss_total 0.2085\n",
      "step 1/3 | epoch 48/50 | batch 42/60 | global_step 2862 | loss_total 1.2255\n",
      "step 1/3 | epoch 48/50 | batch 43/60 | global_step 2863 | loss_total 0.2087\n",
      "step 1/3 | epoch 48/50 | batch 44/60 | global_step 2864 | loss_total 0.2114\n",
      "step 1/3 | epoch 48/50 | batch 45/60 | global_step 2865 | loss_total 0.2085\n",
      "step 1/3 | epoch 48/50 | batch 46/60 | global_step 2866 | loss_total 0.2108\n",
      "step 1/3 | epoch 48/50 | batch 47/60 | global_step 2867 | loss_total 0.4158\n",
      "step 1/3 | epoch 48/50 | batch 48/60 | global_step 2868 | loss_total 0.2158\n",
      "step 1/3 | epoch 48/50 | batch 49/60 | global_step 2869 | loss_total 0.2093\n",
      "step 1/3 | epoch 48/50 | batch 50/60 | global_step 2870 | loss_total 0.3466\n",
      "step 1/3 | epoch 48/50 | batch 51/60 | global_step 2871 | loss_total 1.2164\n",
      "step 1/3 | epoch 48/50 | batch 52/60 | global_step 2872 | loss_total 0.2275\n",
      "step 1/3 | epoch 48/50 | batch 53/60 | global_step 2873 | loss_total 0.9527\n",
      "step 1/3 | epoch 48/50 | batch 54/60 | global_step 2874 | loss_total 0.2084\n",
      "step 1/3 | epoch 48/50 | batch 55/60 | global_step 2875 | loss_total 0.2175\n",
      "step 1/3 | epoch 48/50 | batch 56/60 | global_step 2876 | loss_total 0.7779\n",
      "step 1/3 | epoch 48/50 | batch 57/60 | global_step 2877 | loss_total 0.2095\n",
      "step 1/3 | epoch 48/50 | batch 58/60 | global_step 2878 | loss_total 0.2135\n",
      "step 1/3 | epoch 48/50 | batch 59/60 | global_step 2879 | loss_total 0.1992\n",
      "step 1/3 | epoch 48/50 | batch 60/60 | global_step 2880 | loss_total 0.8546\n",
      "[epoch done] step 1/3 epoch 48/50 | train_total=0.3531 val_total=0.1905\n",
      "step 1/3 | epoch 49/50 | batch 1/60 | global_step 2881 | loss_total 0.2532\n",
      "step 1/3 | epoch 49/50 | batch 2/60 | global_step 2882 | loss_total 1.2280\n",
      "step 1/3 | epoch 49/50 | batch 3/60 | global_step 2883 | loss_total 0.4373\n",
      "step 1/3 | epoch 49/50 | batch 4/60 | global_step 2884 | loss_total 0.5881\n",
      "step 1/3 | epoch 49/50 | batch 5/60 | global_step 2885 | loss_total 0.4487\n",
      "step 1/3 | epoch 49/50 | batch 6/60 | global_step 2886 | loss_total 0.2700\n",
      "step 1/3 | epoch 49/50 | batch 7/60 | global_step 2887 | loss_total 0.3505\n",
      "step 1/3 | epoch 49/50 | batch 8/60 | global_step 2888 | loss_total 0.2542\n",
      "step 1/3 | epoch 49/50 | batch 9/60 | global_step 2889 | loss_total 0.2147\n",
      "step 1/3 | epoch 49/50 | batch 10/60 | global_step 2890 | loss_total 0.2706\n",
      "step 1/3 | epoch 49/50 | batch 11/60 | global_step 2891 | loss_total 0.2474\n",
      "step 1/3 | epoch 49/50 | batch 12/60 | global_step 2892 | loss_total 0.3027\n",
      "step 1/3 | epoch 49/50 | batch 13/60 | global_step 2893 | loss_total 0.2182\n",
      "step 1/3 | epoch 49/50 | batch 14/60 | global_step 2894 | loss_total 0.4699\n",
      "step 1/3 | epoch 49/50 | batch 15/60 | global_step 2895 | loss_total 0.4782\n",
      "step 1/3 | epoch 49/50 | batch 16/60 | global_step 2896 | loss_total 0.3601\n",
      "step 1/3 | epoch 49/50 | batch 17/60 | global_step 2897 | loss_total 0.2279\n",
      "step 1/3 | epoch 49/50 | batch 18/60 | global_step 2898 | loss_total 0.2379\n",
      "step 1/3 | epoch 49/50 | batch 19/60 | global_step 2899 | loss_total 1.0180\n",
      "step 1/3 | epoch 49/50 | batch 20/60 | global_step 2900 | loss_total 0.3322\n",
      "step 1/3 | epoch 49/50 | batch 21/60 | global_step 2901 | loss_total 0.2181\n",
      "step 1/3 | epoch 49/50 | batch 22/60 | global_step 2902 | loss_total 0.2185\n",
      "step 1/3 | epoch 49/50 | batch 23/60 | global_step 2903 | loss_total 0.9273\n",
      "step 1/3 | epoch 49/50 | batch 24/60 | global_step 2904 | loss_total 0.3777\n",
      "step 1/3 | epoch 49/50 | batch 25/60 | global_step 2905 | loss_total 0.2168\n",
      "step 1/3 | epoch 49/50 | batch 26/60 | global_step 2906 | loss_total 0.2268\n",
      "step 1/3 | epoch 49/50 | batch 27/60 | global_step 2907 | loss_total 0.2557\n",
      "step 1/3 | epoch 49/50 | batch 28/60 | global_step 2908 | loss_total 0.6781\n",
      "step 1/3 | epoch 49/50 | batch 29/60 | global_step 2909 | loss_total 0.6030\n",
      "step 1/3 | epoch 49/50 | batch 30/60 | global_step 2910 | loss_total 0.4167\n",
      "step 1/3 | epoch 49/50 | batch 31/60 | global_step 2911 | loss_total 0.2573\n",
      "step 1/3 | epoch 49/50 | batch 32/60 | global_step 2912 | loss_total 0.2719\n",
      "step 1/3 | epoch 49/50 | batch 33/60 | global_step 2913 | loss_total 0.3333\n",
      "step 1/3 | epoch 49/50 | batch 34/60 | global_step 2914 | loss_total 0.2224\n",
      "step 1/3 | epoch 49/50 | batch 35/60 | global_step 2915 | loss_total 0.2586\n",
      "step 1/3 | epoch 49/50 | batch 36/60 | global_step 2916 | loss_total 0.2267\n",
      "step 1/3 | epoch 49/50 | batch 37/60 | global_step 2917 | loss_total 0.2210\n",
      "step 1/3 | epoch 49/50 | batch 38/60 | global_step 2918 | loss_total 0.2221\n",
      "step 1/3 | epoch 49/50 | batch 39/60 | global_step 2919 | loss_total 0.8537\n",
      "step 1/3 | epoch 49/50 | batch 40/60 | global_step 2920 | loss_total 0.2164\n",
      "step 1/3 | epoch 49/50 | batch 41/60 | global_step 2921 | loss_total 0.2104\n",
      "step 1/3 | epoch 49/50 | batch 42/60 | global_step 2922 | loss_total 0.2198\n",
      "step 1/3 | epoch 49/50 | batch 43/60 | global_step 2923 | loss_total 0.2642\n",
      "step 1/3 | epoch 49/50 | batch 44/60 | global_step 2924 | loss_total 0.3520\n",
      "step 1/3 | epoch 49/50 | batch 45/60 | global_step 2925 | loss_total 0.8901\n",
      "step 1/3 | epoch 49/50 | batch 46/60 | global_step 2926 | loss_total 0.5398\n",
      "step 1/3 | epoch 49/50 | batch 47/60 | global_step 2927 | loss_total 0.2299\n",
      "step 1/3 | epoch 49/50 | batch 48/60 | global_step 2928 | loss_total 0.2429\n",
      "step 1/3 | epoch 49/50 | batch 49/60 | global_step 2929 | loss_total 0.2064\n",
      "step 1/3 | epoch 49/50 | batch 50/60 | global_step 2930 | loss_total 0.2636\n",
      "step 1/3 | epoch 49/50 | batch 51/60 | global_step 2931 | loss_total 1.4140\n",
      "step 1/3 | epoch 49/50 | batch 52/60 | global_step 2932 | loss_total 0.2103\n",
      "step 1/3 | epoch 49/50 | batch 53/60 | global_step 2933 | loss_total 0.2043\n",
      "step 1/3 | epoch 49/50 | batch 54/60 | global_step 2934 | loss_total 0.2203\n",
      "step 1/3 | epoch 49/50 | batch 55/60 | global_step 2935 | loss_total 0.2878\n",
      "step 1/3 | epoch 49/50 | batch 56/60 | global_step 2936 | loss_total 0.2036\n",
      "step 1/3 | epoch 49/50 | batch 57/60 | global_step 2937 | loss_total 0.2193\n",
      "step 1/3 | epoch 49/50 | batch 58/60 | global_step 2938 | loss_total 0.2039\n",
      "step 1/3 | epoch 49/50 | batch 59/60 | global_step 2939 | loss_total 0.2027\n",
      "step 1/3 | epoch 49/50 | batch 60/60 | global_step 2940 | loss_total 0.2055\n",
      "[epoch done] step 1/3 epoch 49/50 | train_total=0.3703 val_total=0.2585\n",
      "step 1/3 | epoch 50/50 | batch 1/60 | global_step 2941 | loss_total 0.2473\n",
      "step 1/3 | epoch 50/50 | batch 2/60 | global_step 2942 | loss_total 0.2504\n",
      "step 1/3 | epoch 50/50 | batch 3/60 | global_step 2943 | loss_total 0.1983\n",
      "step 1/3 | epoch 50/50 | batch 4/60 | global_step 2944 | loss_total 0.6115\n",
      "step 1/3 | epoch 50/50 | batch 5/60 | global_step 2945 | loss_total 0.2666\n",
      "step 1/3 | epoch 50/50 | batch 6/60 | global_step 2946 | loss_total 0.2632\n",
      "step 1/3 | epoch 50/50 | batch 7/60 | global_step 2947 | loss_total 0.2672\n",
      "step 1/3 | epoch 50/50 | batch 8/60 | global_step 2948 | loss_total 0.2572\n",
      "step 1/3 | epoch 50/50 | batch 9/60 | global_step 2949 | loss_total 0.2781\n",
      "step 1/3 | epoch 50/50 | batch 10/60 | global_step 2950 | loss_total 0.2477\n",
      "step 1/3 | epoch 50/50 | batch 11/60 | global_step 2951 | loss_total 0.2458\n",
      "step 1/3 | epoch 50/50 | batch 12/60 | global_step 2952 | loss_total 0.2207\n",
      "step 1/3 | epoch 50/50 | batch 13/60 | global_step 2953 | loss_total 0.2207\n",
      "step 1/3 | epoch 50/50 | batch 14/60 | global_step 2954 | loss_total 1.2773\n",
      "step 1/3 | epoch 50/50 | batch 15/60 | global_step 2955 | loss_total 0.3137\n",
      "step 1/3 | epoch 50/50 | batch 16/60 | global_step 2956 | loss_total 0.2151\n",
      "step 1/3 | epoch 50/50 | batch 17/60 | global_step 2957 | loss_total 0.3680\n",
      "step 1/3 | epoch 50/50 | batch 18/60 | global_step 2958 | loss_total 0.2230\n",
      "step 1/3 | epoch 50/50 | batch 19/60 | global_step 2959 | loss_total 0.6120\n",
      "step 1/3 | epoch 50/50 | batch 20/60 | global_step 2960 | loss_total 0.2156\n",
      "step 1/3 | epoch 50/50 | batch 21/60 | global_step 2961 | loss_total 0.2167\n",
      "step 1/3 | epoch 50/50 | batch 22/60 | global_step 2962 | loss_total 0.7553\n",
      "step 1/3 | epoch 50/50 | batch 23/60 | global_step 2963 | loss_total 0.2140\n",
      "step 1/3 | epoch 50/50 | batch 24/60 | global_step 2964 | loss_total 0.5771\n",
      "step 1/3 | epoch 50/50 | batch 25/60 | global_step 2965 | loss_total 0.2143\n",
      "step 1/3 | epoch 50/50 | batch 26/60 | global_step 2966 | loss_total 0.2101\n",
      "step 1/3 | epoch 50/50 | batch 27/60 | global_step 2967 | loss_total 0.2151\n",
      "step 1/3 | epoch 50/50 | batch 28/60 | global_step 2968 | loss_total 0.2811\n",
      "step 1/3 | epoch 50/50 | batch 29/60 | global_step 2969 | loss_total 0.3618\n",
      "step 1/3 | epoch 50/50 | batch 30/60 | global_step 2970 | loss_total 0.2445\n",
      "step 1/3 | epoch 50/50 | batch 31/60 | global_step 2971 | loss_total 0.2538\n",
      "step 1/3 | epoch 50/50 | batch 32/60 | global_step 2972 | loss_total 0.3483\n",
      "step 1/3 | epoch 50/50 | batch 33/60 | global_step 2973 | loss_total 0.3423\n",
      "step 1/3 | epoch 50/50 | batch 34/60 | global_step 2974 | loss_total 0.6660\n",
      "step 1/3 | epoch 50/50 | batch 35/60 | global_step 2975 | loss_total 0.2159\n",
      "step 1/3 | epoch 50/50 | batch 36/60 | global_step 2976 | loss_total 0.2293\n",
      "step 1/3 | epoch 50/50 | batch 37/60 | global_step 2977 | loss_total 0.2154\n",
      "step 1/3 | epoch 50/50 | batch 38/60 | global_step 2978 | loss_total 1.3120\n",
      "step 1/3 | epoch 50/50 | batch 39/60 | global_step 2979 | loss_total 0.2152\n",
      "step 1/3 | epoch 50/50 | batch 40/60 | global_step 2980 | loss_total 0.2130\n",
      "step 1/3 | epoch 50/50 | batch 41/60 | global_step 2981 | loss_total 0.3250\n",
      "step 1/3 | epoch 50/50 | batch 42/60 | global_step 2982 | loss_total 0.4387\n",
      "step 1/3 | epoch 50/50 | batch 43/60 | global_step 2983 | loss_total 0.2085\n",
      "step 1/3 | epoch 50/50 | batch 44/60 | global_step 2984 | loss_total 0.2548\n",
      "step 1/3 | epoch 50/50 | batch 45/60 | global_step 2985 | loss_total 0.2015\n",
      "step 1/3 | epoch 50/50 | batch 46/60 | global_step 2986 | loss_total 0.2112\n",
      "step 1/3 | epoch 50/50 | batch 47/60 | global_step 2987 | loss_total 0.3068\n",
      "step 1/3 | epoch 50/50 | batch 48/60 | global_step 2988 | loss_total 0.2094\n",
      "step 1/3 | epoch 50/50 | batch 49/60 | global_step 2989 | loss_total 0.2116\n",
      "step 1/3 | epoch 50/50 | batch 50/60 | global_step 2990 | loss_total 0.1990\n",
      "step 1/3 | epoch 50/50 | batch 51/60 | global_step 2991 | loss_total 0.3489\n",
      "step 1/3 | epoch 50/50 | batch 52/60 | global_step 2992 | loss_total 0.1993\n",
      "step 1/3 | epoch 50/50 | batch 53/60 | global_step 2993 | loss_total 0.1991\n",
      "step 1/3 | epoch 50/50 | batch 54/60 | global_step 2994 | loss_total 0.2082\n",
      "step 1/3 | epoch 50/50 | batch 55/60 | global_step 2995 | loss_total 0.2760\n",
      "step 1/3 | epoch 50/50 | batch 56/60 | global_step 2996 | loss_total 0.1996\n",
      "step 1/3 | epoch 50/50 | batch 57/60 | global_step 2997 | loss_total 0.1995\n",
      "step 1/3 | epoch 50/50 | batch 58/60 | global_step 2998 | loss_total 0.2088\n",
      "step 1/3 | epoch 50/50 | batch 59/60 | global_step 2999 | loss_total 0.2050\n",
      "step 1/3 | epoch 50/50 | batch 60/60 | global_step 3000 | loss_total 0.2076\n",
      "[epoch done] step 1/3 epoch 50/50 | train_total=0.3153 val_total=0.1694\n",
      "\n",
      "[Phase 2] files=14,135 train_chunks=1,473 val_chunks=269 epochs=50\n",
      "trainable params tensors: 26/26\n",
      "step 2/3 | epoch 1/50 | batch 1/60 | global_step 3001 | loss_total 14.5398\n",
      "step 2/3 | epoch 1/50 | batch 2/60 | global_step 3002 | loss_total 14.2653\n",
      "step 2/3 | epoch 1/50 | batch 3/60 | global_step 3003 | loss_total 13.7410\n",
      "step 2/3 | epoch 1/50 | batch 4/60 | global_step 3004 | loss_total 13.1446\n",
      "step 2/3 | epoch 1/50 | batch 5/60 | global_step 3005 | loss_total 12.8649\n",
      "step 2/3 | epoch 1/50 | batch 6/60 | global_step 3006 | loss_total 12.1516\n",
      "step 2/3 | epoch 1/50 | batch 7/60 | global_step 3007 | loss_total 11.1102\n",
      "step 2/3 | epoch 1/50 | batch 8/60 | global_step 3008 | loss_total 10.7163\n",
      "step 2/3 | epoch 1/50 | batch 9/60 | global_step 3009 | loss_total 8.2920\n",
      "step 2/3 | epoch 1/50 | batch 10/60 | global_step 3010 | loss_total 8.5867\n",
      "step 2/3 | epoch 1/50 | batch 11/60 | global_step 3011 | loss_total 7.7228\n",
      "step 2/3 | epoch 1/50 | batch 12/60 | global_step 3012 | loss_total 7.1313\n",
      "step 2/3 | epoch 1/50 | batch 13/60 | global_step 3013 | loss_total 6.6095\n",
      "step 2/3 | epoch 1/50 | batch 14/60 | global_step 3014 | loss_total 6.0765\n",
      "step 2/3 | epoch 1/50 | batch 15/60 | global_step 3015 | loss_total 6.2672\n",
      "step 2/3 | epoch 1/50 | batch 16/60 | global_step 3016 | loss_total 5.4840\n",
      "step 2/3 | epoch 1/50 | batch 17/60 | global_step 3017 | loss_total 6.0841\n",
      "step 2/3 | epoch 1/50 | batch 18/60 | global_step 3018 | loss_total 5.2251\n",
      "step 2/3 | epoch 1/50 | batch 19/60 | global_step 3019 | loss_total 5.0025\n",
      "step 2/3 | epoch 1/50 | batch 20/60 | global_step 3020 | loss_total 5.2268\n",
      "step 2/3 | epoch 1/50 | batch 21/60 | global_step 3021 | loss_total 5.6003\n",
      "step 2/3 | epoch 1/50 | batch 22/60 | global_step 3022 | loss_total 5.4121\n",
      "step 2/3 | epoch 1/50 | batch 23/60 | global_step 3023 | loss_total 6.2598\n",
      "step 2/3 | epoch 1/50 | batch 24/60 | global_step 3024 | loss_total 5.6724\n",
      "step 2/3 | epoch 1/50 | batch 25/60 | global_step 3025 | loss_total 6.5748\n",
      "step 2/3 | epoch 1/50 | batch 26/60 | global_step 3026 | loss_total 6.1583\n",
      "step 2/3 | epoch 1/50 | batch 27/60 | global_step 3027 | loss_total 6.0104\n",
      "step 2/3 | epoch 1/50 | batch 28/60 | global_step 3028 | loss_total 6.7242\n",
      "step 2/3 | epoch 1/50 | batch 29/60 | global_step 3029 | loss_total 6.3975\n",
      "step 2/3 | epoch 1/50 | batch 30/60 | global_step 3030 | loss_total 7.0758\n",
      "step 2/3 | epoch 1/50 | batch 31/60 | global_step 3031 | loss_total 7.3847\n",
      "step 2/3 | epoch 1/50 | batch 32/60 | global_step 3032 | loss_total 7.8685\n",
      "step 2/3 | epoch 1/50 | batch 33/60 | global_step 3033 | loss_total 7.6539\n",
      "step 2/3 | epoch 1/50 | batch 34/60 | global_step 3034 | loss_total 7.0716\n",
      "step 2/3 | epoch 1/50 | batch 35/60 | global_step 3035 | loss_total 7.6692\n",
      "step 2/3 | epoch 1/50 | batch 36/60 | global_step 3036 | loss_total 7.4755\n",
      "step 2/3 | epoch 1/50 | batch 37/60 | global_step 3037 | loss_total 7.5953\n",
      "step 2/3 | epoch 1/50 | batch 38/60 | global_step 3038 | loss_total 8.0002\n",
      "step 2/3 | epoch 1/50 | batch 39/60 | global_step 3039 | loss_total 7.7813\n",
      "step 2/3 | epoch 1/50 | batch 40/60 | global_step 3040 | loss_total 7.6905\n",
      "step 2/3 | epoch 1/50 | batch 41/60 | global_step 3041 | loss_total 8.1495\n",
      "step 2/3 | epoch 1/50 | batch 42/60 | global_step 3042 | loss_total 8.0814\n",
      "step 2/3 | epoch 1/50 | batch 43/60 | global_step 3043 | loss_total 8.0516\n",
      "step 2/3 | epoch 1/50 | batch 44/60 | global_step 3044 | loss_total 8.0125\n",
      "step 2/3 | epoch 1/50 | batch 45/60 | global_step 3045 | loss_total 7.7079\n",
      "step 2/3 | epoch 1/50 | batch 46/60 | global_step 3046 | loss_total 6.8058\n",
      "step 2/3 | epoch 1/50 | batch 47/60 | global_step 3047 | loss_total 7.4745\n",
      "step 2/3 | epoch 1/50 | batch 48/60 | global_step 3048 | loss_total 6.3738\n",
      "step 2/3 | epoch 1/50 | batch 49/60 | global_step 3049 | loss_total 7.1631\n",
      "step 2/3 | epoch 1/50 | batch 50/60 | global_step 3050 | loss_total 6.1423\n",
      "step 2/3 | epoch 1/50 | batch 51/60 | global_step 3051 | loss_total 6.1039\n",
      "step 2/3 | epoch 1/50 | batch 52/60 | global_step 3052 | loss_total 6.5066\n",
      "step 2/3 | epoch 1/50 | batch 53/60 | global_step 3053 | loss_total 5.6549\n",
      "step 2/3 | epoch 1/50 | batch 54/60 | global_step 3054 | loss_total 5.4263\n",
      "step 2/3 | epoch 1/50 | batch 55/60 | global_step 3055 | loss_total 6.1629\n",
      "step 2/3 | epoch 1/50 | batch 56/60 | global_step 3056 | loss_total 5.2674\n",
      "step 2/3 | epoch 1/50 | batch 57/60 | global_step 3057 | loss_total 5.2901\n",
      "step 2/3 | epoch 1/50 | batch 58/60 | global_step 3058 | loss_total 5.6411\n",
      "step 2/3 | epoch 1/50 | batch 59/60 | global_step 3059 | loss_total 4.6995\n",
      "step 2/3 | epoch 1/50 | batch 60/60 | global_step 3060 | loss_total 5.5670\n",
      "[epoch done] step 2/3 epoch 1/50 | train_total=7.4767 val_total=5.1818\n",
      "step 2/3 | epoch 2/50 | batch 1/60 | global_step 3061 | loss_total 5.6579\n",
      "step 2/3 | epoch 2/50 | batch 2/60 | global_step 3062 | loss_total 6.3329\n",
      "step 2/3 | epoch 2/50 | batch 3/60 | global_step 3063 | loss_total 4.1950\n",
      "step 2/3 | epoch 2/50 | batch 4/60 | global_step 3064 | loss_total 4.6543\n",
      "step 2/3 | epoch 2/50 | batch 5/60 | global_step 3065 | loss_total 5.1466\n",
      "step 2/3 | epoch 2/50 | batch 6/60 | global_step 3066 | loss_total 4.0291\n",
      "step 2/3 | epoch 2/50 | batch 7/60 | global_step 3067 | loss_total 3.8933\n",
      "step 2/3 | epoch 2/50 | batch 8/60 | global_step 3068 | loss_total 4.7007\n",
      "step 2/3 | epoch 2/50 | batch 9/60 | global_step 3069 | loss_total 3.4981\n",
      "step 2/3 | epoch 2/50 | batch 10/60 | global_step 3070 | loss_total 4.3692\n",
      "step 2/3 | epoch 2/50 | batch 11/60 | global_step 3071 | loss_total 3.3236\n",
      "step 2/3 | epoch 2/50 | batch 12/60 | global_step 3072 | loss_total 3.2627\n",
      "step 2/3 | epoch 2/50 | batch 13/60 | global_step 3073 | loss_total 2.9556\n",
      "step 2/3 | epoch 2/50 | batch 14/60 | global_step 3074 | loss_total 2.8205\n",
      "step 2/3 | epoch 2/50 | batch 15/60 | global_step 3075 | loss_total 2.7083\n",
      "step 2/3 | epoch 2/50 | batch 16/60 | global_step 3076 | loss_total 3.6425\n",
      "step 2/3 | epoch 2/50 | batch 17/60 | global_step 3077 | loss_total 3.3711\n",
      "step 2/3 | epoch 2/50 | batch 18/60 | global_step 3078 | loss_total 2.2752\n",
      "step 2/3 | epoch 2/50 | batch 19/60 | global_step 3079 | loss_total 3.1880\n",
      "step 2/3 | epoch 2/50 | batch 20/60 | global_step 3080 | loss_total 1.8652\n",
      "step 2/3 | epoch 2/50 | batch 21/60 | global_step 3081 | loss_total 1.8259\n",
      "step 2/3 | epoch 2/50 | batch 22/60 | global_step 3082 | loss_total 1.7032\n",
      "step 2/3 | epoch 2/50 | batch 23/60 | global_step 3083 | loss_total 3.7617\n",
      "step 2/3 | epoch 2/50 | batch 24/60 | global_step 3084 | loss_total 1.9085\n",
      "step 2/3 | epoch 2/50 | batch 25/60 | global_step 3085 | loss_total 1.6535\n",
      "step 2/3 | epoch 2/50 | batch 26/60 | global_step 3086 | loss_total 3.3693\n",
      "step 2/3 | epoch 2/50 | batch 27/60 | global_step 3087 | loss_total 2.4152\n",
      "step 2/3 | epoch 2/50 | batch 28/60 | global_step 3088 | loss_total 4.4171\n",
      "step 2/3 | epoch 2/50 | batch 29/60 | global_step 3089 | loss_total 1.9083\n",
      "step 2/3 | epoch 2/50 | batch 30/60 | global_step 3090 | loss_total 1.4912\n",
      "step 2/3 | epoch 2/50 | batch 31/60 | global_step 3091 | loss_total 1.6033\n",
      "step 2/3 | epoch 2/50 | batch 32/60 | global_step 3092 | loss_total 1.5196\n",
      "step 2/3 | epoch 2/50 | batch 33/60 | global_step 3093 | loss_total 2.2330\n",
      "step 2/3 | epoch 2/50 | batch 34/60 | global_step 3094 | loss_total 1.3680\n",
      "step 2/3 | epoch 2/50 | batch 35/60 | global_step 3095 | loss_total 1.4385\n",
      "step 2/3 | epoch 2/50 | batch 36/60 | global_step 3096 | loss_total 1.1520\n",
      "step 2/3 | epoch 2/50 | batch 37/60 | global_step 3097 | loss_total 1.0709\n",
      "step 2/3 | epoch 2/50 | batch 38/60 | global_step 3098 | loss_total 1.1669\n",
      "step 2/3 | epoch 2/50 | batch 39/60 | global_step 3099 | loss_total 1.8725\n",
      "step 2/3 | epoch 2/50 | batch 40/60 | global_step 3100 | loss_total 1.3839\n",
      "step 2/3 | epoch 2/50 | batch 41/60 | global_step 3101 | loss_total 1.3714\n",
      "step 2/3 | epoch 2/50 | batch 42/60 | global_step 3102 | loss_total 1.8454\n",
      "step 2/3 | epoch 2/50 | batch 43/60 | global_step 3103 | loss_total 0.7921\n",
      "step 2/3 | epoch 2/50 | batch 44/60 | global_step 3104 | loss_total 1.3551\n",
      "step 2/3 | epoch 2/50 | batch 45/60 | global_step 3105 | loss_total 1.3541\n",
      "step 2/3 | epoch 2/50 | batch 46/60 | global_step 3106 | loss_total 2.1240\n",
      "step 2/3 | epoch 2/50 | batch 47/60 | global_step 3107 | loss_total 1.3259\n",
      "step 2/3 | epoch 2/50 | batch 48/60 | global_step 3108 | loss_total 1.0607\n",
      "step 2/3 | epoch 2/50 | batch 49/60 | global_step 3109 | loss_total 1.8840\n",
      "step 2/3 | epoch 2/50 | batch 50/60 | global_step 3110 | loss_total 2.1192\n",
      "step 2/3 | epoch 2/50 | batch 51/60 | global_step 3111 | loss_total 1.5483\n",
      "step 2/3 | epoch 2/50 | batch 52/60 | global_step 3112 | loss_total 2.5392\n",
      "step 2/3 | epoch 2/50 | batch 53/60 | global_step 3113 | loss_total 1.5035\n",
      "step 2/3 | epoch 2/50 | batch 54/60 | global_step 3114 | loss_total 3.1785\n",
      "step 2/3 | epoch 2/50 | batch 55/60 | global_step 3115 | loss_total 1.4115\n",
      "step 2/3 | epoch 2/50 | batch 56/60 | global_step 3116 | loss_total 2.0928\n",
      "step 2/3 | epoch 2/50 | batch 57/60 | global_step 3117 | loss_total 1.3245\n",
      "step 2/3 | epoch 2/50 | batch 58/60 | global_step 3118 | loss_total 2.0099\n",
      "step 2/3 | epoch 2/50 | batch 59/60 | global_step 3119 | loss_total 1.4237\n",
      "step 2/3 | epoch 2/50 | batch 60/60 | global_step 3120 | loss_total 1.2165\n",
      "[epoch done] step 2/3 epoch 2/50 | train_total=2.4772 val_total=1.5826\n",
      "step 2/3 | epoch 3/50 | batch 1/60 | global_step 3121 | loss_total 1.4502\n",
      "step 2/3 | epoch 3/50 | batch 2/60 | global_step 3122 | loss_total 1.3303\n",
      "step 2/3 | epoch 3/50 | batch 3/60 | global_step 3123 | loss_total 2.0529\n",
      "step 2/3 | epoch 3/50 | batch 4/60 | global_step 3124 | loss_total 1.7338\n",
      "step 2/3 | epoch 3/50 | batch 5/60 | global_step 3125 | loss_total 1.3384\n",
      "step 2/3 | epoch 3/50 | batch 6/60 | global_step 3126 | loss_total 1.0262\n",
      "step 2/3 | epoch 3/50 | batch 7/60 | global_step 3127 | loss_total 1.4986\n",
      "step 2/3 | epoch 3/50 | batch 8/60 | global_step 3128 | loss_total 1.3386\n",
      "step 2/3 | epoch 3/50 | batch 9/60 | global_step 3129 | loss_total 1.3265\n",
      "step 2/3 | epoch 3/50 | batch 10/60 | global_step 3130 | loss_total 1.7435\n",
      "step 2/3 | epoch 3/50 | batch 11/60 | global_step 3131 | loss_total 1.4417\n",
      "step 2/3 | epoch 3/50 | batch 12/60 | global_step 3132 | loss_total 1.1851\n",
      "step 2/3 | epoch 3/50 | batch 13/60 | global_step 3133 | loss_total 1.4199\n",
      "step 2/3 | epoch 3/50 | batch 14/60 | global_step 3134 | loss_total 2.5531\n",
      "step 2/3 | epoch 3/50 | batch 15/60 | global_step 3135 | loss_total 1.4041\n",
      "step 2/3 | epoch 3/50 | batch 16/60 | global_step 3136 | loss_total 2.1538\n",
      "step 2/3 | epoch 3/50 | batch 17/60 | global_step 3137 | loss_total 1.1867\n",
      "step 2/3 | epoch 3/50 | batch 18/60 | global_step 3138 | loss_total 2.6757\n",
      "step 2/3 | epoch 3/50 | batch 19/60 | global_step 3139 | loss_total 1.2096\n",
      "step 2/3 | epoch 3/50 | batch 20/60 | global_step 3140 | loss_total 1.8638\n",
      "step 2/3 | epoch 3/50 | batch 21/60 | global_step 3141 | loss_total 1.9973\n",
      "step 2/3 | epoch 3/50 | batch 22/60 | global_step 3142 | loss_total 1.2803\n",
      "step 2/3 | epoch 3/50 | batch 23/60 | global_step 3143 | loss_total 1.1912\n",
      "step 2/3 | epoch 3/50 | batch 24/60 | global_step 3144 | loss_total 1.1770\n",
      "step 2/3 | epoch 3/50 | batch 25/60 | global_step 3145 | loss_total 1.5596\n",
      "step 2/3 | epoch 3/50 | batch 26/60 | global_step 3146 | loss_total 1.4417\n",
      "step 2/3 | epoch 3/50 | batch 27/60 | global_step 3147 | loss_total 1.4151\n",
      "step 2/3 | epoch 3/50 | batch 28/60 | global_step 3148 | loss_total 1.2819\n",
      "step 2/3 | epoch 3/50 | batch 29/60 | global_step 3149 | loss_total 1.2702\n",
      "step 2/3 | epoch 3/50 | batch 30/60 | global_step 3150 | loss_total 2.0026\n",
      "step 2/3 | epoch 3/50 | batch 31/60 | global_step 3151 | loss_total 1.2580\n",
      "step 2/3 | epoch 3/50 | batch 32/60 | global_step 3152 | loss_total 1.2012\n",
      "step 2/3 | epoch 3/50 | batch 33/60 | global_step 3153 | loss_total 1.2320\n",
      "step 2/3 | epoch 3/50 | batch 34/60 | global_step 3154 | loss_total 1.9698\n",
      "step 2/3 | epoch 3/50 | batch 35/60 | global_step 3155 | loss_total 1.2155\n",
      "step 2/3 | epoch 3/50 | batch 36/60 | global_step 3156 | loss_total 1.2120\n",
      "step 2/3 | epoch 3/50 | batch 37/60 | global_step 3157 | loss_total 1.9816\n",
      "step 2/3 | epoch 3/50 | batch 38/60 | global_step 3158 | loss_total 1.2071\n",
      "step 2/3 | epoch 3/50 | batch 39/60 | global_step 3159 | loss_total 1.1972\n",
      "step 2/3 | epoch 3/50 | batch 40/60 | global_step 3160 | loss_total 1.2873\n",
      "step 2/3 | epoch 3/50 | batch 41/60 | global_step 3161 | loss_total 1.2128\n",
      "step 2/3 | epoch 3/50 | batch 42/60 | global_step 3162 | loss_total 1.2011\n",
      "step 2/3 | epoch 3/50 | batch 43/60 | global_step 3163 | loss_total 1.2027\n",
      "step 2/3 | epoch 3/50 | batch 44/60 | global_step 3164 | loss_total 1.2427\n",
      "step 2/3 | epoch 3/50 | batch 45/60 | global_step 3165 | loss_total 2.0764\n",
      "step 2/3 | epoch 3/50 | batch 46/60 | global_step 3166 | loss_total 1.1443\n",
      "step 2/3 | epoch 3/50 | batch 47/60 | global_step 3167 | loss_total 2.0764\n",
      "step 2/3 | epoch 3/50 | batch 48/60 | global_step 3168 | loss_total 1.1211\n",
      "step 2/3 | epoch 3/50 | batch 49/60 | global_step 3169 | loss_total 1.0948\n",
      "step 2/3 | epoch 3/50 | batch 50/60 | global_step 3170 | loss_total 1.0832\n",
      "step 2/3 | epoch 3/50 | batch 51/60 | global_step 3171 | loss_total 1.3257\n",
      "step 2/3 | epoch 3/50 | batch 52/60 | global_step 3172 | loss_total 1.9785\n",
      "step 2/3 | epoch 3/50 | batch 53/60 | global_step 3173 | loss_total 1.2806\n",
      "step 2/3 | epoch 3/50 | batch 54/60 | global_step 3174 | loss_total 1.1214\n",
      "step 2/3 | epoch 3/50 | batch 55/60 | global_step 3175 | loss_total 1.1702\n",
      "step 2/3 | epoch 3/50 | batch 56/60 | global_step 3176 | loss_total 1.1181\n",
      "step 2/3 | epoch 3/50 | batch 57/60 | global_step 3177 | loss_total 1.0728\n",
      "step 2/3 | epoch 3/50 | batch 58/60 | global_step 3178 | loss_total 1.0664\n",
      "step 2/3 | epoch 3/50 | batch 59/60 | global_step 3179 | loss_total 1.9318\n",
      "step 2/3 | epoch 3/50 | batch 60/60 | global_step 3180 | loss_total 1.3328\n",
      "[epoch done] step 2/3 epoch 3/50 | train_total=1.4527 val_total=1.0828\n",
      "step 2/3 | epoch 4/50 | batch 1/60 | global_step 3181 | loss_total 1.0737\n",
      "step 2/3 | epoch 4/50 | batch 2/60 | global_step 3182 | loss_total 2.0287\n",
      "step 2/3 | epoch 4/50 | batch 3/60 | global_step 3183 | loss_total 2.0238\n",
      "step 2/3 | epoch 4/50 | batch 4/60 | global_step 3184 | loss_total 1.0332\n",
      "step 2/3 | epoch 4/50 | batch 5/60 | global_step 3185 | loss_total 1.3967\n",
      "step 2/3 | epoch 4/50 | batch 6/60 | global_step 3186 | loss_total 1.0255\n",
      "step 2/3 | epoch 4/50 | batch 7/60 | global_step 3187 | loss_total 0.9622\n",
      "step 2/3 | epoch 4/50 | batch 8/60 | global_step 3188 | loss_total 1.0461\n",
      "step 2/3 | epoch 4/50 | batch 9/60 | global_step 3189 | loss_total 0.9250\n",
      "step 2/3 | epoch 4/50 | batch 10/60 | global_step 3190 | loss_total 0.9415\n",
      "step 2/3 | epoch 4/50 | batch 11/60 | global_step 3191 | loss_total 1.1517\n",
      "step 2/3 | epoch 4/50 | batch 12/60 | global_step 3192 | loss_total 1.2205\n",
      "step 2/3 | epoch 4/50 | batch 13/60 | global_step 3193 | loss_total 0.8831\n",
      "step 2/3 | epoch 4/50 | batch 14/60 | global_step 3194 | loss_total 1.8304\n",
      "step 2/3 | epoch 4/50 | batch 15/60 | global_step 3195 | loss_total 1.5803\n",
      "step 2/3 | epoch 4/50 | batch 16/60 | global_step 3196 | loss_total 2.1578\n",
      "step 2/3 | epoch 4/50 | batch 17/60 | global_step 3197 | loss_total 0.9400\n",
      "step 2/3 | epoch 4/50 | batch 18/60 | global_step 3198 | loss_total 0.9454\n",
      "step 2/3 | epoch 4/50 | batch 19/60 | global_step 3199 | loss_total 1.3284\n",
      "step 2/3 | epoch 4/50 | batch 20/60 | global_step 3200 | loss_total 1.8952\n",
      "step 2/3 | epoch 4/50 | batch 21/60 | global_step 3201 | loss_total 0.9646\n",
      "step 2/3 | epoch 4/50 | batch 22/60 | global_step 3202 | loss_total 2.8168\n",
      "step 2/3 | epoch 4/50 | batch 23/60 | global_step 3203 | loss_total 1.0731\n",
      "step 2/3 | epoch 4/50 | batch 24/60 | global_step 3204 | loss_total 1.1739\n",
      "step 2/3 | epoch 4/50 | batch 25/60 | global_step 3205 | loss_total 0.9446\n",
      "step 2/3 | epoch 4/50 | batch 26/60 | global_step 3206 | loss_total 1.9114\n",
      "step 2/3 | epoch 4/50 | batch 27/60 | global_step 3207 | loss_total 0.9058\n",
      "step 2/3 | epoch 4/50 | batch 28/60 | global_step 3208 | loss_total 1.2143\n",
      "step 2/3 | epoch 4/50 | batch 29/60 | global_step 3209 | loss_total 1.2093\n",
      "step 2/3 | epoch 4/50 | batch 30/60 | global_step 3210 | loss_total 0.9294\n",
      "step 2/3 | epoch 4/50 | batch 31/60 | global_step 3211 | loss_total 1.0702\n",
      "step 2/3 | epoch 4/50 | batch 32/60 | global_step 3212 | loss_total 1.8954\n",
      "step 2/3 | epoch 4/50 | batch 33/60 | global_step 3213 | loss_total 1.3805\n",
      "step 2/3 | epoch 4/50 | batch 34/60 | global_step 3214 | loss_total 0.9273\n",
      "step 2/3 | epoch 4/50 | batch 35/60 | global_step 3215 | loss_total 0.9011\n",
      "step 2/3 | epoch 4/50 | batch 36/60 | global_step 3216 | loss_total 0.9012\n",
      "step 2/3 | epoch 4/50 | batch 37/60 | global_step 3217 | loss_total 1.4736\n",
      "step 2/3 | epoch 4/50 | batch 38/60 | global_step 3218 | loss_total 1.8659\n",
      "step 2/3 | epoch 4/50 | batch 39/60 | global_step 3219 | loss_total 1.0174\n",
      "step 2/3 | epoch 4/50 | batch 40/60 | global_step 3220 | loss_total 1.1550\n",
      "step 2/3 | epoch 4/50 | batch 41/60 | global_step 3221 | loss_total 0.8856\n",
      "step 2/3 | epoch 4/50 | batch 42/60 | global_step 3222 | loss_total 0.9708\n",
      "step 2/3 | epoch 4/50 | batch 43/60 | global_step 3223 | loss_total 1.8668\n",
      "step 2/3 | epoch 4/50 | batch 44/60 | global_step 3224 | loss_total 0.8712\n",
      "step 2/3 | epoch 4/50 | batch 45/60 | global_step 3225 | loss_total 1.2292\n",
      "step 2/3 | epoch 4/50 | batch 46/60 | global_step 3226 | loss_total 0.8872\n",
      "step 2/3 | epoch 4/50 | batch 47/60 | global_step 3227 | loss_total 1.1057\n",
      "step 2/3 | epoch 4/50 | batch 48/60 | global_step 3228 | loss_total 0.8895\n",
      "step 2/3 | epoch 4/50 | batch 49/60 | global_step 3229 | loss_total 2.8063\n",
      "step 2/3 | epoch 4/50 | batch 50/60 | global_step 3230 | loss_total 0.9752\n",
      "step 2/3 | epoch 4/50 | batch 51/60 | global_step 3231 | loss_total 1.5704\n",
      "step 2/3 | epoch 4/50 | batch 52/60 | global_step 3232 | loss_total 1.4617\n",
      "step 2/3 | epoch 4/50 | batch 53/60 | global_step 3233 | loss_total 0.8075\n",
      "step 2/3 | epoch 4/50 | batch 54/60 | global_step 3234 | loss_total 0.8923\n",
      "step 2/3 | epoch 4/50 | batch 55/60 | global_step 3235 | loss_total 1.8816\n",
      "step 2/3 | epoch 4/50 | batch 56/60 | global_step 3236 | loss_total 1.8199\n",
      "step 2/3 | epoch 4/50 | batch 57/60 | global_step 3237 | loss_total 1.1136\n",
      "step 2/3 | epoch 4/50 | batch 58/60 | global_step 3238 | loss_total 2.3393\n",
      "step 2/3 | epoch 4/50 | batch 59/60 | global_step 3239 | loss_total 0.9000\n",
      "step 2/3 | epoch 4/50 | batch 60/60 | global_step 3240 | loss_total 0.8486\n",
      "[epoch done] step 2/3 epoch 4/50 | train_total=1.3040 val_total=1.3653\n",
      "step 2/3 | epoch 5/50 | batch 1/60 | global_step 3241 | loss_total 2.8644\n",
      "step 2/3 | epoch 5/50 | batch 2/60 | global_step 3242 | loss_total 1.9240\n",
      "step 2/3 | epoch 5/50 | batch 3/60 | global_step 3243 | loss_total 1.0213\n",
      "step 2/3 | epoch 5/50 | batch 4/60 | global_step 3244 | loss_total 1.2430\n",
      "step 2/3 | epoch 5/50 | batch 5/60 | global_step 3245 | loss_total 1.2642\n",
      "step 2/3 | epoch 5/50 | batch 6/60 | global_step 3246 | loss_total 0.8857\n",
      "step 2/3 | epoch 5/50 | batch 7/60 | global_step 3247 | loss_total 1.0157\n",
      "step 2/3 | epoch 5/50 | batch 8/60 | global_step 3248 | loss_total 0.9262\n",
      "step 2/3 | epoch 5/50 | batch 9/60 | global_step 3249 | loss_total 0.9449\n",
      "step 2/3 | epoch 5/50 | batch 10/60 | global_step 3250 | loss_total 1.2954\n",
      "step 2/3 | epoch 5/50 | batch 11/60 | global_step 3251 | loss_total 1.2407\n",
      "step 2/3 | epoch 5/50 | batch 12/60 | global_step 3252 | loss_total 0.8767\n",
      "step 2/3 | epoch 5/50 | batch 13/60 | global_step 3253 | loss_total 1.9274\n",
      "step 2/3 | epoch 5/50 | batch 14/60 | global_step 3254 | loss_total 1.2363\n",
      "step 2/3 | epoch 5/50 | batch 15/60 | global_step 3255 | loss_total 1.2667\n",
      "step 2/3 | epoch 5/50 | batch 16/60 | global_step 3256 | loss_total 2.0374\n",
      "step 2/3 | epoch 5/50 | batch 17/60 | global_step 3257 | loss_total 1.2383\n",
      "step 2/3 | epoch 5/50 | batch 18/60 | global_step 3258 | loss_total 0.8587\n",
      "step 2/3 | epoch 5/50 | batch 19/60 | global_step 3259 | loss_total 0.9996\n",
      "step 2/3 | epoch 5/50 | batch 20/60 | global_step 3260 | loss_total 0.8642\n",
      "step 2/3 | epoch 5/50 | batch 21/60 | global_step 3261 | loss_total 1.9483\n",
      "step 2/3 | epoch 5/50 | batch 22/60 | global_step 3262 | loss_total 0.8691\n",
      "step 2/3 | epoch 5/50 | batch 23/60 | global_step 3263 | loss_total 2.8918\n",
      "step 2/3 | epoch 5/50 | batch 24/60 | global_step 3264 | loss_total 1.6731\n",
      "step 2/3 | epoch 5/50 | batch 25/60 | global_step 3265 | loss_total 1.0129\n",
      "step 2/3 | epoch 5/50 | batch 26/60 | global_step 3266 | loss_total 0.9097\n",
      "step 2/3 | epoch 5/50 | batch 27/60 | global_step 3267 | loss_total 1.9064\n",
      "step 2/3 | epoch 5/50 | batch 28/60 | global_step 3268 | loss_total 1.0524\n",
      "step 2/3 | epoch 5/50 | batch 29/60 | global_step 3269 | loss_total 1.6428\n",
      "step 2/3 | epoch 5/50 | batch 30/60 | global_step 3270 | loss_total 1.1343\n",
      "step 2/3 | epoch 5/50 | batch 31/60 | global_step 3271 | loss_total 0.8613\n",
      "step 2/3 | epoch 5/50 | batch 32/60 | global_step 3272 | loss_total 2.8252\n",
      "step 2/3 | epoch 5/50 | batch 33/60 | global_step 3273 | loss_total 0.8897\n",
      "step 2/3 | epoch 5/50 | batch 34/60 | global_step 3274 | loss_total 1.7218\n",
      "step 2/3 | epoch 5/50 | batch 35/60 | global_step 3275 | loss_total 0.9232\n",
      "step 2/3 | epoch 5/50 | batch 36/60 | global_step 3276 | loss_total 1.7117\n",
      "step 2/3 | epoch 5/50 | batch 37/60 | global_step 3277 | loss_total 0.9394\n",
      "step 2/3 | epoch 5/50 | batch 38/60 | global_step 3278 | loss_total 1.2857\n",
      "step 2/3 | epoch 5/50 | batch 39/60 | global_step 3279 | loss_total 1.0834\n",
      "step 2/3 | epoch 5/50 | batch 40/60 | global_step 3280 | loss_total 0.8847\n",
      "step 2/3 | epoch 5/50 | batch 41/60 | global_step 3281 | loss_total 2.3498\n",
      "step 2/3 | epoch 5/50 | batch 42/60 | global_step 3282 | loss_total 0.9461\n",
      "step 2/3 | epoch 5/50 | batch 43/60 | global_step 3283 | loss_total 1.2450\n",
      "step 2/3 | epoch 5/50 | batch 44/60 | global_step 3284 | loss_total 1.1880\n",
      "step 2/3 | epoch 5/50 | batch 45/60 | global_step 3285 | loss_total 1.6126\n",
      "step 2/3 | epoch 5/50 | batch 46/60 | global_step 3286 | loss_total 0.9934\n",
      "step 2/3 | epoch 5/50 | batch 47/60 | global_step 3287 | loss_total 1.8902\n",
      "step 2/3 | epoch 5/50 | batch 48/60 | global_step 3288 | loss_total 1.9886\n",
      "step 2/3 | epoch 5/50 | batch 49/60 | global_step 3289 | loss_total 1.3364\n",
      "step 2/3 | epoch 5/50 | batch 50/60 | global_step 3290 | loss_total 1.9802\n",
      "step 2/3 | epoch 5/50 | batch 51/60 | global_step 3291 | loss_total 0.9534\n",
      "step 2/3 | epoch 5/50 | batch 52/60 | global_step 3292 | loss_total 1.6499\n",
      "step 2/3 | epoch 5/50 | batch 53/60 | global_step 3293 | loss_total 0.9302\n",
      "step 2/3 | epoch 5/50 | batch 54/60 | global_step 3294 | loss_total 1.6844\n",
      "step 2/3 | epoch 5/50 | batch 55/60 | global_step 3295 | loss_total 2.3171\n",
      "step 2/3 | epoch 5/50 | batch 56/60 | global_step 3296 | loss_total 1.0634\n",
      "step 2/3 | epoch 5/50 | batch 57/60 | global_step 3297 | loss_total 2.3135\n",
      "step 2/3 | epoch 5/50 | batch 58/60 | global_step 3298 | loss_total 0.9479\n",
      "step 2/3 | epoch 5/50 | batch 59/60 | global_step 3299 | loss_total 1.6793\n",
      "step 2/3 | epoch 5/50 | batch 60/60 | global_step 3300 | loss_total 0.9710\n",
      "[epoch done] step 2/3 epoch 5/50 | train_total=1.4023 val_total=1.0729\n",
      "step 2/3 | epoch 6/50 | batch 1/60 | global_step 3301 | loss_total 1.8051\n",
      "step 2/3 | epoch 6/50 | batch 2/60 | global_step 3302 | loss_total 1.6301\n",
      "step 2/3 | epoch 6/50 | batch 3/60 | global_step 3303 | loss_total 1.6056\n",
      "step 2/3 | epoch 6/50 | batch 4/60 | global_step 3304 | loss_total 1.6556\n",
      "step 2/3 | epoch 6/50 | batch 5/60 | global_step 3305 | loss_total 1.1509\n",
      "step 2/3 | epoch 6/50 | batch 6/60 | global_step 3306 | loss_total 1.0729\n",
      "step 2/3 | epoch 6/50 | batch 7/60 | global_step 3307 | loss_total 1.0665\n",
      "step 2/3 | epoch 6/50 | batch 8/60 | global_step 3308 | loss_total 2.1872\n",
      "step 2/3 | epoch 6/50 | batch 9/60 | global_step 3309 | loss_total 1.5629\n",
      "step 2/3 | epoch 6/50 | batch 10/60 | global_step 3310 | loss_total 1.1094\n",
      "step 2/3 | epoch 6/50 | batch 11/60 | global_step 3311 | loss_total 1.1661\n",
      "step 2/3 | epoch 6/50 | batch 12/60 | global_step 3312 | loss_total 1.8437\n",
      "step 2/3 | epoch 6/50 | batch 13/60 | global_step 3313 | loss_total 1.1611\n",
      "step 2/3 | epoch 6/50 | batch 14/60 | global_step 3314 | loss_total 1.5925\n",
      "step 2/3 | epoch 6/50 | batch 15/60 | global_step 3315 | loss_total 1.1319\n",
      "step 2/3 | epoch 6/50 | batch 16/60 | global_step 3316 | loss_total 1.0077\n",
      "step 2/3 | epoch 6/50 | batch 17/60 | global_step 3317 | loss_total 1.0739\n",
      "step 2/3 | epoch 6/50 | batch 18/60 | global_step 3318 | loss_total 1.5746\n",
      "step 2/3 | epoch 6/50 | batch 19/60 | global_step 3319 | loss_total 1.7246\n",
      "step 2/3 | epoch 6/50 | batch 20/60 | global_step 3320 | loss_total 1.0277\n",
      "step 2/3 | epoch 6/50 | batch 21/60 | global_step 3321 | loss_total 1.0204\n",
      "step 2/3 | epoch 6/50 | batch 22/60 | global_step 3322 | loss_total 0.9747\n",
      "step 2/3 | epoch 6/50 | batch 23/60 | global_step 3323 | loss_total 0.9833\n",
      "step 2/3 | epoch 6/50 | batch 24/60 | global_step 3324 | loss_total 1.0343\n",
      "step 2/3 | epoch 6/50 | batch 25/60 | global_step 3325 | loss_total 0.9750\n",
      "step 2/3 | epoch 6/50 | batch 26/60 | global_step 3326 | loss_total 0.9735\n",
      "step 2/3 | epoch 6/50 | batch 27/60 | global_step 3327 | loss_total 1.6873\n",
      "step 2/3 | epoch 6/50 | batch 28/60 | global_step 3328 | loss_total 1.5642\n",
      "step 2/3 | epoch 6/50 | batch 29/60 | global_step 3329 | loss_total 0.9885\n",
      "step 2/3 | epoch 6/50 | batch 30/60 | global_step 3330 | loss_total 2.3884\n",
      "step 2/3 | epoch 6/50 | batch 31/60 | global_step 3331 | loss_total 2.2433\n",
      "step 2/3 | epoch 6/50 | batch 32/60 | global_step 3332 | loss_total 0.9325\n",
      "step 2/3 | epoch 6/50 | batch 33/60 | global_step 3333 | loss_total 1.0920\n",
      "step 2/3 | epoch 6/50 | batch 34/60 | global_step 3334 | loss_total 2.1968\n",
      "step 2/3 | epoch 6/50 | batch 35/60 | global_step 3335 | loss_total 0.9532\n",
      "step 2/3 | epoch 6/50 | batch 36/60 | global_step 3336 | loss_total 1.6456\n",
      "step 2/3 | epoch 6/50 | batch 37/60 | global_step 3337 | loss_total 1.6125\n",
      "step 2/3 | epoch 6/50 | batch 38/60 | global_step 3338 | loss_total 1.1039\n",
      "step 2/3 | epoch 6/50 | batch 39/60 | global_step 3339 | loss_total 1.0605\n",
      "step 2/3 | epoch 6/50 | batch 40/60 | global_step 3340 | loss_total 0.9566\n",
      "step 2/3 | epoch 6/50 | batch 41/60 | global_step 3341 | loss_total 1.6447\n",
      "step 2/3 | epoch 6/50 | batch 42/60 | global_step 3342 | loss_total 1.5784\n",
      "step 2/3 | epoch 6/50 | batch 43/60 | global_step 3343 | loss_total 1.3955\n",
      "step 2/3 | epoch 6/50 | batch 44/60 | global_step 3344 | loss_total 0.9376\n",
      "step 2/3 | epoch 6/50 | batch 45/60 | global_step 3345 | loss_total 2.0574\n",
      "step 2/3 | epoch 6/50 | batch 46/60 | global_step 3346 | loss_total 1.8712\n",
      "step 2/3 | epoch 6/50 | batch 47/60 | global_step 3347 | loss_total 1.5401\n",
      "step 2/3 | epoch 6/50 | batch 48/60 | global_step 3348 | loss_total 0.9742\n",
      "step 2/3 | epoch 6/50 | batch 49/60 | global_step 3349 | loss_total 0.9715\n",
      "step 2/3 | epoch 6/50 | batch 50/60 | global_step 3350 | loss_total 1.5267\n",
      "step 2/3 | epoch 6/50 | batch 51/60 | global_step 3351 | loss_total 1.9058\n",
      "step 2/3 | epoch 6/50 | batch 52/60 | global_step 3352 | loss_total 1.2701\n",
      "step 2/3 | epoch 6/50 | batch 53/60 | global_step 3353 | loss_total 1.2706\n",
      "step 2/3 | epoch 6/50 | batch 54/60 | global_step 3354 | loss_total 0.9012\n",
      "step 2/3 | epoch 6/50 | batch 55/60 | global_step 3355 | loss_total 1.4754\n",
      "step 2/3 | epoch 6/50 | batch 56/60 | global_step 3356 | loss_total 1.2767\n",
      "step 2/3 | epoch 6/50 | batch 57/60 | global_step 3357 | loss_total 0.9461\n",
      "step 2/3 | epoch 6/50 | batch 58/60 | global_step 3358 | loss_total 1.0381\n",
      "step 2/3 | epoch 6/50 | batch 59/60 | global_step 3359 | loss_total 1.4536\n",
      "step 2/3 | epoch 6/50 | batch 60/60 | global_step 3360 | loss_total 0.8245\n",
      "[epoch done] step 2/3 epoch 6/50 | train_total=1.3566 val_total=1.5505\n",
      "step 2/3 | epoch 7/50 | batch 1/60 | global_step 3361 | loss_total 0.9105\n",
      "step 2/3 | epoch 7/50 | batch 2/60 | global_step 3362 | loss_total 2.1723\n",
      "step 2/3 | epoch 7/50 | batch 3/60 | global_step 3363 | loss_total 0.9119\n",
      "step 2/3 | epoch 7/50 | batch 4/60 | global_step 3364 | loss_total 1.1562\n",
      "step 2/3 | epoch 7/50 | batch 5/60 | global_step 3365 | loss_total 1.9993\n",
      "step 2/3 | epoch 7/50 | batch 6/60 | global_step 3366 | loss_total 0.9716\n",
      "step 2/3 | epoch 7/50 | batch 7/60 | global_step 3367 | loss_total 0.8569\n",
      "step 2/3 | epoch 7/50 | batch 8/60 | global_step 3368 | loss_total 0.8274\n",
      "step 2/3 | epoch 7/50 | batch 9/60 | global_step 3369 | loss_total 0.8488\n",
      "step 2/3 | epoch 7/50 | batch 10/60 | global_step 3370 | loss_total 0.8306\n",
      "step 2/3 | epoch 7/50 | batch 11/60 | global_step 3371 | loss_total 0.8060\n",
      "step 2/3 | epoch 7/50 | batch 12/60 | global_step 3372 | loss_total 1.5171\n",
      "step 2/3 | epoch 7/50 | batch 13/60 | global_step 3373 | loss_total 1.6070\n",
      "step 2/3 | epoch 7/50 | batch 14/60 | global_step 3374 | loss_total 2.3022\n",
      "step 2/3 | epoch 7/50 | batch 15/60 | global_step 3375 | loss_total 0.7862\n",
      "step 2/3 | epoch 7/50 | batch 16/60 | global_step 3376 | loss_total 0.8705\n",
      "step 2/3 | epoch 7/50 | batch 17/60 | global_step 3377 | loss_total 0.8268\n",
      "step 2/3 | epoch 7/50 | batch 18/60 | global_step 3378 | loss_total 1.0743\n",
      "step 2/3 | epoch 7/50 | batch 19/60 | global_step 3379 | loss_total 0.7485\n",
      "step 2/3 | epoch 7/50 | batch 20/60 | global_step 3380 | loss_total 0.7190\n",
      "step 2/3 | epoch 7/50 | batch 21/60 | global_step 3381 | loss_total 0.8595\n",
      "step 2/3 | epoch 7/50 | batch 22/60 | global_step 3382 | loss_total 0.7109\n",
      "step 2/3 | epoch 7/50 | batch 23/60 | global_step 3383 | loss_total 0.6710\n",
      "step 2/3 | epoch 7/50 | batch 24/60 | global_step 3384 | loss_total 0.6772\n",
      "step 2/3 | epoch 7/50 | batch 25/60 | global_step 3385 | loss_total 0.6972\n",
      "step 2/3 | epoch 7/50 | batch 26/60 | global_step 3386 | loss_total 0.6302\n",
      "step 2/3 | epoch 7/50 | batch 27/60 | global_step 3387 | loss_total 0.8382\n",
      "step 2/3 | epoch 7/50 | batch 28/60 | global_step 3388 | loss_total 1.6467\n",
      "step 2/3 | epoch 7/50 | batch 29/60 | global_step 3389 | loss_total 1.8095\n",
      "step 2/3 | epoch 7/50 | batch 30/60 | global_step 3390 | loss_total 1.0147\n",
      "step 2/3 | epoch 7/50 | batch 31/60 | global_step 3391 | loss_total 0.8464\n",
      "step 2/3 | epoch 7/50 | batch 32/60 | global_step 3392 | loss_total 0.7387\n",
      "step 2/3 | epoch 7/50 | batch 33/60 | global_step 3393 | loss_total 0.7395\n",
      "step 2/3 | epoch 7/50 | batch 34/60 | global_step 3394 | loss_total 0.7165\n",
      "step 2/3 | epoch 7/50 | batch 35/60 | global_step 3395 | loss_total 0.8316\n",
      "step 2/3 | epoch 7/50 | batch 36/60 | global_step 3396 | loss_total 1.7029\n",
      "step 2/3 | epoch 7/50 | batch 37/60 | global_step 3397 | loss_total 0.7393\n",
      "step 2/3 | epoch 7/50 | batch 38/60 | global_step 3398 | loss_total 2.0892\n",
      "step 2/3 | epoch 7/50 | batch 39/60 | global_step 3399 | loss_total 0.7206\n",
      "step 2/3 | epoch 7/50 | batch 40/60 | global_step 3400 | loss_total 2.2428\n",
      "step 2/3 | epoch 7/50 | batch 41/60 | global_step 3401 | loss_total 0.7280\n",
      "step 2/3 | epoch 7/50 | batch 42/60 | global_step 3402 | loss_total 0.7676\n",
      "step 2/3 | epoch 7/50 | batch 43/60 | global_step 3403 | loss_total 1.7048\n",
      "step 2/3 | epoch 7/50 | batch 44/60 | global_step 3404 | loss_total 1.7474\n",
      "step 2/3 | epoch 7/50 | batch 45/60 | global_step 3405 | loss_total 0.7336\n",
      "step 2/3 | epoch 7/50 | batch 46/60 | global_step 3406 | loss_total 0.7347\n",
      "step 2/3 | epoch 7/50 | batch 47/60 | global_step 3407 | loss_total 1.7137\n",
      "step 2/3 | epoch 7/50 | batch 48/60 | global_step 3408 | loss_total 1.7393\n",
      "step 2/3 | epoch 7/50 | batch 49/60 | global_step 3409 | loss_total 0.7721\n",
      "step 2/3 | epoch 7/50 | batch 50/60 | global_step 3410 | loss_total 1.6864\n",
      "step 2/3 | epoch 7/50 | batch 51/60 | global_step 3411 | loss_total 0.8394\n",
      "step 2/3 | epoch 7/50 | batch 52/60 | global_step 3412 | loss_total 1.6606\n",
      "step 2/3 | epoch 7/50 | batch 53/60 | global_step 3413 | loss_total 0.8240\n",
      "step 2/3 | epoch 7/50 | batch 54/60 | global_step 3414 | loss_total 0.8858\n",
      "step 2/3 | epoch 7/50 | batch 55/60 | global_step 3415 | loss_total 1.6785\n",
      "step 2/3 | epoch 7/50 | batch 56/60 | global_step 3416 | loss_total 1.8942\n",
      "step 2/3 | epoch 7/50 | batch 57/60 | global_step 3417 | loss_total 0.8983\n",
      "step 2/3 | epoch 7/50 | batch 58/60 | global_step 3418 | loss_total 0.9175\n",
      "step 2/3 | epoch 7/50 | batch 59/60 | global_step 3419 | loss_total 1.0150\n",
      "step 2/3 | epoch 7/50 | batch 60/60 | global_step 3420 | loss_total 1.7004\n",
      "[epoch done] step 2/3 epoch 7/50 | train_total=1.1384 val_total=1.0697\n",
      "step 2/3 | epoch 8/50 | batch 1/60 | global_step 3421 | loss_total 1.8638\n",
      "step 2/3 | epoch 8/50 | batch 2/60 | global_step 3422 | loss_total 0.9027\n",
      "step 2/3 | epoch 8/50 | batch 3/60 | global_step 3423 | loss_total 0.9662\n",
      "step 2/3 | epoch 8/50 | batch 4/60 | global_step 3424 | loss_total 1.8511\n",
      "step 2/3 | epoch 8/50 | batch 5/60 | global_step 3425 | loss_total 0.9493\n",
      "step 2/3 | epoch 8/50 | batch 6/60 | global_step 3426 | loss_total 2.8903\n",
      "step 2/3 | epoch 8/50 | batch 7/60 | global_step 3427 | loss_total 0.9855\n",
      "step 2/3 | epoch 8/50 | batch 8/60 | global_step 3428 | loss_total 1.7761\n",
      "step 2/3 | epoch 8/50 | batch 9/60 | global_step 3429 | loss_total 1.7561\n",
      "step 2/3 | epoch 8/50 | batch 10/60 | global_step 3430 | loss_total 1.7130\n",
      "step 2/3 | epoch 8/50 | batch 11/60 | global_step 3431 | loss_total 1.7314\n",
      "step 2/3 | epoch 8/50 | batch 12/60 | global_step 3432 | loss_total 1.8519\n",
      "step 2/3 | epoch 8/50 | batch 13/60 | global_step 3433 | loss_total 1.0078\n",
      "step 2/3 | epoch 8/50 | batch 14/60 | global_step 3434 | loss_total 1.2760\n",
      "step 2/3 | epoch 8/50 | batch 15/60 | global_step 3435 | loss_total 1.1031\n",
      "step 2/3 | epoch 8/50 | batch 16/60 | global_step 3436 | loss_total 2.0352\n",
      "step 2/3 | epoch 8/50 | batch 17/60 | global_step 3437 | loss_total 1.1177\n",
      "step 2/3 | epoch 8/50 | batch 18/60 | global_step 3438 | loss_total 1.2164\n",
      "step 2/3 | epoch 8/50 | batch 19/60 | global_step 3439 | loss_total 1.7445\n",
      "step 2/3 | epoch 8/50 | batch 20/60 | global_step 3440 | loss_total 1.7448\n",
      "step 2/3 | epoch 8/50 | batch 21/60 | global_step 3441 | loss_total 1.1701\n",
      "step 2/3 | epoch 8/50 | batch 22/60 | global_step 3442 | loss_total 1.2055\n",
      "step 2/3 | epoch 8/50 | batch 23/60 | global_step 3443 | loss_total 1.6483\n",
      "step 2/3 | epoch 8/50 | batch 24/60 | global_step 3444 | loss_total 1.2348\n",
      "step 2/3 | epoch 8/50 | batch 25/60 | global_step 3445 | loss_total 1.6269\n",
      "step 2/3 | epoch 8/50 | batch 26/60 | global_step 3446 | loss_total 1.9814\n",
      "step 2/3 | epoch 8/50 | batch 27/60 | global_step 3447 | loss_total 1.8665\n",
      "step 2/3 | epoch 8/50 | batch 28/60 | global_step 3448 | loss_total 1.2784\n",
      "step 2/3 | epoch 8/50 | batch 29/60 | global_step 3449 | loss_total 1.2411\n",
      "step 2/3 | epoch 8/50 | batch 30/60 | global_step 3450 | loss_total 1.1265\n",
      "step 2/3 | epoch 8/50 | batch 31/60 | global_step 3451 | loss_total 2.2868\n",
      "step 2/3 | epoch 8/50 | batch 32/60 | global_step 3452 | loss_total 2.0319\n",
      "step 2/3 | epoch 8/50 | batch 33/60 | global_step 3453 | loss_total 1.2022\n",
      "step 2/3 | epoch 8/50 | batch 34/60 | global_step 3454 | loss_total 1.8795\n",
      "step 2/3 | epoch 8/50 | batch 35/60 | global_step 3455 | loss_total 1.1812\n",
      "step 2/3 | epoch 8/50 | batch 36/60 | global_step 3456 | loss_total 1.1101\n",
      "step 2/3 | epoch 8/50 | batch 37/60 | global_step 3457 | loss_total 1.1480\n",
      "step 2/3 | epoch 8/50 | batch 38/60 | global_step 3458 | loss_total 1.1083\n",
      "step 2/3 | epoch 8/50 | batch 39/60 | global_step 3459 | loss_total 2.0459\n",
      "step 2/3 | epoch 8/50 | batch 40/60 | global_step 3460 | loss_total 1.1110\n",
      "step 2/3 | epoch 8/50 | batch 41/60 | global_step 3461 | loss_total 1.6171\n",
      "step 2/3 | epoch 8/50 | batch 42/60 | global_step 3462 | loss_total 1.5548\n",
      "step 2/3 | epoch 8/50 | batch 43/60 | global_step 3463 | loss_total 1.8377\n",
      "step 2/3 | epoch 8/50 | batch 44/60 | global_step 3464 | loss_total 1.0842\n",
      "step 2/3 | epoch 8/50 | batch 45/60 | global_step 3465 | loss_total 1.0531\n",
      "step 2/3 | epoch 8/50 | batch 46/60 | global_step 3466 | loss_total 1.0523\n",
      "step 2/3 | epoch 8/50 | batch 47/60 | global_step 3467 | loss_total 1.0300\n",
      "step 2/3 | epoch 8/50 | batch 48/60 | global_step 3468 | loss_total 1.0131\n",
      "step 2/3 | epoch 8/50 | batch 49/60 | global_step 3469 | loss_total 1.7620\n",
      "step 2/3 | epoch 8/50 | batch 50/60 | global_step 3470 | loss_total 1.0469\n",
      "step 2/3 | epoch 8/50 | batch 51/60 | global_step 3471 | loss_total 0.9458\n",
      "step 2/3 | epoch 8/50 | batch 52/60 | global_step 3472 | loss_total 1.4327\n",
      "step 2/3 | epoch 8/50 | batch 53/60 | global_step 3473 | loss_total 1.5620\n",
      "step 2/3 | epoch 8/50 | batch 54/60 | global_step 3474 | loss_total 0.9042\n",
      "step 2/3 | epoch 8/50 | batch 55/60 | global_step 3475 | loss_total 1.4763\n",
      "step 2/3 | epoch 8/50 | batch 56/60 | global_step 3476 | loss_total 0.8552\n",
      "step 2/3 | epoch 8/50 | batch 57/60 | global_step 3477 | loss_total 0.8468\n",
      "step 2/3 | epoch 8/50 | batch 58/60 | global_step 3478 | loss_total 0.8371\n",
      "step 2/3 | epoch 8/50 | batch 59/60 | global_step 3479 | loss_total 0.8026\n",
      "step 2/3 | epoch 8/50 | batch 60/60 | global_step 3480 | loss_total 0.8624\n",
      "[epoch done] step 2/3 epoch 8/50 | train_total=1.3924 val_total=1.2399\n",
      "step 2/3 | epoch 9/50 | batch 1/60 | global_step 3481 | loss_total 0.8417\n",
      "step 2/3 | epoch 9/50 | batch 2/60 | global_step 3482 | loss_total 0.7597\n",
      "step 2/3 | epoch 9/50 | batch 3/60 | global_step 3483 | loss_total 0.7196\n",
      "step 2/3 | epoch 9/50 | batch 4/60 | global_step 3484 | loss_total 0.7764\n",
      "step 2/3 | epoch 9/50 | batch 5/60 | global_step 3485 | loss_total 0.6754\n",
      "step 2/3 | epoch 9/50 | batch 6/60 | global_step 3486 | loss_total 0.7129\n",
      "step 2/3 | epoch 9/50 | batch 7/60 | global_step 3487 | loss_total 1.4636\n",
      "step 2/3 | epoch 9/50 | batch 8/60 | global_step 3488 | loss_total 1.6198\n",
      "step 2/3 | epoch 9/50 | batch 9/60 | global_step 3489 | loss_total 0.6520\n",
      "step 2/3 | epoch 9/50 | batch 10/60 | global_step 3490 | loss_total 0.6510\n",
      "step 2/3 | epoch 9/50 | batch 11/60 | global_step 3491 | loss_total 1.2688\n",
      "step 2/3 | epoch 9/50 | batch 12/60 | global_step 3492 | loss_total 0.5513\n",
      "step 2/3 | epoch 9/50 | batch 13/60 | global_step 3493 | loss_total 2.0815\n",
      "step 2/3 | epoch 9/50 | batch 14/60 | global_step 3494 | loss_total 1.2658\n",
      "step 2/3 | epoch 9/50 | batch 15/60 | global_step 3495 | loss_total 2.3090\n",
      "step 2/3 | epoch 9/50 | batch 16/60 | global_step 3496 | loss_total 0.5259\n",
      "step 2/3 | epoch 9/50 | batch 17/60 | global_step 3497 | loss_total 1.6414\n",
      "step 2/3 | epoch 9/50 | batch 18/60 | global_step 3498 | loss_total 1.7708\n",
      "step 2/3 | epoch 9/50 | batch 19/60 | global_step 3499 | loss_total 0.7373\n",
      "step 2/3 | epoch 9/50 | batch 20/60 | global_step 3500 | loss_total 1.7957\n",
      "step 2/3 | epoch 9/50 | batch 21/60 | global_step 3501 | loss_total 0.6658\n",
      "step 2/3 | epoch 9/50 | batch 22/60 | global_step 3502 | loss_total 1.4888\n",
      "step 2/3 | epoch 9/50 | batch 23/60 | global_step 3503 | loss_total 1.4132\n",
      "step 2/3 | epoch 9/50 | batch 24/60 | global_step 3504 | loss_total 0.5066\n",
      "step 2/3 | epoch 9/50 | batch 25/60 | global_step 3505 | loss_total 0.4527\n",
      "step 2/3 | epoch 9/50 | batch 26/60 | global_step 3506 | loss_total 0.4457\n",
      "step 2/3 | epoch 9/50 | batch 27/60 | global_step 3507 | loss_total 0.4910\n",
      "step 2/3 | epoch 9/50 | batch 28/60 | global_step 3508 | loss_total 2.2531\n",
      "step 2/3 | epoch 9/50 | batch 29/60 | global_step 3509 | loss_total 1.1948\n",
      "step 2/3 | epoch 9/50 | batch 30/60 | global_step 3510 | loss_total 0.6032\n",
      "step 2/3 | epoch 9/50 | batch 31/60 | global_step 3511 | loss_total 0.5473\n",
      "step 2/3 | epoch 9/50 | batch 32/60 | global_step 3512 | loss_total 0.6331\n",
      "step 2/3 | epoch 9/50 | batch 33/60 | global_step 3513 | loss_total 1.1968\n",
      "step 2/3 | epoch 9/50 | batch 34/60 | global_step 3514 | loss_total 0.4047\n",
      "step 2/3 | epoch 9/50 | batch 35/60 | global_step 3515 | loss_total 0.3888\n",
      "step 2/3 | epoch 9/50 | batch 36/60 | global_step 3516 | loss_total 1.2230\n",
      "step 2/3 | epoch 9/50 | batch 37/60 | global_step 3517 | loss_total 1.5980\n",
      "step 2/3 | epoch 9/50 | batch 38/60 | global_step 3518 | loss_total 2.3309\n",
      "step 2/3 | epoch 9/50 | batch 39/60 | global_step 3519 | loss_total 1.6829\n",
      "step 2/3 | epoch 9/50 | batch 40/60 | global_step 3520 | loss_total 1.2218\n",
      "step 2/3 | epoch 9/50 | batch 41/60 | global_step 3521 | loss_total 1.1390\n",
      "step 2/3 | epoch 9/50 | batch 42/60 | global_step 3522 | loss_total 0.6812\n",
      "step 2/3 | epoch 9/50 | batch 43/60 | global_step 3523 | loss_total 1.4869\n",
      "step 2/3 | epoch 9/50 | batch 44/60 | global_step 3524 | loss_total 0.8413\n",
      "step 2/3 | epoch 9/50 | batch 45/60 | global_step 3525 | loss_total 1.1181\n",
      "step 2/3 | epoch 9/50 | batch 46/60 | global_step 3526 | loss_total 0.7083\n",
      "step 2/3 | epoch 9/50 | batch 47/60 | global_step 3527 | loss_total 1.0127\n",
      "step 2/3 | epoch 9/50 | batch 48/60 | global_step 3528 | loss_total 1.4899\n",
      "step 2/3 | epoch 9/50 | batch 49/60 | global_step 3529 | loss_total 0.6032\n",
      "step 2/3 | epoch 9/50 | batch 50/60 | global_step 3530 | loss_total 0.8670\n",
      "step 2/3 | epoch 9/50 | batch 51/60 | global_step 3531 | loss_total 1.3029\n",
      "step 2/3 | epoch 9/50 | batch 52/60 | global_step 3532 | loss_total 1.3243\n",
      "step 2/3 | epoch 9/50 | batch 53/60 | global_step 3533 | loss_total 2.1067\n",
      "step 2/3 | epoch 9/50 | batch 54/60 | global_step 3534 | loss_total 0.5922\n",
      "step 2/3 | epoch 9/50 | batch 55/60 | global_step 3535 | loss_total 1.6923\n",
      "step 2/3 | epoch 9/50 | batch 56/60 | global_step 3536 | loss_total 1.1126\n",
      "step 2/3 | epoch 9/50 | batch 57/60 | global_step 3537 | loss_total 1.4684\n",
      "step 2/3 | epoch 9/50 | batch 58/60 | global_step 3538 | loss_total 1.0067\n",
      "step 2/3 | epoch 9/50 | batch 59/60 | global_step 3539 | loss_total 0.7169\n",
      "step 2/3 | epoch 9/50 | batch 60/60 | global_step 3540 | loss_total 0.7182\n",
      "[epoch done] step 2/3 epoch 9/50 | train_total=1.0925 val_total=1.6114\n",
      "step 2/3 | epoch 10/50 | batch 1/60 | global_step 3541 | loss_total 1.0241\n",
      "step 2/3 | epoch 10/50 | batch 2/60 | global_step 3542 | loss_total 0.6930\n",
      "step 2/3 | epoch 10/50 | batch 3/60 | global_step 3543 | loss_total 1.5085\n",
      "step 2/3 | epoch 10/50 | batch 4/60 | global_step 3544 | loss_total 1.2171\n",
      "step 2/3 | epoch 10/50 | batch 5/60 | global_step 3545 | loss_total 2.0885\n",
      "step 2/3 | epoch 10/50 | batch 6/60 | global_step 3546 | loss_total 0.8602\n",
      "step 2/3 | epoch 10/50 | batch 7/60 | global_step 3547 | loss_total 0.7793\n",
      "step 2/3 | epoch 10/50 | batch 8/60 | global_step 3548 | loss_total 0.8518\n",
      "step 2/3 | epoch 10/50 | batch 9/60 | global_step 3549 | loss_total 1.6724\n",
      "step 2/3 | epoch 10/50 | batch 10/60 | global_step 3550 | loss_total 1.0400\n",
      "step 2/3 | epoch 10/50 | batch 11/60 | global_step 3551 | loss_total 1.2647\n",
      "step 2/3 | epoch 10/50 | batch 12/60 | global_step 3552 | loss_total 0.9667\n",
      "step 2/3 | epoch 10/50 | batch 13/60 | global_step 3553 | loss_total 0.8810\n",
      "step 2/3 | epoch 10/50 | batch 14/60 | global_step 3554 | loss_total 1.0330\n",
      "step 2/3 | epoch 10/50 | batch 15/60 | global_step 3555 | loss_total 1.7763\n",
      "step 2/3 | epoch 10/50 | batch 16/60 | global_step 3556 | loss_total 1.0366\n",
      "step 2/3 | epoch 10/50 | batch 17/60 | global_step 3557 | loss_total 1.8191\n",
      "step 2/3 | epoch 10/50 | batch 18/60 | global_step 3558 | loss_total 0.9732\n",
      "step 2/3 | epoch 10/50 | batch 19/60 | global_step 3559 | loss_total 2.2756\n",
      "step 2/3 | epoch 10/50 | batch 20/60 | global_step 3560 | loss_total 1.0710\n",
      "step 2/3 | epoch 10/50 | batch 21/60 | global_step 3561 | loss_total 2.5099\n",
      "step 2/3 | epoch 10/50 | batch 22/60 | global_step 3562 | loss_total 0.9835\n",
      "step 2/3 | epoch 10/50 | batch 23/60 | global_step 3563 | loss_total 0.9448\n",
      "step 2/3 | epoch 10/50 | batch 24/60 | global_step 3564 | loss_total 1.6078\n",
      "step 2/3 | epoch 10/50 | batch 25/60 | global_step 3565 | loss_total 0.9455\n",
      "step 2/3 | epoch 10/50 | batch 26/60 | global_step 3566 | loss_total 0.9696\n",
      "step 2/3 | epoch 10/50 | batch 27/60 | global_step 3567 | loss_total 1.5918\n",
      "step 2/3 | epoch 10/50 | batch 28/60 | global_step 3568 | loss_total 2.2418\n",
      "step 2/3 | epoch 10/50 | batch 29/60 | global_step 3569 | loss_total 0.9746\n",
      "step 2/3 | epoch 10/50 | batch 30/60 | global_step 3570 | loss_total 0.9768\n",
      "step 2/3 | epoch 10/50 | batch 31/60 | global_step 3571 | loss_total 1.4880\n",
      "step 2/3 | epoch 10/50 | batch 32/60 | global_step 3572 | loss_total 0.9465\n",
      "step 2/3 | epoch 10/50 | batch 33/60 | global_step 3573 | loss_total 1.0089\n",
      "step 2/3 | epoch 10/50 | batch 34/60 | global_step 3574 | loss_total 0.9559\n",
      "step 2/3 | epoch 10/50 | batch 35/60 | global_step 3575 | loss_total 1.7492\n",
      "step 2/3 | epoch 10/50 | batch 36/60 | global_step 3576 | loss_total 0.9789\n",
      "step 2/3 | epoch 10/50 | batch 37/60 | global_step 3577 | loss_total 1.0251\n",
      "step 2/3 | epoch 10/50 | batch 38/60 | global_step 3578 | loss_total 1.1435\n",
      "step 2/3 | epoch 10/50 | batch 39/60 | global_step 3579 | loss_total 1.3460\n",
      "step 2/3 | epoch 10/50 | batch 40/60 | global_step 3580 | loss_total 1.5386\n",
      "step 2/3 | epoch 10/50 | batch 41/60 | global_step 3581 | loss_total 1.0174\n",
      "step 2/3 | epoch 10/50 | batch 42/60 | global_step 3582 | loss_total 1.1105\n",
      "step 2/3 | epoch 10/50 | batch 43/60 | global_step 3583 | loss_total 1.0989\n",
      "step 2/3 | epoch 10/50 | batch 44/60 | global_step 3584 | loss_total 0.9290\n",
      "step 2/3 | epoch 10/50 | batch 45/60 | global_step 3585 | loss_total 3.8614\n",
      "step 2/3 | epoch 10/50 | batch 46/60 | global_step 3586 | loss_total 1.0224\n",
      "step 2/3 | epoch 10/50 | batch 47/60 | global_step 3587 | loss_total 1.0081\n",
      "step 2/3 | epoch 10/50 | batch 48/60 | global_step 3588 | loss_total 2.6575\n",
      "step 2/3 | epoch 10/50 | batch 49/60 | global_step 3589 | loss_total 2.7460\n",
      "step 2/3 | epoch 10/50 | batch 50/60 | global_step 3590 | loss_total 0.9863\n",
      "step 2/3 | epoch 10/50 | batch 51/60 | global_step 3591 | loss_total 2.0635\n",
      "step 2/3 | epoch 10/50 | batch 52/60 | global_step 3592 | loss_total 1.7550\n",
      "step 2/3 | epoch 10/50 | batch 53/60 | global_step 3593 | loss_total 1.1713\n",
      "step 2/3 | epoch 10/50 | batch 54/60 | global_step 3594 | loss_total 1.9611\n",
      "step 2/3 | epoch 10/50 | batch 55/60 | global_step 3595 | loss_total 1.0179\n",
      "step 2/3 | epoch 10/50 | batch 56/60 | global_step 3596 | loss_total 1.7226\n",
      "step 2/3 | epoch 10/50 | batch 57/60 | global_step 3597 | loss_total 1.1592\n",
      "step 2/3 | epoch 10/50 | batch 58/60 | global_step 3598 | loss_total 1.9664\n",
      "step 2/3 | epoch 10/50 | batch 59/60 | global_step 3599 | loss_total 1.0900\n",
      "step 2/3 | epoch 10/50 | batch 60/60 | global_step 3600 | loss_total 1.0703\n",
      "[epoch done] step 2/3 epoch 10/50 | train_total=1.3696 val_total=1.1020\n",
      "step 2/3 | epoch 11/50 | batch 1/60 | global_step 3601 | loss_total 1.1183\n",
      "step 2/3 | epoch 11/50 | batch 2/60 | global_step 3602 | loss_total 1.1523\n",
      "step 2/3 | epoch 11/50 | batch 3/60 | global_step 3603 | loss_total 1.0174\n",
      "step 2/3 | epoch 11/50 | batch 4/60 | global_step 3604 | loss_total 1.0870\n",
      "step 2/3 | epoch 11/50 | batch 5/60 | global_step 3605 | loss_total 1.7454\n",
      "step 2/3 | epoch 11/50 | batch 6/60 | global_step 3606 | loss_total 0.9975\n",
      "step 2/3 | epoch 11/50 | batch 7/60 | global_step 3607 | loss_total 1.8918\n",
      "step 2/3 | epoch 11/50 | batch 8/60 | global_step 3608 | loss_total 1.1022\n",
      "step 2/3 | epoch 11/50 | batch 9/60 | global_step 3609 | loss_total 1.1358\n",
      "step 2/3 | epoch 11/50 | batch 10/60 | global_step 3610 | loss_total 1.1810\n",
      "step 2/3 | epoch 11/50 | batch 11/60 | global_step 3611 | loss_total 1.3517\n",
      "step 2/3 | epoch 11/50 | batch 12/60 | global_step 3612 | loss_total 2.0229\n",
      "step 2/3 | epoch 11/50 | batch 13/60 | global_step 3613 | loss_total 1.9170\n",
      "step 2/3 | epoch 11/50 | batch 14/60 | global_step 3614 | loss_total 1.0307\n",
      "step 2/3 | epoch 11/50 | batch 15/60 | global_step 3615 | loss_total 1.1628\n",
      "step 2/3 | epoch 11/50 | batch 16/60 | global_step 3616 | loss_total 1.8550\n",
      "step 2/3 | epoch 11/50 | batch 17/60 | global_step 3617 | loss_total 2.8210\n",
      "step 2/3 | epoch 11/50 | batch 18/60 | global_step 3618 | loss_total 2.0779\n",
      "step 2/3 | epoch 11/50 | batch 19/60 | global_step 3619 | loss_total 1.0591\n",
      "step 2/3 | epoch 11/50 | batch 20/60 | global_step 3620 | loss_total 2.8166\n",
      "step 2/3 | epoch 11/50 | batch 21/60 | global_step 3621 | loss_total 1.0270\n",
      "step 2/3 | epoch 11/50 | batch 22/60 | global_step 3622 | loss_total 1.0887\n",
      "step 2/3 | epoch 11/50 | batch 23/60 | global_step 3623 | loss_total 1.5624\n",
      "step 2/3 | epoch 11/50 | batch 24/60 | global_step 3624 | loss_total 1.9121\n",
      "step 2/3 | epoch 11/50 | batch 25/60 | global_step 3625 | loss_total 1.1280\n",
      "step 2/3 | epoch 11/50 | batch 26/60 | global_step 3626 | loss_total 1.7960\n",
      "step 2/3 | epoch 11/50 | batch 27/60 | global_step 3627 | loss_total 1.0800\n",
      "step 2/3 | epoch 11/50 | batch 28/60 | global_step 3628 | loss_total 1.0744\n",
      "step 2/3 | epoch 11/50 | batch 29/60 | global_step 3629 | loss_total 1.4181\n",
      "step 2/3 | epoch 11/50 | batch 30/60 | global_step 3630 | loss_total 1.8048\n",
      "step 2/3 | epoch 11/50 | batch 31/60 | global_step 3631 | loss_total 0.9033\n",
      "step 2/3 | epoch 11/50 | batch 32/60 | global_step 3632 | loss_total 0.9876\n",
      "step 2/3 | epoch 11/50 | batch 33/60 | global_step 3633 | loss_total 1.0143\n",
      "step 2/3 | epoch 11/50 | batch 34/60 | global_step 3634 | loss_total 1.7664\n",
      "step 2/3 | epoch 11/50 | batch 35/60 | global_step 3635 | loss_total 1.7775\n",
      "step 2/3 | epoch 11/50 | batch 36/60 | global_step 3636 | loss_total 2.1118\n",
      "step 2/3 | epoch 11/50 | batch 37/60 | global_step 3637 | loss_total 0.9638\n",
      "step 2/3 | epoch 11/50 | batch 38/60 | global_step 3638 | loss_total 1.6180\n",
      "step 2/3 | epoch 11/50 | batch 39/60 | global_step 3639 | loss_total 1.7511\n",
      "step 2/3 | epoch 11/50 | batch 40/60 | global_step 3640 | loss_total 1.0766\n",
      "step 2/3 | epoch 11/50 | batch 41/60 | global_step 3641 | loss_total 1.0147\n",
      "step 2/3 | epoch 11/50 | batch 42/60 | global_step 3642 | loss_total 1.5959\n",
      "step 2/3 | epoch 11/50 | batch 43/60 | global_step 3643 | loss_total 0.9414\n",
      "step 2/3 | epoch 11/50 | batch 44/60 | global_step 3644 | loss_total 0.9840\n",
      "step 2/3 | epoch 11/50 | batch 45/60 | global_step 3645 | loss_total 1.7011\n",
      "step 2/3 | epoch 11/50 | batch 46/60 | global_step 3646 | loss_total 1.6237\n",
      "step 2/3 | epoch 11/50 | batch 47/60 | global_step 3647 | loss_total 1.7211\n",
      "step 2/3 | epoch 11/50 | batch 48/60 | global_step 3648 | loss_total 1.0091\n",
      "step 2/3 | epoch 11/50 | batch 49/60 | global_step 3649 | loss_total 0.9218\n",
      "step 2/3 | epoch 11/50 | batch 50/60 | global_step 3650 | loss_total 0.9755\n",
      "step 2/3 | epoch 11/50 | batch 51/60 | global_step 3651 | loss_total 0.9073\n",
      "step 2/3 | epoch 11/50 | batch 52/60 | global_step 3652 | loss_total 0.9531\n",
      "step 2/3 | epoch 11/50 | batch 53/60 | global_step 3653 | loss_total 1.6145\n",
      "step 2/3 | epoch 11/50 | batch 54/60 | global_step 3654 | loss_total 0.8610\n",
      "step 2/3 | epoch 11/50 | batch 55/60 | global_step 3655 | loss_total 0.8704\n",
      "step 2/3 | epoch 11/50 | batch 56/60 | global_step 3656 | loss_total 0.8322\n",
      "step 2/3 | epoch 11/50 | batch 57/60 | global_step 3657 | loss_total 1.4492\n",
      "step 2/3 | epoch 11/50 | batch 58/60 | global_step 3658 | loss_total 1.4340\n",
      "step 2/3 | epoch 11/50 | batch 59/60 | global_step 3659 | loss_total 0.7650\n",
      "step 2/3 | epoch 11/50 | batch 60/60 | global_step 3660 | loss_total 0.9164\n",
      "[epoch done] step 2/3 epoch 11/50 | train_total=1.3583 val_total=1.0040\n",
      "step 2/3 | epoch 12/50 | batch 1/60 | global_step 3661 | loss_total 0.7148\n",
      "step 2/3 | epoch 12/50 | batch 2/60 | global_step 3662 | loss_total 0.6935\n",
      "step 2/3 | epoch 12/50 | batch 3/60 | global_step 3663 | loss_total 0.7311\n",
      "step 2/3 | epoch 12/50 | batch 4/60 | global_step 3664 | loss_total 1.4531\n",
      "step 2/3 | epoch 12/50 | batch 5/60 | global_step 3665 | loss_total 0.8616\n",
      "step 2/3 | epoch 12/50 | batch 6/60 | global_step 3666 | loss_total 1.8572\n",
      "step 2/3 | epoch 12/50 | batch 7/60 | global_step 3667 | loss_total 0.8068\n",
      "step 2/3 | epoch 12/50 | batch 8/60 | global_step 3668 | loss_total 1.3822\n",
      "step 2/3 | epoch 12/50 | batch 9/60 | global_step 3669 | loss_total 1.9759\n",
      "step 2/3 | epoch 12/50 | batch 10/60 | global_step 3670 | loss_total 1.2026\n",
      "step 2/3 | epoch 12/50 | batch 11/60 | global_step 3671 | loss_total 0.6185\n",
      "step 2/3 | epoch 12/50 | batch 12/60 | global_step 3672 | loss_total 1.9589\n",
      "step 2/3 | epoch 12/50 | batch 13/60 | global_step 3673 | loss_total 0.6452\n",
      "step 2/3 | epoch 12/50 | batch 14/60 | global_step 3674 | loss_total 0.6681\n",
      "step 2/3 | epoch 12/50 | batch 15/60 | global_step 3675 | loss_total 0.6040\n",
      "step 2/3 | epoch 12/50 | batch 16/60 | global_step 3676 | loss_total 1.1328\n",
      "step 2/3 | epoch 12/50 | batch 17/60 | global_step 3677 | loss_total 0.7411\n",
      "step 2/3 | epoch 12/50 | batch 18/60 | global_step 3678 | loss_total 0.5602\n",
      "step 2/3 | epoch 12/50 | batch 19/60 | global_step 3679 | loss_total 0.8281\n",
      "step 2/3 | epoch 12/50 | batch 20/60 | global_step 3680 | loss_total 1.6646\n",
      "step 2/3 | epoch 12/50 | batch 21/60 | global_step 3681 | loss_total 2.0814\n",
      "step 2/3 | epoch 12/50 | batch 22/60 | global_step 3682 | loss_total 1.3570\n",
      "step 2/3 | epoch 12/50 | batch 23/60 | global_step 3683 | loss_total 0.7851\n",
      "step 2/3 | epoch 12/50 | batch 24/60 | global_step 3684 | loss_total 1.0106\n",
      "step 2/3 | epoch 12/50 | batch 25/60 | global_step 3685 | loss_total 0.5012\n",
      "step 2/3 | epoch 12/50 | batch 26/60 | global_step 3686 | loss_total 1.1768\n",
      "step 2/3 | epoch 12/50 | batch 27/60 | global_step 3687 | loss_total 0.7740\n",
      "step 2/3 | epoch 12/50 | batch 28/60 | global_step 3688 | loss_total 1.2358\n",
      "step 2/3 | epoch 12/50 | batch 29/60 | global_step 3689 | loss_total 0.5216\n",
      "step 2/3 | epoch 12/50 | batch 30/60 | global_step 3690 | loss_total 1.3594\n",
      "step 2/3 | epoch 12/50 | batch 31/60 | global_step 3691 | loss_total 1.9176\n",
      "step 2/3 | epoch 12/50 | batch 32/60 | global_step 3692 | loss_total 1.0375\n",
      "step 2/3 | epoch 12/50 | batch 33/60 | global_step 3693 | loss_total 0.7339\n",
      "step 2/3 | epoch 12/50 | batch 34/60 | global_step 3694 | loss_total 0.4980\n",
      "step 2/3 | epoch 12/50 | batch 35/60 | global_step 3695 | loss_total 0.5240\n",
      "step 2/3 | epoch 12/50 | batch 36/60 | global_step 3696 | loss_total 1.9960\n",
      "step 2/3 | epoch 12/50 | batch 37/60 | global_step 3697 | loss_total 0.7656\n",
      "step 2/3 | epoch 12/50 | batch 38/60 | global_step 3698 | loss_total 1.7473\n",
      "step 2/3 | epoch 12/50 | batch 39/60 | global_step 3699 | loss_total 1.6750\n",
      "step 2/3 | epoch 12/50 | batch 40/60 | global_step 3700 | loss_total 1.5471\n",
      "step 2/3 | epoch 12/50 | batch 41/60 | global_step 3701 | loss_total 0.7643\n",
      "step 2/3 | epoch 12/50 | batch 42/60 | global_step 3702 | loss_total 0.8219\n",
      "step 2/3 | epoch 12/50 | batch 43/60 | global_step 3703 | loss_total 2.5680\n",
      "step 2/3 | epoch 12/50 | batch 44/60 | global_step 3704 | loss_total 1.5714\n",
      "step 2/3 | epoch 12/50 | batch 45/60 | global_step 3705 | loss_total 1.5567\n",
      "step 2/3 | epoch 12/50 | batch 46/60 | global_step 3706 | loss_total 0.7969\n",
      "step 2/3 | epoch 12/50 | batch 47/60 | global_step 3707 | loss_total 1.7977\n",
      "step 2/3 | epoch 12/50 | batch 48/60 | global_step 3708 | loss_total 1.8392\n",
      "step 2/3 | epoch 12/50 | batch 49/60 | global_step 3709 | loss_total 0.8048\n",
      "step 2/3 | epoch 12/50 | batch 50/60 | global_step 3710 | loss_total 0.8347\n",
      "step 2/3 | epoch 12/50 | batch 51/60 | global_step 3711 | loss_total 0.8206\n",
      "step 2/3 | epoch 12/50 | batch 52/60 | global_step 3712 | loss_total 0.8178\n",
      "step 2/3 | epoch 12/50 | batch 53/60 | global_step 3713 | loss_total 1.9929\n",
      "step 2/3 | epoch 12/50 | batch 54/60 | global_step 3714 | loss_total 1.0199\n",
      "step 2/3 | epoch 12/50 | batch 55/60 | global_step 3715 | loss_total 0.6466\n",
      "step 2/3 | epoch 12/50 | batch 56/60 | global_step 3716 | loss_total 0.6501\n",
      "step 2/3 | epoch 12/50 | batch 57/60 | global_step 3717 | loss_total 0.9546\n",
      "step 2/3 | epoch 12/50 | batch 58/60 | global_step 3718 | loss_total 2.5721\n",
      "step 2/3 | epoch 12/50 | batch 59/60 | global_step 3719 | loss_total 1.0565\n",
      "step 2/3 | epoch 12/50 | batch 60/60 | global_step 3720 | loss_total 0.8214\n",
      "[epoch done] step 2/3 epoch 12/50 | train_total=1.1509 val_total=1.5917\n",
      "step 2/3 | epoch 13/50 | batch 1/60 | global_step 3721 | loss_total 2.7168\n",
      "step 2/3 | epoch 13/50 | batch 2/60 | global_step 3722 | loss_total 1.5722\n",
      "step 2/3 | epoch 13/50 | batch 3/60 | global_step 3723 | loss_total 2.0582\n",
      "step 2/3 | epoch 13/50 | batch 4/60 | global_step 3724 | loss_total 1.3944\n",
      "step 2/3 | epoch 13/50 | batch 5/60 | global_step 3725 | loss_total 1.0478\n",
      "step 2/3 | epoch 13/50 | batch 6/60 | global_step 3726 | loss_total 1.1011\n",
      "step 2/3 | epoch 13/50 | batch 7/60 | global_step 3727 | loss_total 0.7367\n",
      "step 2/3 | epoch 13/50 | batch 8/60 | global_step 3728 | loss_total 0.9292\n",
      "step 2/3 | epoch 13/50 | batch 9/60 | global_step 3729 | loss_total 0.9187\n",
      "step 2/3 | epoch 13/50 | batch 10/60 | global_step 3730 | loss_total 1.6866\n",
      "step 2/3 | epoch 13/50 | batch 11/60 | global_step 3731 | loss_total 0.9522\n",
      "step 2/3 | epoch 13/50 | batch 12/60 | global_step 3732 | loss_total 0.8268\n",
      "step 2/3 | epoch 13/50 | batch 13/60 | global_step 3733 | loss_total 0.9481\n",
      "step 2/3 | epoch 13/50 | batch 14/60 | global_step 3734 | loss_total 0.7970\n",
      "step 2/3 | epoch 13/50 | batch 15/60 | global_step 3735 | loss_total 0.7930\n",
      "step 2/3 | epoch 13/50 | batch 16/60 | global_step 3736 | loss_total 0.8913\n",
      "step 2/3 | epoch 13/50 | batch 17/60 | global_step 3737 | loss_total 0.7961\n",
      "step 2/3 | epoch 13/50 | batch 18/60 | global_step 3738 | loss_total 0.7545\n",
      "step 2/3 | epoch 13/50 | batch 19/60 | global_step 3739 | loss_total 0.7415\n",
      "step 2/3 | epoch 13/50 | batch 20/60 | global_step 3740 | loss_total 1.0050\n",
      "step 2/3 | epoch 13/50 | batch 21/60 | global_step 3741 | loss_total 0.9433\n",
      "step 2/3 | epoch 13/50 | batch 22/60 | global_step 3742 | loss_total 1.7628\n",
      "step 2/3 | epoch 13/50 | batch 23/60 | global_step 3743 | loss_total 1.5983\n",
      "step 2/3 | epoch 13/50 | batch 24/60 | global_step 3744 | loss_total 2.7744\n",
      "step 2/3 | epoch 13/50 | batch 25/60 | global_step 3745 | loss_total 2.5829\n",
      "step 2/3 | epoch 13/50 | batch 26/60 | global_step 3746 | loss_total 1.9870\n",
      "step 2/3 | epoch 13/50 | batch 27/60 | global_step 3747 | loss_total 0.7968\n",
      "step 2/3 | epoch 13/50 | batch 28/60 | global_step 3748 | loss_total 0.8719\n",
      "step 2/3 | epoch 13/50 | batch 29/60 | global_step 3749 | loss_total 0.7893\n",
      "step 2/3 | epoch 13/50 | batch 30/60 | global_step 3750 | loss_total 0.7875\n",
      "step 2/3 | epoch 13/50 | batch 31/60 | global_step 3751 | loss_total 0.7430\n",
      "step 2/3 | epoch 13/50 | batch 32/60 | global_step 3752 | loss_total 0.8260\n",
      "step 2/3 | epoch 13/50 | batch 33/60 | global_step 3753 | loss_total 0.7986\n",
      "step 2/3 | epoch 13/50 | batch 34/60 | global_step 3754 | loss_total 0.7856\n",
      "step 2/3 | epoch 13/50 | batch 35/60 | global_step 3755 | loss_total 1.7973\n",
      "step 2/3 | epoch 13/50 | batch 36/60 | global_step 3756 | loss_total 0.8009\n",
      "step 2/3 | epoch 13/50 | batch 37/60 | global_step 3757 | loss_total 2.9229\n",
      "step 2/3 | epoch 13/50 | batch 38/60 | global_step 3758 | loss_total 1.8433\n",
      "step 2/3 | epoch 13/50 | batch 39/60 | global_step 3759 | loss_total 1.7175\n",
      "step 2/3 | epoch 13/50 | batch 40/60 | global_step 3760 | loss_total 0.7942\n",
      "step 2/3 | epoch 13/50 | batch 41/60 | global_step 3761 | loss_total 0.7708\n",
      "step 2/3 | epoch 13/50 | batch 42/60 | global_step 3762 | loss_total 1.4553\n",
      "step 2/3 | epoch 13/50 | batch 43/60 | global_step 3763 | loss_total 1.7440\n",
      "step 2/3 | epoch 13/50 | batch 44/60 | global_step 3764 | loss_total 2.5817\n",
      "step 2/3 | epoch 13/50 | batch 45/60 | global_step 3765 | loss_total 0.7802\n",
      "step 2/3 | epoch 13/50 | batch 46/60 | global_step 3766 | loss_total 0.7861\n",
      "step 2/3 | epoch 13/50 | batch 47/60 | global_step 3767 | loss_total 2.4498\n",
      "step 2/3 | epoch 13/50 | batch 48/60 | global_step 3768 | loss_total 0.7904\n",
      "step 2/3 | epoch 13/50 | batch 49/60 | global_step 3769 | loss_total 0.7940\n",
      "step 2/3 | epoch 13/50 | batch 50/60 | global_step 3770 | loss_total 0.8155\n",
      "step 2/3 | epoch 13/50 | batch 51/60 | global_step 3771 | loss_total 1.5545\n",
      "step 2/3 | epoch 13/50 | batch 52/60 | global_step 3772 | loss_total 0.7905\n",
      "step 2/3 | epoch 13/50 | batch 53/60 | global_step 3773 | loss_total 1.6732\n",
      "step 2/3 | epoch 13/50 | batch 54/60 | global_step 3774 | loss_total 1.5331\n",
      "step 2/3 | epoch 13/50 | batch 55/60 | global_step 3775 | loss_total 1.6625\n",
      "step 2/3 | epoch 13/50 | batch 56/60 | global_step 3776 | loss_total 1.7360\n",
      "step 2/3 | epoch 13/50 | batch 57/60 | global_step 3777 | loss_total 0.8264\n",
      "step 2/3 | epoch 13/50 | batch 58/60 | global_step 3778 | loss_total 1.7133\n",
      "step 2/3 | epoch 13/50 | batch 59/60 | global_step 3779 | loss_total 1.7758\n",
      "step 2/3 | epoch 13/50 | batch 60/60 | global_step 3780 | loss_total 0.7873\n",
      "[epoch done] step 2/3 epoch 13/50 | train_total=1.2935 val_total=0.9656\n",
      "step 2/3 | epoch 14/50 | batch 1/60 | global_step 3781 | loss_total 0.9074\n",
      "step 2/3 | epoch 14/50 | batch 2/60 | global_step 3782 | loss_total 1.5983\n",
      "step 2/3 | epoch 14/50 | batch 3/60 | global_step 3783 | loss_total 0.8177\n",
      "step 2/3 | epoch 14/50 | batch 4/60 | global_step 3784 | loss_total 1.6486\n",
      "step 2/3 | epoch 14/50 | batch 5/60 | global_step 3785 | loss_total 1.1811\n",
      "step 2/3 | epoch 14/50 | batch 6/60 | global_step 3786 | loss_total 0.8073\n",
      "step 2/3 | epoch 14/50 | batch 7/60 | global_step 3787 | loss_total 0.8014\n",
      "step 2/3 | epoch 14/50 | batch 8/60 | global_step 3788 | loss_total 0.8170\n",
      "step 2/3 | epoch 14/50 | batch 9/60 | global_step 3789 | loss_total 3.0109\n",
      "step 2/3 | epoch 14/50 | batch 10/60 | global_step 3790 | loss_total 0.8262\n",
      "step 2/3 | epoch 14/50 | batch 11/60 | global_step 3791 | loss_total 2.2584\n",
      "step 2/3 | epoch 14/50 | batch 12/60 | global_step 3792 | loss_total 0.9591\n",
      "step 2/3 | epoch 14/50 | batch 13/60 | global_step 3793 | loss_total 0.8501\n",
      "step 2/3 | epoch 14/50 | batch 14/60 | global_step 3794 | loss_total 1.5895\n",
      "step 2/3 | epoch 14/50 | batch 15/60 | global_step 3795 | loss_total 0.9272\n",
      "step 2/3 | epoch 14/50 | batch 16/60 | global_step 3796 | loss_total 0.9455\n",
      "step 2/3 | epoch 14/50 | batch 17/60 | global_step 3797 | loss_total 0.9310\n",
      "step 2/3 | epoch 14/50 | batch 18/60 | global_step 3798 | loss_total 0.7953\n",
      "step 2/3 | epoch 14/50 | batch 19/60 | global_step 3799 | loss_total 0.7968\n",
      "step 2/3 | epoch 14/50 | batch 20/60 | global_step 3800 | loss_total 1.6474\n",
      "step 2/3 | epoch 14/50 | batch 21/60 | global_step 3801 | loss_total 0.8929\n",
      "step 2/3 | epoch 14/50 | batch 22/60 | global_step 3802 | loss_total 1.0521\n",
      "step 2/3 | epoch 14/50 | batch 23/60 | global_step 3803 | loss_total 0.8278\n",
      "step 2/3 | epoch 14/50 | batch 24/60 | global_step 3804 | loss_total 0.8359\n",
      "step 2/3 | epoch 14/50 | batch 25/60 | global_step 3805 | loss_total 0.7538\n",
      "step 2/3 | epoch 14/50 | batch 26/60 | global_step 3806 | loss_total 0.7511\n",
      "step 2/3 | epoch 14/50 | batch 27/60 | global_step 3807 | loss_total 0.8123\n",
      "step 2/3 | epoch 14/50 | batch 28/60 | global_step 3808 | loss_total 0.8960\n",
      "step 2/3 | epoch 14/50 | batch 29/60 | global_step 3809 | loss_total 2.5581\n",
      "step 2/3 | epoch 14/50 | batch 30/60 | global_step 3810 | loss_total 1.2249\n",
      "step 2/3 | epoch 14/50 | batch 31/60 | global_step 3811 | loss_total 0.7419\n",
      "step 2/3 | epoch 14/50 | batch 32/60 | global_step 3812 | loss_total 0.8762\n",
      "step 2/3 | epoch 14/50 | batch 33/60 | global_step 3813 | loss_total 1.7988\n",
      "step 2/3 | epoch 14/50 | batch 34/60 | global_step 3814 | loss_total 0.7287\n",
      "step 2/3 | epoch 14/50 | batch 35/60 | global_step 3815 | loss_total 0.7231\n",
      "step 2/3 | epoch 14/50 | batch 36/60 | global_step 3816 | loss_total 0.8012\n",
      "step 2/3 | epoch 14/50 | batch 37/60 | global_step 3817 | loss_total 0.7961\n",
      "step 2/3 | epoch 14/50 | batch 38/60 | global_step 3818 | loss_total 2.3037\n",
      "step 2/3 | epoch 14/50 | batch 39/60 | global_step 3819 | loss_total 1.6331\n",
      "step 2/3 | epoch 14/50 | batch 40/60 | global_step 3820 | loss_total 0.7973\n",
      "step 2/3 | epoch 14/50 | batch 41/60 | global_step 3821 | loss_total 2.7655\n",
      "step 2/3 | epoch 14/50 | batch 42/60 | global_step 3822 | loss_total 0.7988\n",
      "step 2/3 | epoch 14/50 | batch 43/60 | global_step 3823 | loss_total 0.7096\n",
      "step 2/3 | epoch 14/50 | batch 44/60 | global_step 3824 | loss_total 2.3895\n",
      "step 2/3 | epoch 14/50 | batch 45/60 | global_step 3825 | loss_total 0.7997\n",
      "step 2/3 | epoch 14/50 | batch 46/60 | global_step 3826 | loss_total 0.7232\n",
      "step 2/3 | epoch 14/50 | batch 47/60 | global_step 3827 | loss_total 0.9591\n",
      "step 2/3 | epoch 14/50 | batch 48/60 | global_step 3828 | loss_total 0.8048\n",
      "step 2/3 | epoch 14/50 | batch 49/60 | global_step 3829 | loss_total 0.8041\n",
      "step 2/3 | epoch 14/50 | batch 50/60 | global_step 3830 | loss_total 0.9111\n",
      "step 2/3 | epoch 14/50 | batch 51/60 | global_step 3831 | loss_total 0.7141\n",
      "step 2/3 | epoch 14/50 | batch 52/60 | global_step 3832 | loss_total 2.1095\n",
      "step 2/3 | epoch 14/50 | batch 53/60 | global_step 3833 | loss_total 0.7140\n",
      "step 2/3 | epoch 14/50 | batch 54/60 | global_step 3834 | loss_total 0.7952\n",
      "step 2/3 | epoch 14/50 | batch 55/60 | global_step 3835 | loss_total 0.9116\n",
      "step 2/3 | epoch 14/50 | batch 56/60 | global_step 3836 | loss_total 0.7922\n",
      "step 2/3 | epoch 14/50 | batch 57/60 | global_step 3837 | loss_total 1.3839\n",
      "step 2/3 | epoch 14/50 | batch 58/60 | global_step 3838 | loss_total 0.9375\n",
      "step 2/3 | epoch 14/50 | batch 59/60 | global_step 3839 | loss_total 2.7412\n",
      "step 2/3 | epoch 14/50 | batch 60/60 | global_step 3840 | loss_total 0.8634\n",
      "[epoch done] step 2/3 epoch 14/50 | train_total=1.1592 val_total=0.9524\n",
      "step 2/3 | epoch 15/50 | batch 1/60 | global_step 3841 | loss_total 1.6660\n",
      "step 2/3 | epoch 15/50 | batch 2/60 | global_step 3842 | loss_total 0.7926\n",
      "step 2/3 | epoch 15/50 | batch 3/60 | global_step 3843 | loss_total 0.8584\n",
      "step 2/3 | epoch 15/50 | batch 4/60 | global_step 3844 | loss_total 1.7785\n",
      "step 2/3 | epoch 15/50 | batch 5/60 | global_step 3845 | loss_total 0.8514\n",
      "step 2/3 | epoch 15/50 | batch 6/60 | global_step 3846 | loss_total 0.7650\n",
      "step 2/3 | epoch 15/50 | batch 7/60 | global_step 3847 | loss_total 0.8099\n",
      "step 2/3 | epoch 15/50 | batch 8/60 | global_step 3848 | loss_total 1.4675\n",
      "step 2/3 | epoch 15/50 | batch 9/60 | global_step 3849 | loss_total 1.6593\n",
      "step 2/3 | epoch 15/50 | batch 10/60 | global_step 3850 | loss_total 0.8242\n",
      "step 2/3 | epoch 15/50 | batch 11/60 | global_step 3851 | loss_total 0.8471\n",
      "step 2/3 | epoch 15/50 | batch 12/60 | global_step 3852 | loss_total 0.8060\n",
      "step 2/3 | epoch 15/50 | batch 13/60 | global_step 3853 | loss_total 1.0133\n",
      "step 2/3 | epoch 15/50 | batch 14/60 | global_step 3854 | loss_total 0.8055\n",
      "step 2/3 | epoch 15/50 | batch 15/60 | global_step 3855 | loss_total 0.8797\n",
      "step 2/3 | epoch 15/50 | batch 16/60 | global_step 3856 | loss_total 2.0916\n",
      "step 2/3 | epoch 15/50 | batch 17/60 | global_step 3857 | loss_total 0.8125\n",
      "step 2/3 | epoch 15/50 | batch 18/60 | global_step 3858 | loss_total 0.8069\n",
      "step 2/3 | epoch 15/50 | batch 19/60 | global_step 3859 | loss_total 1.4496\n",
      "step 2/3 | epoch 15/50 | batch 20/60 | global_step 3860 | loss_total 2.6521\n",
      "step 2/3 | epoch 15/50 | batch 21/60 | global_step 3861 | loss_total 1.5881\n",
      "step 2/3 | epoch 15/50 | batch 22/60 | global_step 3862 | loss_total 0.8449\n",
      "step 2/3 | epoch 15/50 | batch 23/60 | global_step 3863 | loss_total 1.6497\n",
      "step 2/3 | epoch 15/50 | batch 24/60 | global_step 3864 | loss_total 2.4317\n",
      "step 2/3 | epoch 15/50 | batch 25/60 | global_step 3865 | loss_total 0.8328\n",
      "step 2/3 | epoch 15/50 | batch 26/60 | global_step 3866 | loss_total 1.4738\n",
      "step 2/3 | epoch 15/50 | batch 27/60 | global_step 3867 | loss_total 0.8482\n",
      "step 2/3 | epoch 15/50 | batch 28/60 | global_step 3868 | loss_total 1.1197\n",
      "step 2/3 | epoch 15/50 | batch 29/60 | global_step 3869 | loss_total 1.0579\n",
      "step 2/3 | epoch 15/50 | batch 30/60 | global_step 3870 | loss_total 0.8424\n",
      "step 2/3 | epoch 15/50 | batch 31/60 | global_step 3871 | loss_total 0.8922\n",
      "step 2/3 | epoch 15/50 | batch 32/60 | global_step 3872 | loss_total 1.4019\n",
      "step 2/3 | epoch 15/50 | batch 33/60 | global_step 3873 | loss_total 1.4192\n",
      "step 2/3 | epoch 15/50 | batch 34/60 | global_step 3874 | loss_total 0.8420\n",
      "step 2/3 | epoch 15/50 | batch 35/60 | global_step 3875 | loss_total 0.8285\n",
      "step 2/3 | epoch 15/50 | batch 36/60 | global_step 3876 | loss_total 1.6564\n",
      "step 2/3 | epoch 15/50 | batch 37/60 | global_step 3877 | loss_total 0.8281\n",
      "step 2/3 | epoch 15/50 | batch 38/60 | global_step 3878 | loss_total 1.6183\n",
      "step 2/3 | epoch 15/50 | batch 39/60 | global_step 3879 | loss_total 0.8416\n",
      "step 2/3 | epoch 15/50 | batch 40/60 | global_step 3880 | loss_total 0.8516\n",
      "step 2/3 | epoch 15/50 | batch 41/60 | global_step 3881 | loss_total 1.5771\n",
      "step 2/3 | epoch 15/50 | batch 42/60 | global_step 3882 | loss_total 0.8161\n",
      "step 2/3 | epoch 15/50 | batch 43/60 | global_step 3883 | loss_total 2.0717\n",
      "step 2/3 | epoch 15/50 | batch 44/60 | global_step 3884 | loss_total 0.8386\n",
      "step 2/3 | epoch 15/50 | batch 45/60 | global_step 3885 | loss_total 1.8000\n",
      "step 2/3 | epoch 15/50 | batch 46/60 | global_step 3886 | loss_total 0.8877\n",
      "step 2/3 | epoch 15/50 | batch 47/60 | global_step 3887 | loss_total 1.3910\n",
      "step 2/3 | epoch 15/50 | batch 48/60 | global_step 3888 | loss_total 2.4007\n",
      "step 2/3 | epoch 15/50 | batch 49/60 | global_step 3889 | loss_total 1.4637\n",
      "step 2/3 | epoch 15/50 | batch 50/60 | global_step 3890 | loss_total 1.5433\n",
      "step 2/3 | epoch 15/50 | batch 51/60 | global_step 3891 | loss_total 1.3325\n",
      "step 2/3 | epoch 15/50 | batch 52/60 | global_step 3892 | loss_total 0.8395\n",
      "step 2/3 | epoch 15/50 | batch 53/60 | global_step 3893 | loss_total 0.8547\n",
      "step 2/3 | epoch 15/50 | batch 54/60 | global_step 3894 | loss_total 0.8535\n",
      "step 2/3 | epoch 15/50 | batch 55/60 | global_step 3895 | loss_total 1.4417\n",
      "step 2/3 | epoch 15/50 | batch 56/60 | global_step 3896 | loss_total 1.3601\n",
      "step 2/3 | epoch 15/50 | batch 57/60 | global_step 3897 | loss_total 1.2892\n",
      "step 2/3 | epoch 15/50 | batch 58/60 | global_step 3898 | loss_total 0.8690\n",
      "step 2/3 | epoch 15/50 | batch 59/60 | global_step 3899 | loss_total 0.7233\n",
      "step 2/3 | epoch 15/50 | batch 60/60 | global_step 3900 | loss_total 2.1253\n",
      "[epoch done] step 2/3 epoch 15/50 | train_total=1.2331 val_total=1.1608\n",
      "step 2/3 | epoch 16/50 | batch 1/60 | global_step 3901 | loss_total 1.8790\n",
      "step 2/3 | epoch 16/50 | batch 2/60 | global_step 3902 | loss_total 0.7672\n",
      "step 2/3 | epoch 16/50 | batch 3/60 | global_step 3903 | loss_total 0.8742\n",
      "step 2/3 | epoch 16/50 | batch 4/60 | global_step 3904 | loss_total 0.7485\n",
      "step 2/3 | epoch 16/50 | batch 5/60 | global_step 3905 | loss_total 1.8639\n",
      "step 2/3 | epoch 16/50 | batch 6/60 | global_step 3906 | loss_total 0.7974\n",
      "step 2/3 | epoch 16/50 | batch 7/60 | global_step 3907 | loss_total 1.5872\n",
      "step 2/3 | epoch 16/50 | batch 8/60 | global_step 3908 | loss_total 0.8830\n",
      "step 2/3 | epoch 16/50 | batch 9/60 | global_step 3909 | loss_total 1.3479\n",
      "step 2/3 | epoch 16/50 | batch 10/60 | global_step 3910 | loss_total 0.9854\n",
      "step 2/3 | epoch 16/50 | batch 11/60 | global_step 3911 | loss_total 1.5278\n",
      "step 2/3 | epoch 16/50 | batch 12/60 | global_step 3912 | loss_total 0.8441\n",
      "step 2/3 | epoch 16/50 | batch 13/60 | global_step 3913 | loss_total 1.2579\n",
      "step 2/3 | epoch 16/50 | batch 14/60 | global_step 3914 | loss_total 1.6925\n",
      "step 2/3 | epoch 16/50 | batch 15/60 | global_step 3915 | loss_total 2.1905\n",
      "step 2/3 | epoch 16/50 | batch 16/60 | global_step 3916 | loss_total 1.5233\n",
      "step 2/3 | epoch 16/50 | batch 17/60 | global_step 3917 | loss_total 0.8275\n",
      "step 2/3 | epoch 16/50 | batch 18/60 | global_step 3918 | loss_total 1.3647\n",
      "step 2/3 | epoch 16/50 | batch 19/60 | global_step 3919 | loss_total 0.8254\n",
      "step 2/3 | epoch 16/50 | batch 20/60 | global_step 3920 | loss_total 1.3350\n",
      "step 2/3 | epoch 16/50 | batch 21/60 | global_step 3921 | loss_total 1.4496\n",
      "step 2/3 | epoch 16/50 | batch 22/60 | global_step 3922 | loss_total 1.4736\n",
      "step 2/3 | epoch 16/50 | batch 23/60 | global_step 3923 | loss_total 1.2999\n",
      "step 2/3 | epoch 16/50 | batch 24/60 | global_step 3924 | loss_total 0.8356\n",
      "step 2/3 | epoch 16/50 | batch 25/60 | global_step 3925 | loss_total 0.9098\n",
      "step 2/3 | epoch 16/50 | batch 26/60 | global_step 3926 | loss_total 0.8510\n",
      "step 2/3 | epoch 16/50 | batch 27/60 | global_step 3927 | loss_total 0.7890\n",
      "step 2/3 | epoch 16/50 | batch 28/60 | global_step 3928 | loss_total 0.9876\n",
      "step 2/3 | epoch 16/50 | batch 29/60 | global_step 3929 | loss_total 1.1007\n",
      "step 2/3 | epoch 16/50 | batch 30/60 | global_step 3930 | loss_total 1.5854\n",
      "step 2/3 | epoch 16/50 | batch 31/60 | global_step 3931 | loss_total 0.8178\n",
      "step 2/3 | epoch 16/50 | batch 32/60 | global_step 3932 | loss_total 0.8691\n",
      "step 2/3 | epoch 16/50 | batch 33/60 | global_step 3933 | loss_total 1.3151\n",
      "step 2/3 | epoch 16/50 | batch 34/60 | global_step 3934 | loss_total 1.8936\n",
      "step 2/3 | epoch 16/50 | batch 35/60 | global_step 3935 | loss_total 0.7960\n",
      "step 2/3 | epoch 16/50 | batch 36/60 | global_step 3936 | loss_total 1.6544\n",
      "step 2/3 | epoch 16/50 | batch 37/60 | global_step 3937 | loss_total 0.8643\n",
      "step 2/3 | epoch 16/50 | batch 38/60 | global_step 3938 | loss_total 2.5946\n",
      "step 2/3 | epoch 16/50 | batch 39/60 | global_step 3939 | loss_total 0.8171\n",
      "step 2/3 | epoch 16/50 | batch 40/60 | global_step 3940 | loss_total 1.0148\n",
      "step 2/3 | epoch 16/50 | batch 41/60 | global_step 3941 | loss_total 1.7093\n",
      "step 2/3 | epoch 16/50 | batch 42/60 | global_step 3942 | loss_total 0.8083\n",
      "step 2/3 | epoch 16/50 | batch 43/60 | global_step 3943 | loss_total 1.5208\n",
      "step 2/3 | epoch 16/50 | batch 44/60 | global_step 3944 | loss_total 0.8243\n",
      "step 2/3 | epoch 16/50 | batch 45/60 | global_step 3945 | loss_total 1.6251\n",
      "step 2/3 | epoch 16/50 | batch 46/60 | global_step 3946 | loss_total 0.8161\n",
      "step 2/3 | epoch 16/50 | batch 47/60 | global_step 3947 | loss_total 0.8166\n",
      "step 2/3 | epoch 16/50 | batch 48/60 | global_step 3948 | loss_total 0.8093\n",
      "step 2/3 | epoch 16/50 | batch 49/60 | global_step 3949 | loss_total 0.8256\n",
      "step 2/3 | epoch 16/50 | batch 50/60 | global_step 3950 | loss_total 0.8862\n",
      "step 2/3 | epoch 16/50 | batch 51/60 | global_step 3951 | loss_total 0.8156\n",
      "step 2/3 | epoch 16/50 | batch 52/60 | global_step 3952 | loss_total 0.8234\n",
      "step 2/3 | epoch 16/50 | batch 53/60 | global_step 3953 | loss_total 1.4269\n",
      "step 2/3 | epoch 16/50 | batch 54/60 | global_step 3954 | loss_total 1.5329\n",
      "step 2/3 | epoch 16/50 | batch 55/60 | global_step 3955 | loss_total 0.7999\n",
      "step 2/3 | epoch 16/50 | batch 56/60 | global_step 3956 | loss_total 1.6829\n",
      "step 2/3 | epoch 16/50 | batch 57/60 | global_step 3957 | loss_total 0.7858\n",
      "step 2/3 | epoch 16/50 | batch 58/60 | global_step 3958 | loss_total 0.7805\n",
      "step 2/3 | epoch 16/50 | batch 59/60 | global_step 3959 | loss_total 1.3914\n",
      "step 2/3 | epoch 16/50 | batch 60/60 | global_step 3960 | loss_total 0.8119\n",
      "[epoch done] step 2/3 epoch 16/50 | train_total=1.1756 val_total=0.8728\n",
      "step 2/3 | epoch 17/50 | batch 1/60 | global_step 3961 | loss_total 0.8620\n",
      "step 2/3 | epoch 17/50 | batch 2/60 | global_step 3962 | loss_total 0.8140\n",
      "step 2/3 | epoch 17/50 | batch 3/60 | global_step 3963 | loss_total 0.7604\n",
      "step 2/3 | epoch 17/50 | batch 4/60 | global_step 3964 | loss_total 0.7779\n",
      "step 2/3 | epoch 17/50 | batch 5/60 | global_step 3965 | loss_total 0.8774\n",
      "step 2/3 | epoch 17/50 | batch 6/60 | global_step 3966 | loss_total 1.8545\n",
      "step 2/3 | epoch 17/50 | batch 7/60 | global_step 3967 | loss_total 1.6319\n",
      "step 2/3 | epoch 17/50 | batch 8/60 | global_step 3968 | loss_total 1.5927\n",
      "step 2/3 | epoch 17/50 | batch 9/60 | global_step 3969 | loss_total 0.8090\n",
      "step 2/3 | epoch 17/50 | batch 10/60 | global_step 3970 | loss_total 1.3457\n",
      "step 2/3 | epoch 17/50 | batch 11/60 | global_step 3971 | loss_total 1.4571\n",
      "step 2/3 | epoch 17/50 | batch 12/60 | global_step 3972 | loss_total 1.6672\n",
      "step 2/3 | epoch 17/50 | batch 13/60 | global_step 3973 | loss_total 1.0097\n",
      "step 2/3 | epoch 17/50 | batch 14/60 | global_step 3974 | loss_total 0.8985\n",
      "step 2/3 | epoch 17/50 | batch 15/60 | global_step 3975 | loss_total 1.5788\n",
      "step 2/3 | epoch 17/50 | batch 16/60 | global_step 3976 | loss_total 0.8546\n",
      "step 2/3 | epoch 17/50 | batch 17/60 | global_step 3977 | loss_total 1.5541\n",
      "step 2/3 | epoch 17/50 | batch 18/60 | global_step 3978 | loss_total 0.8500\n",
      "step 2/3 | epoch 17/50 | batch 19/60 | global_step 3979 | loss_total 0.8554\n",
      "step 2/3 | epoch 17/50 | batch 20/60 | global_step 3980 | loss_total 0.8194\n",
      "step 2/3 | epoch 17/50 | batch 21/60 | global_step 3981 | loss_total 1.6138\n",
      "step 2/3 | epoch 17/50 | batch 22/60 | global_step 3982 | loss_total 0.8807\n",
      "step 2/3 | epoch 17/50 | batch 23/60 | global_step 3983 | loss_total 2.1733\n",
      "step 2/3 | epoch 17/50 | batch 24/60 | global_step 3984 | loss_total 1.6947\n",
      "step 2/3 | epoch 17/50 | batch 25/60 | global_step 3985 | loss_total 1.1497\n",
      "step 2/3 | epoch 17/50 | batch 26/60 | global_step 3986 | loss_total 1.7010\n",
      "step 2/3 | epoch 17/50 | batch 27/60 | global_step 3987 | loss_total 0.8542\n",
      "step 2/3 | epoch 17/50 | batch 28/60 | global_step 3988 | loss_total 0.8808\n",
      "step 2/3 | epoch 17/50 | batch 29/60 | global_step 3989 | loss_total 0.8197\n",
      "step 2/3 | epoch 17/50 | batch 30/60 | global_step 3990 | loss_total 0.9106\n",
      "step 2/3 | epoch 17/50 | batch 31/60 | global_step 3991 | loss_total 0.8364\n",
      "step 2/3 | epoch 17/50 | batch 32/60 | global_step 3992 | loss_total 1.1016\n",
      "step 2/3 | epoch 17/50 | batch 33/60 | global_step 3993 | loss_total 0.8025\n",
      "step 2/3 | epoch 17/50 | batch 34/60 | global_step 3994 | loss_total 1.4668\n",
      "step 2/3 | epoch 17/50 | batch 35/60 | global_step 3995 | loss_total 0.8121\n",
      "step 2/3 | epoch 17/50 | batch 36/60 | global_step 3996 | loss_total 1.6377\n",
      "step 2/3 | epoch 17/50 | batch 37/60 | global_step 3997 | loss_total 0.7991\n",
      "step 2/3 | epoch 17/50 | batch 38/60 | global_step 3998 | loss_total 2.3656\n",
      "step 2/3 | epoch 17/50 | batch 39/60 | global_step 3999 | loss_total 0.8232\n",
      "step 2/3 | epoch 17/50 | batch 40/60 | global_step 4000 | loss_total 0.8064\n",
      "step 2/3 | epoch 17/50 | batch 41/60 | global_step 4001 | loss_total 1.4436\n",
      "step 2/3 | epoch 17/50 | batch 42/60 | global_step 4002 | loss_total 0.8006\n",
      "step 2/3 | epoch 17/50 | batch 43/60 | global_step 4003 | loss_total 1.5198\n",
      "step 2/3 | epoch 17/50 | batch 44/60 | global_step 4004 | loss_total 1.6901\n",
      "step 2/3 | epoch 17/50 | batch 45/60 | global_step 4005 | loss_total 1.3847\n",
      "step 2/3 | epoch 17/50 | batch 46/60 | global_step 4006 | loss_total 2.1521\n",
      "step 2/3 | epoch 17/50 | batch 47/60 | global_step 4007 | loss_total 1.9925\n",
      "step 2/3 | epoch 17/50 | batch 48/60 | global_step 4008 | loss_total 1.6484\n",
      "step 2/3 | epoch 17/50 | batch 49/60 | global_step 4009 | loss_total 1.8135\n",
      "step 2/3 | epoch 17/50 | batch 50/60 | global_step 4010 | loss_total 0.8034\n",
      "step 2/3 | epoch 17/50 | batch 51/60 | global_step 4011 | loss_total 0.9014\n",
      "step 2/3 | epoch 17/50 | batch 52/60 | global_step 4012 | loss_total 0.8105\n",
      "step 2/3 | epoch 17/50 | batch 53/60 | global_step 4013 | loss_total 1.3324\n",
      "step 2/3 | epoch 17/50 | batch 54/60 | global_step 4014 | loss_total 0.7736\n",
      "step 2/3 | epoch 17/50 | batch 55/60 | global_step 4015 | loss_total 0.9684\n",
      "step 2/3 | epoch 17/50 | batch 56/60 | global_step 4016 | loss_total 1.6051\n",
      "step 2/3 | epoch 17/50 | batch 57/60 | global_step 4017 | loss_total 0.8813\n",
      "step 2/3 | epoch 17/50 | batch 58/60 | global_step 4018 | loss_total 0.9346\n",
      "step 2/3 | epoch 17/50 | batch 59/60 | global_step 4019 | loss_total 0.8533\n",
      "step 2/3 | epoch 17/50 | batch 60/60 | global_step 4020 | loss_total 1.9781\n",
      "[epoch done] step 2/3 epoch 17/50 | train_total=1.2215 val_total=0.9922\n",
      "step 2/3 | epoch 18/50 | batch 1/60 | global_step 4021 | loss_total 1.5326\n",
      "step 2/3 | epoch 18/50 | batch 2/60 | global_step 4022 | loss_total 0.9624\n",
      "step 2/3 | epoch 18/50 | batch 3/60 | global_step 4023 | loss_total 0.9172\n",
      "step 2/3 | epoch 18/50 | batch 4/60 | global_step 4024 | loss_total 0.9127\n",
      "step 2/3 | epoch 18/50 | batch 5/60 | global_step 4025 | loss_total 1.4256\n",
      "step 2/3 | epoch 18/50 | batch 6/60 | global_step 4026 | loss_total 0.8625\n",
      "step 2/3 | epoch 18/50 | batch 7/60 | global_step 4027 | loss_total 0.8724\n",
      "step 2/3 | epoch 18/50 | batch 8/60 | global_step 4028 | loss_total 0.9013\n",
      "step 2/3 | epoch 18/50 | batch 9/60 | global_step 4029 | loss_total 0.8529\n",
      "step 2/3 | epoch 18/50 | batch 10/60 | global_step 4030 | loss_total 0.9002\n",
      "step 2/3 | epoch 18/50 | batch 11/60 | global_step 4031 | loss_total 1.0867\n",
      "step 2/3 | epoch 18/50 | batch 12/60 | global_step 4032 | loss_total 0.9103\n",
      "step 2/3 | epoch 18/50 | batch 13/60 | global_step 4033 | loss_total 0.8338\n",
      "step 2/3 | epoch 18/50 | batch 14/60 | global_step 4034 | loss_total 0.8249\n",
      "step 2/3 | epoch 18/50 | batch 15/60 | global_step 4035 | loss_total 1.9565\n",
      "step 2/3 | epoch 18/50 | batch 16/60 | global_step 4036 | loss_total 0.8329\n",
      "step 2/3 | epoch 18/50 | batch 17/60 | global_step 4037 | loss_total 1.6165\n",
      "step 2/3 | epoch 18/50 | batch 18/60 | global_step 4038 | loss_total 0.8452\n",
      "step 2/3 | epoch 18/50 | batch 19/60 | global_step 4039 | loss_total 0.8208\n",
      "step 2/3 | epoch 18/50 | batch 20/60 | global_step 4040 | loss_total 0.9025\n",
      "step 2/3 | epoch 18/50 | batch 21/60 | global_step 4041 | loss_total 0.8227\n",
      "step 2/3 | epoch 18/50 | batch 22/60 | global_step 4042 | loss_total 1.6562\n",
      "step 2/3 | epoch 18/50 | batch 23/60 | global_step 4043 | loss_total 1.4940\n",
      "step 2/3 | epoch 18/50 | batch 24/60 | global_step 4044 | loss_total 1.4007\n",
      "step 2/3 | epoch 18/50 | batch 25/60 | global_step 4045 | loss_total 1.3331\n",
      "step 2/3 | epoch 18/50 | batch 26/60 | global_step 4046 | loss_total 1.4122\n",
      "step 2/3 | epoch 18/50 | batch 27/60 | global_step 4047 | loss_total 0.8164\n",
      "step 2/3 | epoch 18/50 | batch 28/60 | global_step 4048 | loss_total 1.5778\n",
      "step 2/3 | epoch 18/50 | batch 29/60 | global_step 4049 | loss_total 0.8097\n",
      "step 2/3 | epoch 18/50 | batch 30/60 | global_step 4050 | loss_total 1.0103\n",
      "step 2/3 | epoch 18/50 | batch 31/60 | global_step 4051 | loss_total 0.9625\n",
      "step 2/3 | epoch 18/50 | batch 32/60 | global_step 4052 | loss_total 0.8350\n",
      "step 2/3 | epoch 18/50 | batch 33/60 | global_step 4053 | loss_total 0.9344\n",
      "step 2/3 | epoch 18/50 | batch 34/60 | global_step 4054 | loss_total 0.7911\n",
      "step 2/3 | epoch 18/50 | batch 35/60 | global_step 4055 | loss_total 0.7725\n",
      "step 2/3 | epoch 18/50 | batch 36/60 | global_step 4056 | loss_total 0.8255\n",
      "step 2/3 | epoch 18/50 | batch 37/60 | global_step 4057 | loss_total 1.8108\n",
      "step 2/3 | epoch 18/50 | batch 38/60 | global_step 4058 | loss_total 1.5697\n",
      "step 2/3 | epoch 18/50 | batch 39/60 | global_step 4059 | loss_total 2.1305\n",
      "step 2/3 | epoch 18/50 | batch 40/60 | global_step 4060 | loss_total 1.2923\n",
      "step 2/3 | epoch 18/50 | batch 41/60 | global_step 4061 | loss_total 1.4622\n",
      "step 2/3 | epoch 18/50 | batch 42/60 | global_step 4062 | loss_total 1.5826\n",
      "step 2/3 | epoch 18/50 | batch 43/60 | global_step 4063 | loss_total 1.3161\n",
      "step 2/3 | epoch 18/50 | batch 44/60 | global_step 4064 | loss_total 0.7796\n",
      "step 2/3 | epoch 18/50 | batch 45/60 | global_step 4065 | loss_total 0.8555\n",
      "step 2/3 | epoch 18/50 | batch 46/60 | global_step 4066 | loss_total 0.7852\n",
      "step 2/3 | epoch 18/50 | batch 47/60 | global_step 4067 | loss_total 0.8972\n",
      "step 2/3 | epoch 18/50 | batch 48/60 | global_step 4068 | loss_total 0.8645\n",
      "step 2/3 | epoch 18/50 | batch 49/60 | global_step 4069 | loss_total 1.9106\n",
      "step 2/3 | epoch 18/50 | batch 50/60 | global_step 4070 | loss_total 0.8546\n",
      "step 2/3 | epoch 18/50 | batch 51/60 | global_step 4071 | loss_total 0.7404\n",
      "step 2/3 | epoch 18/50 | batch 52/60 | global_step 4072 | loss_total 0.8826\n",
      "step 2/3 | epoch 18/50 | batch 53/60 | global_step 4073 | loss_total 0.9896\n",
      "step 2/3 | epoch 18/50 | batch 54/60 | global_step 4074 | loss_total 0.7573\n",
      "step 2/3 | epoch 18/50 | batch 55/60 | global_step 4075 | loss_total 0.9341\n",
      "step 2/3 | epoch 18/50 | batch 56/60 | global_step 4076 | loss_total 1.5839\n",
      "step 2/3 | epoch 18/50 | batch 57/60 | global_step 4077 | loss_total 0.6979\n",
      "step 2/3 | epoch 18/50 | batch 58/60 | global_step 4078 | loss_total 1.5748\n",
      "step 2/3 | epoch 18/50 | batch 59/60 | global_step 4079 | loss_total 0.8326\n",
      "step 2/3 | epoch 18/50 | batch 60/60 | global_step 4080 | loss_total 1.6049\n",
      "[epoch done] step 2/3 epoch 18/50 | train_total=1.1156 val_total=1.0089\n",
      "step 2/3 | epoch 19/50 | batch 1/60 | global_step 4081 | loss_total 1.0702\n",
      "step 2/3 | epoch 19/50 | batch 2/60 | global_step 4082 | loss_total 0.9188\n",
      "step 2/3 | epoch 19/50 | batch 3/60 | global_step 4083 | loss_total 1.7419\n",
      "step 2/3 | epoch 19/50 | batch 4/60 | global_step 4084 | loss_total 0.9016\n",
      "step 2/3 | epoch 19/50 | batch 5/60 | global_step 4085 | loss_total 1.4314\n",
      "step 2/3 | epoch 19/50 | batch 6/60 | global_step 4086 | loss_total 0.7739\n",
      "step 2/3 | epoch 19/50 | batch 7/60 | global_step 4087 | loss_total 0.8222\n",
      "step 2/3 | epoch 19/50 | batch 8/60 | global_step 4088 | loss_total 0.8199\n",
      "step 2/3 | epoch 19/50 | batch 9/60 | global_step 4089 | loss_total 1.6528\n",
      "step 2/3 | epoch 19/50 | batch 10/60 | global_step 4090 | loss_total 2.2109\n",
      "step 2/3 | epoch 19/50 | batch 11/60 | global_step 4091 | loss_total 0.8519\n",
      "step 2/3 | epoch 19/50 | batch 12/60 | global_step 4092 | loss_total 0.8082\n",
      "step 2/3 | epoch 19/50 | batch 13/60 | global_step 4093 | loss_total 0.8300\n",
      "step 2/3 | epoch 19/50 | batch 14/60 | global_step 4094 | loss_total 1.6238\n",
      "step 2/3 | epoch 19/50 | batch 15/60 | global_step 4095 | loss_total 1.6135\n",
      "step 2/3 | epoch 19/50 | batch 16/60 | global_step 4096 | loss_total 1.6745\n",
      "step 2/3 | epoch 19/50 | batch 17/60 | global_step 4097 | loss_total 0.9224\n",
      "step 2/3 | epoch 19/50 | batch 18/60 | global_step 4098 | loss_total 0.8222\n",
      "step 2/3 | epoch 19/50 | batch 19/60 | global_step 4099 | loss_total 2.1120\n",
      "step 2/3 | epoch 19/50 | batch 20/60 | global_step 4100 | loss_total 0.8782\n",
      "step 2/3 | epoch 19/50 | batch 21/60 | global_step 4101 | loss_total 0.8190\n",
      "step 2/3 | epoch 19/50 | batch 22/60 | global_step 4102 | loss_total 0.8444\n",
      "step 2/3 | epoch 19/50 | batch 23/60 | global_step 4103 | loss_total 0.8161\n",
      "step 2/3 | epoch 19/50 | batch 24/60 | global_step 4104 | loss_total 0.8018\n",
      "step 2/3 | epoch 19/50 | batch 25/60 | global_step 4105 | loss_total 1.4952\n",
      "step 2/3 | epoch 19/50 | batch 26/60 | global_step 4106 | loss_total 1.8671\n",
      "step 2/3 | epoch 19/50 | batch 27/60 | global_step 4107 | loss_total 1.6078\n",
      "step 2/3 | epoch 19/50 | batch 28/60 | global_step 4108 | loss_total 0.8241\n",
      "step 2/3 | epoch 19/50 | batch 29/60 | global_step 4109 | loss_total 0.9382\n",
      "step 2/3 | epoch 19/50 | batch 30/60 | global_step 4110 | loss_total 0.8371\n",
      "step 2/3 | epoch 19/50 | batch 31/60 | global_step 4111 | loss_total 0.8432\n",
      "step 2/3 | epoch 19/50 | batch 32/60 | global_step 4112 | loss_total 0.8314\n",
      "step 2/3 | epoch 19/50 | batch 33/60 | global_step 4113 | loss_total 1.4705\n",
      "step 2/3 | epoch 19/50 | batch 34/60 | global_step 4114 | loss_total 0.8154\n",
      "step 2/3 | epoch 19/50 | batch 35/60 | global_step 4115 | loss_total 0.8143\n",
      "step 2/3 | epoch 19/50 | batch 36/60 | global_step 4116 | loss_total 1.7004\n",
      "step 2/3 | epoch 19/50 | batch 37/60 | global_step 4117 | loss_total 1.4254\n",
      "step 2/3 | epoch 19/50 | batch 38/60 | global_step 4118 | loss_total 0.8202\n",
      "step 2/3 | epoch 19/50 | batch 39/60 | global_step 4119 | loss_total 1.4878\n",
      "step 2/3 | epoch 19/50 | batch 40/60 | global_step 4120 | loss_total 0.9146\n",
      "step 2/3 | epoch 19/50 | batch 41/60 | global_step 4121 | loss_total 0.8053\n",
      "step 2/3 | epoch 19/50 | batch 42/60 | global_step 4122 | loss_total 0.8221\n",
      "step 2/3 | epoch 19/50 | batch 43/60 | global_step 4123 | loss_total 0.8176\n",
      "step 2/3 | epoch 19/50 | batch 44/60 | global_step 4124 | loss_total 1.3756\n",
      "step 2/3 | epoch 19/50 | batch 45/60 | global_step 4125 | loss_total 0.8079\n",
      "step 2/3 | epoch 19/50 | batch 46/60 | global_step 4126 | loss_total 0.7710\n",
      "step 2/3 | epoch 19/50 | batch 47/60 | global_step 4127 | loss_total 0.8132\n",
      "step 2/3 | epoch 19/50 | batch 48/60 | global_step 4128 | loss_total 0.9428\n",
      "step 2/3 | epoch 19/50 | batch 49/60 | global_step 4129 | loss_total 2.4992\n",
      "step 2/3 | epoch 19/50 | batch 50/60 | global_step 4130 | loss_total 0.7869\n",
      "step 2/3 | epoch 19/50 | batch 51/60 | global_step 4131 | loss_total 0.7523\n",
      "step 2/3 | epoch 19/50 | batch 52/60 | global_step 4132 | loss_total 0.7508\n",
      "step 2/3 | epoch 19/50 | batch 53/60 | global_step 4133 | loss_total 0.8384\n",
      "step 2/3 | epoch 19/50 | batch 54/60 | global_step 4134 | loss_total 0.7798\n",
      "step 2/3 | epoch 19/50 | batch 55/60 | global_step 4135 | loss_total 0.7424\n",
      "step 2/3 | epoch 19/50 | batch 56/60 | global_step 4136 | loss_total 1.6404\n",
      "step 2/3 | epoch 19/50 | batch 57/60 | global_step 4137 | loss_total 0.8468\n",
      "step 2/3 | epoch 19/50 | batch 58/60 | global_step 4138 | loss_total 0.7864\n",
      "step 2/3 | epoch 19/50 | batch 59/60 | global_step 4139 | loss_total 0.7786\n",
      "step 2/3 | epoch 19/50 | batch 60/60 | global_step 4140 | loss_total 1.7179\n",
      "[epoch done] step 2/3 epoch 19/50 | train_total=1.1088 val_total=0.9024\n",
      "step 2/3 | epoch 20/50 | batch 1/60 | global_step 4141 | loss_total 2.1127\n",
      "step 2/3 | epoch 20/50 | batch 2/60 | global_step 4142 | loss_total 0.7629\n",
      "step 2/3 | epoch 20/50 | batch 3/60 | global_step 4143 | loss_total 0.9218\n",
      "step 2/3 | epoch 20/50 | batch 4/60 | global_step 4144 | loss_total 0.7828\n",
      "step 2/3 | epoch 20/50 | batch 5/60 | global_step 4145 | loss_total 0.9486\n",
      "step 2/3 | epoch 20/50 | batch 6/60 | global_step 4146 | loss_total 1.6868\n",
      "step 2/3 | epoch 20/50 | batch 7/60 | global_step 4147 | loss_total 0.7670\n",
      "step 2/3 | epoch 20/50 | batch 8/60 | global_step 4148 | loss_total 0.7763\n",
      "step 2/3 | epoch 20/50 | batch 9/60 | global_step 4149 | loss_total 0.8360\n",
      "step 2/3 | epoch 20/50 | batch 10/60 | global_step 4150 | loss_total 0.7685\n",
      "step 2/3 | epoch 20/50 | batch 11/60 | global_step 4151 | loss_total 0.7648\n",
      "step 2/3 | epoch 20/50 | batch 12/60 | global_step 4152 | loss_total 0.7376\n",
      "step 2/3 | epoch 20/50 | batch 13/60 | global_step 4153 | loss_total 0.7713\n",
      "step 2/3 | epoch 20/50 | batch 14/60 | global_step 4154 | loss_total 0.9406\n",
      "step 2/3 | epoch 20/50 | batch 15/60 | global_step 4155 | loss_total 1.5714\n",
      "step 2/3 | epoch 20/50 | batch 16/60 | global_step 4156 | loss_total 0.7514\n",
      "step 2/3 | epoch 20/50 | batch 17/60 | global_step 4157 | loss_total 0.9816\n",
      "step 2/3 | epoch 20/50 | batch 18/60 | global_step 4158 | loss_total 0.7603\n",
      "step 2/3 | epoch 20/50 | batch 19/60 | global_step 4159 | loss_total 0.7857\n",
      "step 2/3 | epoch 20/50 | batch 20/60 | global_step 4160 | loss_total 1.3147\n",
      "step 2/3 | epoch 20/50 | batch 21/60 | global_step 4161 | loss_total 0.7783\n",
      "step 2/3 | epoch 20/50 | batch 22/60 | global_step 4162 | loss_total 1.7871\n",
      "step 2/3 | epoch 20/50 | batch 23/60 | global_step 4163 | loss_total 1.7545\n",
      "step 2/3 | epoch 20/50 | batch 24/60 | global_step 4164 | loss_total 1.4843\n",
      "step 2/3 | epoch 20/50 | batch 25/60 | global_step 4165 | loss_total 1.2168\n",
      "step 2/3 | epoch 20/50 | batch 26/60 | global_step 4166 | loss_total 2.2528\n",
      "step 2/3 | epoch 20/50 | batch 27/60 | global_step 4167 | loss_total 1.6855\n",
      "step 2/3 | epoch 20/50 | batch 28/60 | global_step 4168 | loss_total 0.7666\n",
      "step 2/3 | epoch 20/50 | batch 29/60 | global_step 4169 | loss_total 1.6092\n",
      "step 2/3 | epoch 20/50 | batch 30/60 | global_step 4170 | loss_total 0.7802\n",
      "step 2/3 | epoch 20/50 | batch 31/60 | global_step 4171 | loss_total 1.7092\n",
      "step 2/3 | epoch 20/50 | batch 32/60 | global_step 4172 | loss_total 0.9332\n",
      "step 2/3 | epoch 20/50 | batch 33/60 | global_step 4173 | loss_total 0.8133\n",
      "step 2/3 | epoch 20/50 | batch 34/60 | global_step 4174 | loss_total 0.7727\n",
      "step 2/3 | epoch 20/50 | batch 35/60 | global_step 4175 | loss_total 0.7790\n",
      "step 2/3 | epoch 20/50 | batch 36/60 | global_step 4176 | loss_total 0.7774\n",
      "step 2/3 | epoch 20/50 | batch 37/60 | global_step 4177 | loss_total 0.7961\n",
      "step 2/3 | epoch 20/50 | batch 38/60 | global_step 4178 | loss_total 0.7749\n",
      "step 2/3 | epoch 20/50 | batch 39/60 | global_step 4179 | loss_total 2.6581\n",
      "step 2/3 | epoch 20/50 | batch 40/60 | global_step 4180 | loss_total 1.9481\n",
      "step 2/3 | epoch 20/50 | batch 41/60 | global_step 4181 | loss_total 0.7861\n",
      "step 2/3 | epoch 20/50 | batch 42/60 | global_step 4182 | loss_total 0.7779\n",
      "step 2/3 | epoch 20/50 | batch 43/60 | global_step 4183 | loss_total 0.7946\n",
      "step 2/3 | epoch 20/50 | batch 44/60 | global_step 4184 | loss_total 0.7755\n",
      "step 2/3 | epoch 20/50 | batch 45/60 | global_step 4185 | loss_total 1.7924\n",
      "step 2/3 | epoch 20/50 | batch 46/60 | global_step 4186 | loss_total 1.1741\n",
      "step 2/3 | epoch 20/50 | batch 47/60 | global_step 4187 | loss_total 0.8258\n",
      "step 2/3 | epoch 20/50 | batch 48/60 | global_step 4188 | loss_total 0.7749\n",
      "step 2/3 | epoch 20/50 | batch 49/60 | global_step 4189 | loss_total 0.7797\n",
      "step 2/3 | epoch 20/50 | batch 50/60 | global_step 4190 | loss_total 0.7703\n",
      "step 2/3 | epoch 20/50 | batch 51/60 | global_step 4191 | loss_total 0.7982\n",
      "step 2/3 | epoch 20/50 | batch 52/60 | global_step 4192 | loss_total 0.8208\n",
      "step 2/3 | epoch 20/50 | batch 53/60 | global_step 4193 | loss_total 0.7717\n",
      "step 2/3 | epoch 20/50 | batch 54/60 | global_step 4194 | loss_total 1.5942\n",
      "step 2/3 | epoch 20/50 | batch 55/60 | global_step 4195 | loss_total 0.7309\n",
      "step 2/3 | epoch 20/50 | batch 56/60 | global_step 4196 | loss_total 1.4451\n",
      "step 2/3 | epoch 20/50 | batch 57/60 | global_step 4197 | loss_total 0.8264\n",
      "step 2/3 | epoch 20/50 | batch 58/60 | global_step 4198 | loss_total 0.7725\n",
      "step 2/3 | epoch 20/50 | batch 59/60 | global_step 4199 | loss_total 0.7408\n",
      "step 2/3 | epoch 20/50 | batch 60/60 | global_step 4200 | loss_total 0.7029\n",
      "[epoch done] step 2/3 epoch 20/50 | train_total=1.0713 val_total=0.9156\n",
      "step 2/3 | epoch 21/50 | batch 1/60 | global_step 4201 | loss_total 1.6874\n",
      "step 2/3 | epoch 21/50 | batch 2/60 | global_step 4202 | loss_total 0.7154\n",
      "step 2/3 | epoch 21/50 | batch 3/60 | global_step 4203 | loss_total 0.7752\n",
      "step 2/3 | epoch 21/50 | batch 4/60 | global_step 4204 | loss_total 2.5203\n",
      "step 2/3 | epoch 21/50 | batch 5/60 | global_step 4205 | loss_total 1.5667\n",
      "step 2/3 | epoch 21/50 | batch 6/60 | global_step 4206 | loss_total 0.7835\n",
      "step 2/3 | epoch 21/50 | batch 7/60 | global_step 4207 | loss_total 0.7808\n",
      "step 2/3 | epoch 21/50 | batch 8/60 | global_step 4208 | loss_total 0.8439\n",
      "step 2/3 | epoch 21/50 | batch 9/60 | global_step 4209 | loss_total 0.7367\n",
      "step 2/3 | epoch 21/50 | batch 10/60 | global_step 4210 | loss_total 0.6659\n",
      "step 2/3 | epoch 21/50 | batch 11/60 | global_step 4211 | loss_total 1.4857\n",
      "step 2/3 | epoch 21/50 | batch 12/60 | global_step 4212 | loss_total 0.8691\n",
      "step 2/3 | epoch 21/50 | batch 13/60 | global_step 4213 | loss_total 1.3817\n",
      "step 2/3 | epoch 21/50 | batch 14/60 | global_step 4214 | loss_total 0.7620\n",
      "step 2/3 | epoch 21/50 | batch 15/60 | global_step 4215 | loss_total 1.6277\n",
      "step 2/3 | epoch 21/50 | batch 16/60 | global_step 4216 | loss_total 0.7273\n",
      "step 2/3 | epoch 21/50 | batch 17/60 | global_step 4217 | loss_total 0.8510\n",
      "step 2/3 | epoch 21/50 | batch 18/60 | global_step 4218 | loss_total 0.7890\n",
      "step 2/3 | epoch 21/50 | batch 19/60 | global_step 4219 | loss_total 2.3389\n",
      "step 2/3 | epoch 21/50 | batch 20/60 | global_step 4220 | loss_total 0.7882\n",
      "step 2/3 | epoch 21/50 | batch 21/60 | global_step 4221 | loss_total 1.3496\n",
      "step 2/3 | epoch 21/50 | batch 22/60 | global_step 4222 | loss_total 0.8052\n",
      "step 2/3 | epoch 21/50 | batch 23/60 | global_step 4223 | loss_total 0.9147\n",
      "step 2/3 | epoch 21/50 | batch 24/60 | global_step 4224 | loss_total 0.8060\n",
      "step 2/3 | epoch 21/50 | batch 25/60 | global_step 4225 | loss_total 0.7963\n",
      "step 2/3 | epoch 21/50 | batch 26/60 | global_step 4226 | loss_total 0.7636\n",
      "step 2/3 | epoch 21/50 | batch 27/60 | global_step 4227 | loss_total 0.7333\n",
      "step 2/3 | epoch 21/50 | batch 28/60 | global_step 4228 | loss_total 2.8846\n",
      "step 2/3 | epoch 21/50 | batch 29/60 | global_step 4229 | loss_total 1.4143\n",
      "step 2/3 | epoch 21/50 | batch 30/60 | global_step 4230 | loss_total 0.7241\n",
      "step 2/3 | epoch 21/50 | batch 31/60 | global_step 4231 | loss_total 0.7399\n",
      "step 2/3 | epoch 21/50 | batch 32/60 | global_step 4232 | loss_total 0.8116\n",
      "step 2/3 | epoch 21/50 | batch 33/60 | global_step 4233 | loss_total 0.8477\n",
      "step 2/3 | epoch 21/50 | batch 34/60 | global_step 4234 | loss_total 3.4550\n",
      "step 2/3 | epoch 21/50 | batch 35/60 | global_step 4235 | loss_total 1.3991\n",
      "step 2/3 | epoch 21/50 | batch 36/60 | global_step 4236 | loss_total 1.7783\n",
      "step 2/3 | epoch 21/50 | batch 37/60 | global_step 4237 | loss_total 1.7670\n",
      "step 2/3 | epoch 21/50 | batch 38/60 | global_step 4238 | loss_total 0.7954\n",
      "step 2/3 | epoch 21/50 | batch 39/60 | global_step 4239 | loss_total 2.4798\n",
      "step 2/3 | epoch 21/50 | batch 40/60 | global_step 4240 | loss_total 0.8088\n",
      "step 2/3 | epoch 21/50 | batch 41/60 | global_step 4241 | loss_total 0.8025\n",
      "step 2/3 | epoch 21/50 | batch 42/60 | global_step 4242 | loss_total 0.7944\n",
      "step 2/3 | epoch 21/50 | batch 43/60 | global_step 4243 | loss_total 1.4981\n",
      "step 2/3 | epoch 21/50 | batch 44/60 | global_step 4244 | loss_total 0.7970\n",
      "step 2/3 | epoch 21/50 | batch 45/60 | global_step 4245 | loss_total 1.7025\n",
      "step 2/3 | epoch 21/50 | batch 46/60 | global_step 4246 | loss_total 0.7861\n",
      "step 2/3 | epoch 21/50 | batch 47/60 | global_step 4247 | loss_total 0.7821\n",
      "step 2/3 | epoch 21/50 | batch 48/60 | global_step 4248 | loss_total 1.5402\n",
      "step 2/3 | epoch 21/50 | batch 49/60 | global_step 4249 | loss_total 0.8095\n",
      "step 2/3 | epoch 21/50 | batch 50/60 | global_step 4250 | loss_total 0.8426\n",
      "step 2/3 | epoch 21/50 | batch 51/60 | global_step 4251 | loss_total 1.6014\n",
      "step 2/3 | epoch 21/50 | batch 52/60 | global_step 4252 | loss_total 0.8067\n",
      "step 2/3 | epoch 21/50 | batch 53/60 | global_step 4253 | loss_total 0.7473\n",
      "step 2/3 | epoch 21/50 | batch 54/60 | global_step 4254 | loss_total 1.6518\n",
      "step 2/3 | epoch 21/50 | batch 55/60 | global_step 4255 | loss_total 1.6279\n",
      "step 2/3 | epoch 21/50 | batch 56/60 | global_step 4256 | loss_total 1.1055\n",
      "step 2/3 | epoch 21/50 | batch 57/60 | global_step 4257 | loss_total 0.8065\n",
      "step 2/3 | epoch 21/50 | batch 58/60 | global_step 4258 | loss_total 1.0053\n",
      "step 2/3 | epoch 21/50 | batch 59/60 | global_step 4259 | loss_total 2.0441\n",
      "step 2/3 | epoch 21/50 | batch 60/60 | global_step 4260 | loss_total 0.8618\n",
      "[epoch done] step 2/3 epoch 21/50 | train_total=1.1897 val_total=0.7794\n",
      "step 2/3 | epoch 22/50 | batch 1/60 | global_step 4261 | loss_total 1.3889\n",
      "step 2/3 | epoch 22/50 | batch 2/60 | global_step 4262 | loss_total 0.8490\n",
      "step 2/3 | epoch 22/50 | batch 3/60 | global_step 4263 | loss_total 1.5664\n",
      "step 2/3 | epoch 22/50 | batch 4/60 | global_step 4264 | loss_total 1.0743\n",
      "step 2/3 | epoch 22/50 | batch 5/60 | global_step 4265 | loss_total 1.9163\n",
      "step 2/3 | epoch 22/50 | batch 6/60 | global_step 4266 | loss_total 0.8642\n",
      "step 2/3 | epoch 22/50 | batch 7/60 | global_step 4267 | loss_total 2.2081\n",
      "step 2/3 | epoch 22/50 | batch 8/60 | global_step 4268 | loss_total 1.4939\n",
      "step 2/3 | epoch 22/50 | batch 9/60 | global_step 4269 | loss_total 2.5242\n",
      "step 2/3 | epoch 22/50 | batch 10/60 | global_step 4270 | loss_total 1.4687\n",
      "step 2/3 | epoch 22/50 | batch 11/60 | global_step 4271 | loss_total 0.8531\n",
      "step 2/3 | epoch 22/50 | batch 12/60 | global_step 4272 | loss_total 1.4070\n",
      "step 2/3 | epoch 22/50 | batch 13/60 | global_step 4273 | loss_total 1.4394\n",
      "step 2/3 | epoch 22/50 | batch 14/60 | global_step 4274 | loss_total 1.3709\n",
      "step 2/3 | epoch 22/50 | batch 15/60 | global_step 4275 | loss_total 1.3041\n",
      "step 2/3 | epoch 22/50 | batch 16/60 | global_step 4276 | loss_total 2.2579\n",
      "step 2/3 | epoch 22/50 | batch 17/60 | global_step 4277 | loss_total 1.5071\n",
      "step 2/3 | epoch 22/50 | batch 18/60 | global_step 4278 | loss_total 1.4986\n",
      "step 2/3 | epoch 22/50 | batch 19/60 | global_step 4279 | loss_total 0.9668\n",
      "step 2/3 | epoch 22/50 | batch 20/60 | global_step 4280 | loss_total 0.9189\n",
      "step 2/3 | epoch 22/50 | batch 21/60 | global_step 4281 | loss_total 0.9240\n",
      "step 2/3 | epoch 22/50 | batch 22/60 | global_step 4282 | loss_total 1.3588\n",
      "step 2/3 | epoch 22/50 | batch 23/60 | global_step 4283 | loss_total 1.4157\n",
      "step 2/3 | epoch 22/50 | batch 24/60 | global_step 4284 | loss_total 1.7251\n",
      "step 2/3 | epoch 22/50 | batch 25/60 | global_step 4285 | loss_total 0.8513\n",
      "step 2/3 | epoch 22/50 | batch 26/60 | global_step 4286 | loss_total 0.8645\n",
      "step 2/3 | epoch 22/50 | batch 27/60 | global_step 4287 | loss_total 1.2434\n",
      "step 2/3 | epoch 22/50 | batch 28/60 | global_step 4288 | loss_total 1.4410\n",
      "step 2/3 | epoch 22/50 | batch 29/60 | global_step 4289 | loss_total 0.8312\n",
      "step 2/3 | epoch 22/50 | batch 30/60 | global_step 4290 | loss_total 0.8317\n",
      "step 2/3 | epoch 22/50 | batch 31/60 | global_step 4291 | loss_total 0.8038\n",
      "step 2/3 | epoch 22/50 | batch 32/60 | global_step 4292 | loss_total 0.8281\n",
      "step 2/3 | epoch 22/50 | batch 33/60 | global_step 4293 | loss_total 0.8059\n",
      "step 2/3 | epoch 22/50 | batch 34/60 | global_step 4294 | loss_total 0.8987\n",
      "step 2/3 | epoch 22/50 | batch 35/60 | global_step 4295 | loss_total 0.7779\n",
      "step 2/3 | epoch 22/50 | batch 36/60 | global_step 4296 | loss_total 0.7868\n",
      "step 2/3 | epoch 22/50 | batch 37/60 | global_step 4297 | loss_total 0.8014\n",
      "step 2/3 | epoch 22/50 | batch 38/60 | global_step 4298 | loss_total 2.4739\n",
      "step 2/3 | epoch 22/50 | batch 39/60 | global_step 4299 | loss_total 0.7645\n",
      "step 2/3 | epoch 22/50 | batch 40/60 | global_step 4300 | loss_total 0.8189\n",
      "step 2/3 | epoch 22/50 | batch 41/60 | global_step 4301 | loss_total 1.4046\n",
      "step 2/3 | epoch 22/50 | batch 42/60 | global_step 4302 | loss_total 0.8148\n",
      "step 2/3 | epoch 22/50 | batch 43/60 | global_step 4303 | loss_total 0.6920\n",
      "step 2/3 | epoch 22/50 | batch 44/60 | global_step 4304 | loss_total 0.7236\n",
      "step 2/3 | epoch 22/50 | batch 45/60 | global_step 4305 | loss_total 2.3225\n",
      "step 2/3 | epoch 22/50 | batch 46/60 | global_step 4306 | loss_total 1.4028\n",
      "step 2/3 | epoch 22/50 | batch 47/60 | global_step 4307 | loss_total 2.1316\n",
      "step 2/3 | epoch 22/50 | batch 48/60 | global_step 4308 | loss_total 2.0938\n",
      "step 2/3 | epoch 22/50 | batch 49/60 | global_step 4309 | loss_total 1.5563\n",
      "step 2/3 | epoch 22/50 | batch 50/60 | global_step 4310 | loss_total 0.7747\n",
      "step 2/3 | epoch 22/50 | batch 51/60 | global_step 4311 | loss_total 1.1702\n",
      "step 2/3 | epoch 22/50 | batch 52/60 | global_step 4312 | loss_total 1.5180\n",
      "step 2/3 | epoch 22/50 | batch 53/60 | global_step 4313 | loss_total 1.7912\n",
      "step 2/3 | epoch 22/50 | batch 54/60 | global_step 4314 | loss_total 0.8544\n",
      "step 2/3 | epoch 22/50 | batch 55/60 | global_step 4315 | loss_total 0.8351\n",
      "step 2/3 | epoch 22/50 | batch 56/60 | global_step 4316 | loss_total 0.7734\n",
      "step 2/3 | epoch 22/50 | batch 57/60 | global_step 4317 | loss_total 0.8169\n",
      "step 2/3 | epoch 22/50 | batch 58/60 | global_step 4318 | loss_total 0.9211\n",
      "step 2/3 | epoch 22/50 | batch 59/60 | global_step 4319 | loss_total 1.5929\n",
      "step 2/3 | epoch 22/50 | batch 60/60 | global_step 4320 | loss_total 0.8805\n",
      "[epoch done] step 2/3 epoch 22/50 | train_total=1.2532 val_total=0.9378\n",
      "step 2/3 | epoch 23/50 | batch 1/60 | global_step 4321 | loss_total 0.9318\n",
      "step 2/3 | epoch 23/50 | batch 2/60 | global_step 4322 | loss_total 1.4080\n",
      "step 2/3 | epoch 23/50 | batch 3/60 | global_step 4323 | loss_total 0.8223\n",
      "step 2/3 | epoch 23/50 | batch 4/60 | global_step 4324 | loss_total 0.8077\n",
      "step 2/3 | epoch 23/50 | batch 5/60 | global_step 4325 | loss_total 0.8661\n",
      "step 2/3 | epoch 23/50 | batch 6/60 | global_step 4326 | loss_total 1.4247\n",
      "step 2/3 | epoch 23/50 | batch 7/60 | global_step 4327 | loss_total 2.6576\n",
      "step 2/3 | epoch 23/50 | batch 8/60 | global_step 4328 | loss_total 0.7242\n",
      "step 2/3 | epoch 23/50 | batch 9/60 | global_step 4329 | loss_total 0.8296\n",
      "step 2/3 | epoch 23/50 | batch 10/60 | global_step 4330 | loss_total 1.4808\n",
      "step 2/3 | epoch 23/50 | batch 11/60 | global_step 4331 | loss_total 0.7595\n",
      "step 2/3 | epoch 23/50 | batch 12/60 | global_step 4332 | loss_total 1.5172\n",
      "step 2/3 | epoch 23/50 | batch 13/60 | global_step 4333 | loss_total 1.5280\n",
      "step 2/3 | epoch 23/50 | batch 14/60 | global_step 4334 | loss_total 0.7436\n",
      "step 2/3 | epoch 23/50 | batch 15/60 | global_step 4335 | loss_total 0.7530\n",
      "step 2/3 | epoch 23/50 | batch 16/60 | global_step 4336 | loss_total 0.8180\n",
      "step 2/3 | epoch 23/50 | batch 17/60 | global_step 4337 | loss_total 0.7756\n",
      "step 2/3 | epoch 23/50 | batch 18/60 | global_step 4338 | loss_total 0.7916\n",
      "step 2/3 | epoch 23/50 | batch 19/60 | global_step 4339 | loss_total 0.8004\n",
      "step 2/3 | epoch 23/50 | batch 20/60 | global_step 4340 | loss_total 2.6038\n",
      "step 2/3 | epoch 23/50 | batch 21/60 | global_step 4341 | loss_total 0.7887\n",
      "step 2/3 | epoch 23/50 | batch 22/60 | global_step 4342 | loss_total 0.9019\n",
      "step 2/3 | epoch 23/50 | batch 23/60 | global_step 4343 | loss_total 0.8095\n",
      "step 2/3 | epoch 23/50 | batch 24/60 | global_step 4344 | loss_total 0.8713\n",
      "step 2/3 | epoch 23/50 | batch 25/60 | global_step 4345 | loss_total 0.7982\n",
      "step 2/3 | epoch 23/50 | batch 26/60 | global_step 4346 | loss_total 0.8200\n",
      "step 2/3 | epoch 23/50 | batch 27/60 | global_step 4347 | loss_total 1.6788\n",
      "step 2/3 | epoch 23/50 | batch 28/60 | global_step 4348 | loss_total 0.8449\n",
      "step 2/3 | epoch 23/50 | batch 29/60 | global_step 4349 | loss_total 1.6582\n",
      "step 2/3 | epoch 23/50 | batch 30/60 | global_step 4350 | loss_total 0.8726\n",
      "step 2/3 | epoch 23/50 | batch 31/60 | global_step 4351 | loss_total 0.9892\n",
      "step 2/3 | epoch 23/50 | batch 32/60 | global_step 4352 | loss_total 0.8911\n",
      "step 2/3 | epoch 23/50 | batch 33/60 | global_step 4353 | loss_total 1.1010\n",
      "step 2/3 | epoch 23/50 | batch 34/60 | global_step 4354 | loss_total 2.4902\n",
      "step 2/3 | epoch 23/50 | batch 35/60 | global_step 4355 | loss_total 1.0127\n",
      "step 2/3 | epoch 23/50 | batch 36/60 | global_step 4356 | loss_total 0.9260\n",
      "step 2/3 | epoch 23/50 | batch 37/60 | global_step 4357 | loss_total 1.7296\n",
      "step 2/3 | epoch 23/50 | batch 38/60 | global_step 4358 | loss_total 1.7130\n",
      "step 2/3 | epoch 23/50 | batch 39/60 | global_step 4359 | loss_total 0.9894\n",
      "step 2/3 | epoch 23/50 | batch 40/60 | global_step 4360 | loss_total 1.0548\n",
      "step 2/3 | epoch 23/50 | batch 41/60 | global_step 4361 | loss_total 1.9114\n",
      "step 2/3 | epoch 23/50 | batch 42/60 | global_step 4362 | loss_total 1.7029\n",
      "step 2/3 | epoch 23/50 | batch 43/60 | global_step 4363 | loss_total 1.6479\n",
      "step 2/3 | epoch 23/50 | batch 44/60 | global_step 4364 | loss_total 2.0257\n",
      "step 2/3 | epoch 23/50 | batch 45/60 | global_step 4365 | loss_total 1.8990\n",
      "step 2/3 | epoch 23/50 | batch 46/60 | global_step 4366 | loss_total 1.3863\n",
      "step 2/3 | epoch 23/50 | batch 47/60 | global_step 4367 | loss_total 1.3743\n",
      "step 2/3 | epoch 23/50 | batch 48/60 | global_step 4368 | loss_total 1.0322\n",
      "step 2/3 | epoch 23/50 | batch 49/60 | global_step 4369 | loss_total 1.1180\n",
      "step 2/3 | epoch 23/50 | batch 50/60 | global_step 4370 | loss_total 1.6778\n",
      "step 2/3 | epoch 23/50 | batch 51/60 | global_step 4371 | loss_total 1.3131\n",
      "step 2/3 | epoch 23/50 | batch 52/60 | global_step 4372 | loss_total 1.1408\n",
      "step 2/3 | epoch 23/50 | batch 53/60 | global_step 4373 | loss_total 1.1151\n",
      "step 2/3 | epoch 23/50 | batch 54/60 | global_step 4374 | loss_total 1.5168\n",
      "step 2/3 | epoch 23/50 | batch 55/60 | global_step 4375 | loss_total 1.2077\n",
      "step 2/3 | epoch 23/50 | batch 56/60 | global_step 4376 | loss_total 1.1411\n",
      "step 2/3 | epoch 23/50 | batch 57/60 | global_step 4377 | loss_total 1.0259\n",
      "step 2/3 | epoch 23/50 | batch 58/60 | global_step 4378 | loss_total 1.9864\n",
      "step 2/3 | epoch 23/50 | batch 59/60 | global_step 4379 | loss_total 1.1647\n",
      "step 2/3 | epoch 23/50 | batch 60/60 | global_step 4380 | loss_total 1.0880\n",
      "[epoch done] step 2/3 epoch 23/50 | train_total=1.2410 val_total=1.0242\n",
      "step 2/3 | epoch 24/50 | batch 1/60 | global_step 4381 | loss_total 1.1427\n",
      "step 2/3 | epoch 24/50 | batch 2/60 | global_step 4382 | loss_total 0.9971\n",
      "step 2/3 | epoch 24/50 | batch 3/60 | global_step 4383 | loss_total 1.1864\n",
      "step 2/3 | epoch 24/50 | batch 4/60 | global_step 4384 | loss_total 1.0086\n",
      "step 2/3 | epoch 24/50 | batch 5/60 | global_step 4385 | loss_total 1.9541\n",
      "step 2/3 | epoch 24/50 | batch 6/60 | global_step 4386 | loss_total 2.5719\n",
      "step 2/3 | epoch 24/50 | batch 7/60 | global_step 4387 | loss_total 1.2406\n",
      "step 2/3 | epoch 24/50 | batch 8/60 | global_step 4388 | loss_total 1.7515\n",
      "step 2/3 | epoch 24/50 | batch 9/60 | global_step 4389 | loss_total 1.2103\n",
      "step 2/3 | epoch 24/50 | batch 10/60 | global_step 4390 | loss_total 1.7356\n",
      "step 2/3 | epoch 24/50 | batch 11/60 | global_step 4391 | loss_total 1.1648\n",
      "step 2/3 | epoch 24/50 | batch 12/60 | global_step 4392 | loss_total 1.1901\n",
      "step 2/3 | epoch 24/50 | batch 13/60 | global_step 4393 | loss_total 1.6190\n",
      "step 2/3 | epoch 24/50 | batch 14/60 | global_step 4394 | loss_total 1.1502\n",
      "step 2/3 | epoch 24/50 | batch 15/60 | global_step 4395 | loss_total 0.8170\n",
      "step 2/3 | epoch 24/50 | batch 16/60 | global_step 4396 | loss_total 1.1411\n",
      "step 2/3 | epoch 24/50 | batch 17/60 | global_step 4397 | loss_total 1.4892\n",
      "step 2/3 | epoch 24/50 | batch 18/60 | global_step 4398 | loss_total 1.9736\n",
      "step 2/3 | epoch 24/50 | batch 19/60 | global_step 4399 | loss_total 0.9699\n",
      "step 2/3 | epoch 24/50 | batch 20/60 | global_step 4400 | loss_total 1.1463\n",
      "step 2/3 | epoch 24/50 | batch 21/60 | global_step 4401 | loss_total 1.1652\n",
      "step 2/3 | epoch 24/50 | batch 22/60 | global_step 4402 | loss_total 0.6902\n",
      "step 2/3 | epoch 24/50 | batch 23/60 | global_step 4403 | loss_total 1.7033\n",
      "step 2/3 | epoch 24/50 | batch 24/60 | global_step 4404 | loss_total 1.8070\n",
      "step 2/3 | epoch 24/50 | batch 25/60 | global_step 4405 | loss_total 1.1524\n",
      "step 2/3 | epoch 24/50 | batch 26/60 | global_step 4406 | loss_total 1.1980\n",
      "step 2/3 | epoch 24/50 | batch 27/60 | global_step 4407 | loss_total 1.0661\n",
      "step 2/3 | epoch 24/50 | batch 28/60 | global_step 4408 | loss_total 1.0114\n",
      "step 2/3 | epoch 24/50 | batch 29/60 | global_step 4409 | loss_total 1.0956\n",
      "step 2/3 | epoch 24/50 | batch 30/60 | global_step 4410 | loss_total 2.2021\n",
      "step 2/3 | epoch 24/50 | batch 31/60 | global_step 4411 | loss_total 1.3679\n",
      "step 2/3 | epoch 24/50 | batch 32/60 | global_step 4412 | loss_total 1.0507\n",
      "step 2/3 | epoch 24/50 | batch 33/60 | global_step 4413 | loss_total 0.9368\n",
      "step 2/3 | epoch 24/50 | batch 34/60 | global_step 4414 | loss_total 1.8666\n",
      "step 2/3 | epoch 24/50 | batch 35/60 | global_step 4415 | loss_total 0.9625\n",
      "step 2/3 | epoch 24/50 | batch 36/60 | global_step 4416 | loss_total 0.8773\n",
      "step 2/3 | epoch 24/50 | batch 37/60 | global_step 4417 | loss_total 1.3384\n",
      "step 2/3 | epoch 24/50 | batch 38/60 | global_step 4418 | loss_total 1.3719\n",
      "step 2/3 | epoch 24/50 | batch 39/60 | global_step 4419 | loss_total 1.7512\n",
      "step 2/3 | epoch 24/50 | batch 40/60 | global_step 4420 | loss_total 0.8685\n",
      "step 2/3 | epoch 24/50 | batch 41/60 | global_step 4421 | loss_total 1.1762\n",
      "step 2/3 | epoch 24/50 | batch 42/60 | global_step 4422 | loss_total 0.9848\n",
      "step 2/3 | epoch 24/50 | batch 43/60 | global_step 4423 | loss_total 0.7865\n",
      "step 2/3 | epoch 24/50 | batch 44/60 | global_step 4424 | loss_total 0.8910\n",
      "step 2/3 | epoch 24/50 | batch 45/60 | global_step 4425 | loss_total 0.7143\n",
      "step 2/3 | epoch 24/50 | batch 46/60 | global_step 4426 | loss_total 2.3720\n",
      "step 2/3 | epoch 24/50 | batch 47/60 | global_step 4427 | loss_total 0.8048\n",
      "step 2/3 | epoch 24/50 | batch 48/60 | global_step 4428 | loss_total 0.8437\n",
      "step 2/3 | epoch 24/50 | batch 49/60 | global_step 4429 | loss_total 1.9769\n",
      "step 2/3 | epoch 24/50 | batch 50/60 | global_step 4430 | loss_total 0.8845\n",
      "step 2/3 | epoch 24/50 | batch 51/60 | global_step 4431 | loss_total 1.3420\n",
      "step 2/3 | epoch 24/50 | batch 52/60 | global_step 4432 | loss_total 1.5281\n",
      "step 2/3 | epoch 24/50 | batch 53/60 | global_step 4433 | loss_total 0.7479\n",
      "step 2/3 | epoch 24/50 | batch 54/60 | global_step 4434 | loss_total 0.6823\n",
      "step 2/3 | epoch 24/50 | batch 55/60 | global_step 4435 | loss_total 0.7442\n",
      "step 2/3 | epoch 24/50 | batch 56/60 | global_step 4436 | loss_total 0.6918\n",
      "step 2/3 | epoch 24/50 | batch 57/60 | global_step 4437 | loss_total 0.8765\n",
      "step 2/3 | epoch 24/50 | batch 58/60 | global_step 4438 | loss_total 1.5562\n",
      "step 2/3 | epoch 24/50 | batch 59/60 | global_step 4439 | loss_total 0.9789\n",
      "step 2/3 | epoch 24/50 | batch 60/60 | global_step 4440 | loss_total 0.5851\n",
      "[epoch done] step 2/3 epoch 24/50 | train_total=1.2352 val_total=0.8857\n",
      "step 2/3 | epoch 25/50 | batch 1/60 | global_step 4441 | loss_total 1.6532\n",
      "step 2/3 | epoch 25/50 | batch 2/60 | global_step 4442 | loss_total 0.8272\n",
      "step 2/3 | epoch 25/50 | batch 3/60 | global_step 4443 | loss_total 1.3910\n",
      "step 2/3 | epoch 25/50 | batch 4/60 | global_step 4444 | loss_total 0.5447\n",
      "step 2/3 | epoch 25/50 | batch 5/60 | global_step 4445 | loss_total 0.7167\n",
      "step 2/3 | epoch 25/50 | batch 6/60 | global_step 4446 | loss_total 0.5529\n",
      "step 2/3 | epoch 25/50 | batch 7/60 | global_step 4447 | loss_total 1.0739\n",
      "step 2/3 | epoch 25/50 | batch 8/60 | global_step 4448 | loss_total 1.2527\n",
      "step 2/3 | epoch 25/50 | batch 9/60 | global_step 4449 | loss_total 0.5203\n",
      "step 2/3 | epoch 25/50 | batch 10/60 | global_step 4450 | loss_total 0.5231\n",
      "step 2/3 | epoch 25/50 | batch 11/60 | global_step 4451 | loss_total 0.5351\n",
      "step 2/3 | epoch 25/50 | batch 12/60 | global_step 4452 | loss_total 1.3077\n",
      "step 2/3 | epoch 25/50 | batch 13/60 | global_step 4453 | loss_total 1.5748\n",
      "step 2/3 | epoch 25/50 | batch 14/60 | global_step 4454 | loss_total 0.9713\n",
      "step 2/3 | epoch 25/50 | batch 15/60 | global_step 4455 | loss_total 1.3942\n",
      "step 2/3 | epoch 25/50 | batch 16/60 | global_step 4456 | loss_total 0.7106\n",
      "step 2/3 | epoch 25/50 | batch 17/60 | global_step 4457 | loss_total 1.5731\n",
      "step 2/3 | epoch 25/50 | batch 18/60 | global_step 4458 | loss_total 1.1242\n",
      "step 2/3 | epoch 25/50 | batch 19/60 | global_step 4459 | loss_total 0.8110\n",
      "step 2/3 | epoch 25/50 | batch 20/60 | global_step 4460 | loss_total 1.0483\n",
      "step 2/3 | epoch 25/50 | batch 21/60 | global_step 4461 | loss_total 0.7961\n",
      "step 2/3 | epoch 25/50 | batch 22/60 | global_step 4462 | loss_total 0.8192\n",
      "step 2/3 | epoch 25/50 | batch 23/60 | global_step 4463 | loss_total 1.3160\n",
      "step 2/3 | epoch 25/50 | batch 24/60 | global_step 4464 | loss_total 0.5939\n",
      "step 2/3 | epoch 25/50 | batch 25/60 | global_step 4465 | loss_total 0.7989\n",
      "step 2/3 | epoch 25/50 | batch 26/60 | global_step 4466 | loss_total 1.6897\n",
      "step 2/3 | epoch 25/50 | batch 27/60 | global_step 4467 | loss_total 0.7316\n",
      "step 2/3 | epoch 25/50 | batch 28/60 | global_step 4468 | loss_total 1.5347\n",
      "step 2/3 | epoch 25/50 | batch 29/60 | global_step 4469 | loss_total 1.0292\n",
      "step 2/3 | epoch 25/50 | batch 30/60 | global_step 4470 | loss_total 1.6429\n",
      "step 2/3 | epoch 25/50 | batch 31/60 | global_step 4471 | loss_total 0.5841\n",
      "step 2/3 | epoch 25/50 | batch 32/60 | global_step 4472 | loss_total 0.9440\n",
      "step 2/3 | epoch 25/50 | batch 33/60 | global_step 4473 | loss_total 2.1927\n",
      "step 2/3 | epoch 25/50 | batch 34/60 | global_step 4474 | loss_total 0.7882\n",
      "step 2/3 | epoch 25/50 | batch 35/60 | global_step 4475 | loss_total 1.5576\n",
      "step 2/3 | epoch 25/50 | batch 36/60 | global_step 4476 | loss_total 0.5931\n",
      "step 2/3 | epoch 25/50 | batch 37/60 | global_step 4477 | loss_total 0.6136\n",
      "step 2/3 | epoch 25/50 | batch 38/60 | global_step 4478 | loss_total 0.9174\n",
      "step 2/3 | epoch 25/50 | batch 39/60 | global_step 4479 | loss_total 1.5441\n",
      "step 2/3 | epoch 25/50 | batch 40/60 | global_step 4480 | loss_total 1.2033\n",
      "step 2/3 | epoch 25/50 | batch 41/60 | global_step 4481 | loss_total 0.5990\n",
      "step 2/3 | epoch 25/50 | batch 42/60 | global_step 4482 | loss_total 0.6188\n",
      "step 2/3 | epoch 25/50 | batch 43/60 | global_step 4483 | loss_total 1.3777\n",
      "step 2/3 | epoch 25/50 | batch 44/60 | global_step 4484 | loss_total 1.3110\n",
      "step 2/3 | epoch 25/50 | batch 45/60 | global_step 4485 | loss_total 0.9489\n",
      "step 2/3 | epoch 25/50 | batch 46/60 | global_step 4486 | loss_total 0.7543\n",
      "step 2/3 | epoch 25/50 | batch 47/60 | global_step 4487 | loss_total 2.1970\n",
      "step 2/3 | epoch 25/50 | batch 48/60 | global_step 4488 | loss_total 0.6318\n",
      "step 2/3 | epoch 25/50 | batch 49/60 | global_step 4489 | loss_total 1.8885\n",
      "step 2/3 | epoch 25/50 | batch 50/60 | global_step 4490 | loss_total 1.0810\n",
      "step 2/3 | epoch 25/50 | batch 51/60 | global_step 4491 | loss_total 0.6480\n",
      "step 2/3 | epoch 25/50 | batch 52/60 | global_step 4492 | loss_total 0.6422\n",
      "step 2/3 | epoch 25/50 | batch 53/60 | global_step 4493 | loss_total 0.9234\n",
      "step 2/3 | epoch 25/50 | batch 54/60 | global_step 4494 | loss_total 0.6307\n",
      "step 2/3 | epoch 25/50 | batch 55/60 | global_step 4495 | loss_total 0.9044\n",
      "step 2/3 | epoch 25/50 | batch 56/60 | global_step 4496 | loss_total 0.8344\n",
      "step 2/3 | epoch 25/50 | batch 57/60 | global_step 4497 | loss_total 2.1102\n",
      "step 2/3 | epoch 25/50 | batch 58/60 | global_step 4498 | loss_total 1.4392\n",
      "step 2/3 | epoch 25/50 | batch 59/60 | global_step 4499 | loss_total 0.6526\n",
      "step 2/3 | epoch 25/50 | batch 60/60 | global_step 4500 | loss_total 1.3106\n",
      "[epoch done] step 2/3 epoch 25/50 | train_total=1.0583 val_total=1.0117\n",
      "step 2/3 | epoch 26/50 | batch 1/60 | global_step 4501 | loss_total 1.3723\n",
      "step 2/3 | epoch 26/50 | batch 2/60 | global_step 4502 | loss_total 1.3467\n",
      "step 2/3 | epoch 26/50 | batch 3/60 | global_step 4503 | loss_total 0.6451\n",
      "step 2/3 | epoch 26/50 | batch 4/60 | global_step 4504 | loss_total 2.1924\n",
      "step 2/3 | epoch 26/50 | batch 5/60 | global_step 4505 | loss_total 0.9202\n",
      "step 2/3 | epoch 26/50 | batch 6/60 | global_step 4506 | loss_total 0.9221\n",
      "step 2/3 | epoch 26/50 | batch 7/60 | global_step 4507 | loss_total 0.6416\n",
      "step 2/3 | epoch 26/50 | batch 8/60 | global_step 4508 | loss_total 0.6819\n",
      "step 2/3 | epoch 26/50 | batch 9/60 | global_step 4509 | loss_total 1.7021\n",
      "step 2/3 | epoch 26/50 | batch 10/60 | global_step 4510 | loss_total 0.9168\n",
      "step 2/3 | epoch 26/50 | batch 11/60 | global_step 4511 | loss_total 0.6635\n",
      "step 2/3 | epoch 26/50 | batch 12/60 | global_step 4512 | loss_total 0.7803\n",
      "step 2/3 | epoch 26/50 | batch 13/60 | global_step 4513 | loss_total 1.6919\n",
      "step 2/3 | epoch 26/50 | batch 14/60 | global_step 4514 | loss_total 0.6613\n",
      "step 2/3 | epoch 26/50 | batch 15/60 | global_step 4515 | loss_total 1.7793\n",
      "step 2/3 | epoch 26/50 | batch 16/60 | global_step 4516 | loss_total 1.1738\n",
      "step 2/3 | epoch 26/50 | batch 17/60 | global_step 4517 | loss_total 2.0832\n",
      "step 2/3 | epoch 26/50 | batch 18/60 | global_step 4518 | loss_total 1.0058\n",
      "step 2/3 | epoch 26/50 | batch 19/60 | global_step 4519 | loss_total 0.6099\n",
      "step 2/3 | epoch 26/50 | batch 20/60 | global_step 4520 | loss_total 0.8174\n",
      "step 2/3 | epoch 26/50 | batch 21/60 | global_step 4521 | loss_total 1.6674\n",
      "step 2/3 | epoch 26/50 | batch 22/60 | global_step 4522 | loss_total 0.7070\n",
      "step 2/3 | epoch 26/50 | batch 23/60 | global_step 4523 | loss_total 0.8248\n",
      "step 2/3 | epoch 26/50 | batch 24/60 | global_step 4524 | loss_total 0.6426\n",
      "step 2/3 | epoch 26/50 | batch 25/60 | global_step 4525 | loss_total 0.6177\n",
      "step 2/3 | epoch 26/50 | batch 26/60 | global_step 4526 | loss_total 1.6047\n",
      "step 2/3 | epoch 26/50 | batch 27/60 | global_step 4527 | loss_total 1.5936\n",
      "step 2/3 | epoch 26/50 | batch 28/60 | global_step 4528 | loss_total 1.4723\n",
      "step 2/3 | epoch 26/50 | batch 29/60 | global_step 4529 | loss_total 1.5149\n",
      "step 2/3 | epoch 26/50 | batch 30/60 | global_step 4530 | loss_total 0.7227\n",
      "step 2/3 | epoch 26/50 | batch 31/60 | global_step 4531 | loss_total 1.7676\n",
      "step 2/3 | epoch 26/50 | batch 32/60 | global_step 4532 | loss_total 0.7383\n",
      "step 2/3 | epoch 26/50 | batch 33/60 | global_step 4533 | loss_total 1.5181\n",
      "step 2/3 | epoch 26/50 | batch 34/60 | global_step 4534 | loss_total 0.7328\n",
      "step 2/3 | epoch 26/50 | batch 35/60 | global_step 4535 | loss_total 1.6385\n",
      "step 2/3 | epoch 26/50 | batch 36/60 | global_step 4536 | loss_total 0.9020\n",
      "step 2/3 | epoch 26/50 | batch 37/60 | global_step 4537 | loss_total 0.9099\n",
      "step 2/3 | epoch 26/50 | batch 38/60 | global_step 4538 | loss_total 1.5774\n",
      "step 2/3 | epoch 26/50 | batch 39/60 | global_step 4539 | loss_total 1.8932\n",
      "step 2/3 | epoch 26/50 | batch 40/60 | global_step 4540 | loss_total 1.0581\n",
      "step 2/3 | epoch 26/50 | batch 41/60 | global_step 4541 | loss_total 1.0589\n",
      "step 2/3 | epoch 26/50 | batch 42/60 | global_step 4542 | loss_total 1.1344\n",
      "step 2/3 | epoch 26/50 | batch 43/60 | global_step 4543 | loss_total 1.9851\n",
      "step 2/3 | epoch 26/50 | batch 44/60 | global_step 4544 | loss_total 1.0013\n",
      "step 2/3 | epoch 26/50 | batch 45/60 | global_step 4545 | loss_total 0.8355\n",
      "step 2/3 | epoch 26/50 | batch 46/60 | global_step 4546 | loss_total 1.0017\n",
      "step 2/3 | epoch 26/50 | batch 47/60 | global_step 4547 | loss_total 1.3040\n",
      "step 2/3 | epoch 26/50 | batch 48/60 | global_step 4548 | loss_total 0.8647\n",
      "step 2/3 | epoch 26/50 | batch 49/60 | global_step 4549 | loss_total 1.4722\n",
      "step 2/3 | epoch 26/50 | batch 50/60 | global_step 4550 | loss_total 1.5121\n",
      "step 2/3 | epoch 26/50 | batch 51/60 | global_step 4551 | loss_total 1.5860\n",
      "step 2/3 | epoch 26/50 | batch 52/60 | global_step 4552 | loss_total 1.0671\n",
      "step 2/3 | epoch 26/50 | batch 53/60 | global_step 4553 | loss_total 0.8073\n",
      "step 2/3 | epoch 26/50 | batch 54/60 | global_step 4554 | loss_total 1.5463\n",
      "step 2/3 | epoch 26/50 | batch 55/60 | global_step 4555 | loss_total 0.8134\n",
      "step 2/3 | epoch 26/50 | batch 56/60 | global_step 4556 | loss_total 0.8449\n",
      "step 2/3 | epoch 26/50 | batch 57/60 | global_step 4557 | loss_total 0.8513\n",
      "step 2/3 | epoch 26/50 | batch 58/60 | global_step 4558 | loss_total 0.8236\n",
      "step 2/3 | epoch 26/50 | batch 59/60 | global_step 4559 | loss_total 0.9404\n",
      "step 2/3 | epoch 26/50 | batch 60/60 | global_step 4560 | loss_total 0.8490\n",
      "[epoch done] step 2/3 epoch 26/50 | train_total=1.1502 val_total=0.8724\n",
      "step 2/3 | epoch 27/50 | batch 1/60 | global_step 4561 | loss_total 0.8406\n",
      "step 2/3 | epoch 27/50 | batch 2/60 | global_step 4562 | loss_total 0.8137\n",
      "step 2/3 | epoch 27/50 | batch 3/60 | global_step 4563 | loss_total 0.8119\n",
      "step 2/3 | epoch 27/50 | batch 4/60 | global_step 4564 | loss_total 0.9491\n",
      "step 2/3 | epoch 27/50 | batch 5/60 | global_step 4565 | loss_total 0.7900\n",
      "step 2/3 | epoch 27/50 | batch 6/60 | global_step 4566 | loss_total 0.8439\n",
      "step 2/3 | epoch 27/50 | batch 7/60 | global_step 4567 | loss_total 1.6222\n",
      "step 2/3 | epoch 27/50 | batch 8/60 | global_step 4568 | loss_total 0.7711\n",
      "step 2/3 | epoch 27/50 | batch 9/60 | global_step 4569 | loss_total 0.7983\n",
      "step 2/3 | epoch 27/50 | batch 10/60 | global_step 4570 | loss_total 0.8084\n",
      "step 2/3 | epoch 27/50 | batch 11/60 | global_step 4571 | loss_total 0.8029\n",
      "step 2/3 | epoch 27/50 | batch 12/60 | global_step 4572 | loss_total 1.3312\n",
      "step 2/3 | epoch 27/50 | batch 13/60 | global_step 4573 | loss_total 2.0387\n",
      "step 2/3 | epoch 27/50 | batch 14/60 | global_step 4574 | loss_total 2.0574\n",
      "step 2/3 | epoch 27/50 | batch 15/60 | global_step 4575 | loss_total 1.6747\n",
      "step 2/3 | epoch 27/50 | batch 16/60 | global_step 4576 | loss_total 1.4581\n",
      "step 2/3 | epoch 27/50 | batch 17/60 | global_step 4577 | loss_total 0.8782\n",
      "step 2/3 | epoch 27/50 | batch 18/60 | global_step 4578 | loss_total 0.7948\n",
      "step 2/3 | epoch 27/50 | batch 19/60 | global_step 4579 | loss_total 0.8014\n",
      "step 2/3 | epoch 27/50 | batch 20/60 | global_step 4580 | loss_total 1.8866\n",
      "step 2/3 | epoch 27/50 | batch 21/60 | global_step 4581 | loss_total 2.2870\n",
      "step 2/3 | epoch 27/50 | batch 22/60 | global_step 4582 | loss_total 0.8228\n",
      "step 2/3 | epoch 27/50 | batch 23/60 | global_step 4583 | loss_total 2.2093\n",
      "step 2/3 | epoch 27/50 | batch 24/60 | global_step 4584 | loss_total 0.7870\n",
      "step 2/3 | epoch 27/50 | batch 25/60 | global_step 4585 | loss_total 0.8518\n",
      "step 2/3 | epoch 27/50 | batch 26/60 | global_step 4586 | loss_total 0.7747\n",
      "step 2/3 | epoch 27/50 | batch 27/60 | global_step 4587 | loss_total 1.3355\n",
      "step 2/3 | epoch 27/50 | batch 28/60 | global_step 4588 | loss_total 0.7834\n",
      "step 2/3 | epoch 27/50 | batch 29/60 | global_step 4589 | loss_total 0.7659\n",
      "step 2/3 | epoch 27/50 | batch 30/60 | global_step 4590 | loss_total 0.7864\n",
      "step 2/3 | epoch 27/50 | batch 31/60 | global_step 4591 | loss_total 0.7737\n",
      "step 2/3 | epoch 27/50 | batch 32/60 | global_step 4592 | loss_total 0.7872\n",
      "step 2/3 | epoch 27/50 | batch 33/60 | global_step 4593 | loss_total 1.4149\n",
      "step 2/3 | epoch 27/50 | batch 34/60 | global_step 4594 | loss_total 1.4028\n",
      "step 2/3 | epoch 27/50 | batch 35/60 | global_step 4595 | loss_total 1.2581\n",
      "step 2/3 | epoch 27/50 | batch 36/60 | global_step 4596 | loss_total 0.8419\n",
      "step 2/3 | epoch 27/50 | batch 37/60 | global_step 4597 | loss_total 0.8134\n",
      "step 2/3 | epoch 27/50 | batch 38/60 | global_step 4598 | loss_total 0.8254\n",
      "step 2/3 | epoch 27/50 | batch 39/60 | global_step 4599 | loss_total 1.7016\n",
      "step 2/3 | epoch 27/50 | batch 40/60 | global_step 4600 | loss_total 1.4128\n",
      "step 2/3 | epoch 27/50 | batch 41/60 | global_step 4601 | loss_total 1.6404\n",
      "step 2/3 | epoch 27/50 | batch 42/60 | global_step 4602 | loss_total 1.4564\n",
      "step 2/3 | epoch 27/50 | batch 43/60 | global_step 4603 | loss_total 0.8747\n",
      "step 2/3 | epoch 27/50 | batch 44/60 | global_step 4604 | loss_total 0.8161\n",
      "step 2/3 | epoch 27/50 | batch 45/60 | global_step 4605 | loss_total 0.8370\n",
      "step 2/3 | epoch 27/50 | batch 46/60 | global_step 4606 | loss_total 0.7725\n",
      "step 2/3 | epoch 27/50 | batch 47/60 | global_step 4607 | loss_total 0.8350\n",
      "step 2/3 | epoch 27/50 | batch 48/60 | global_step 4608 | loss_total 0.7782\n",
      "step 2/3 | epoch 27/50 | batch 49/60 | global_step 4609 | loss_total 0.7666\n",
      "step 2/3 | epoch 27/50 | batch 50/60 | global_step 4610 | loss_total 1.8765\n",
      "step 2/3 | epoch 27/50 | batch 51/60 | global_step 4611 | loss_total 0.8817\n",
      "step 2/3 | epoch 27/50 | batch 52/60 | global_step 4612 | loss_total 0.8206\n",
      "step 2/3 | epoch 27/50 | batch 53/60 | global_step 4613 | loss_total 1.5994\n",
      "step 2/3 | epoch 27/50 | batch 54/60 | global_step 4614 | loss_total 0.8013\n",
      "step 2/3 | epoch 27/50 | batch 55/60 | global_step 4615 | loss_total 0.7788\n",
      "step 2/3 | epoch 27/50 | batch 56/60 | global_step 4616 | loss_total 2.0031\n",
      "step 2/3 | epoch 27/50 | batch 57/60 | global_step 4617 | loss_total 0.8327\n",
      "step 2/3 | epoch 27/50 | batch 58/60 | global_step 4618 | loss_total 0.7926\n",
      "step 2/3 | epoch 27/50 | batch 59/60 | global_step 4619 | loss_total 0.7876\n",
      "step 2/3 | epoch 27/50 | batch 60/60 | global_step 4620 | loss_total 1.3443\n",
      "[epoch done] step 2/3 epoch 27/50 | train_total=1.1117 val_total=0.7556\n",
      "step 2/3 | epoch 28/50 | batch 1/60 | global_step 4621 | loss_total 0.7960\n",
      "step 2/3 | epoch 28/50 | batch 2/60 | global_step 4622 | loss_total 1.1188\n",
      "step 2/3 | epoch 28/50 | batch 3/60 | global_step 4623 | loss_total 0.8982\n",
      "step 2/3 | epoch 28/50 | batch 4/60 | global_step 4624 | loss_total 0.7581\n",
      "step 2/3 | epoch 28/50 | batch 5/60 | global_step 4625 | loss_total 1.4068\n",
      "step 2/3 | epoch 28/50 | batch 6/60 | global_step 4626 | loss_total 0.7953\n",
      "step 2/3 | epoch 28/50 | batch 7/60 | global_step 4627 | loss_total 0.8578\n",
      "step 2/3 | epoch 28/50 | batch 8/60 | global_step 4628 | loss_total 1.5403\n",
      "step 2/3 | epoch 28/50 | batch 9/60 | global_step 4629 | loss_total 0.8273\n",
      "step 2/3 | epoch 28/50 | batch 10/60 | global_step 4630 | loss_total 0.7810\n",
      "step 2/3 | epoch 28/50 | batch 11/60 | global_step 4631 | loss_total 1.7687\n",
      "step 2/3 | epoch 28/50 | batch 12/60 | global_step 4632 | loss_total 1.4255\n",
      "step 2/3 | epoch 28/50 | batch 13/60 | global_step 4633 | loss_total 0.9267\n",
      "step 2/3 | epoch 28/50 | batch 14/60 | global_step 4634 | loss_total 1.4748\n",
      "step 2/3 | epoch 28/50 | batch 15/60 | global_step 4635 | loss_total 2.4338\n",
      "step 2/3 | epoch 28/50 | batch 16/60 | global_step 4636 | loss_total 0.9234\n",
      "step 2/3 | epoch 28/50 | batch 17/60 | global_step 4637 | loss_total 0.8297\n",
      "step 2/3 | epoch 28/50 | batch 18/60 | global_step 4638 | loss_total 1.4111\n",
      "step 2/3 | epoch 28/50 | batch 19/60 | global_step 4639 | loss_total 1.4921\n",
      "step 2/3 | epoch 28/50 | batch 20/60 | global_step 4640 | loss_total 0.8202\n",
      "step 2/3 | epoch 28/50 | batch 21/60 | global_step 4641 | loss_total 0.7723\n",
      "step 2/3 | epoch 28/50 | batch 22/60 | global_step 4642 | loss_total 1.6544\n",
      "step 2/3 | epoch 28/50 | batch 23/60 | global_step 4643 | loss_total 2.4840\n",
      "step 2/3 | epoch 28/50 | batch 24/60 | global_step 4644 | loss_total 2.4317\n",
      "step 2/3 | epoch 28/50 | batch 25/60 | global_step 4645 | loss_total 1.8134\n",
      "step 2/3 | epoch 28/50 | batch 26/60 | global_step 4646 | loss_total 1.9275\n",
      "step 2/3 | epoch 28/50 | batch 27/60 | global_step 4647 | loss_total 2.1953\n",
      "step 2/3 | epoch 28/50 | batch 28/60 | global_step 4648 | loss_total 0.9363\n",
      "step 2/3 | epoch 28/50 | batch 29/60 | global_step 4649 | loss_total 1.2004\n",
      "step 2/3 | epoch 28/50 | batch 30/60 | global_step 4650 | loss_total 1.0231\n",
      "step 2/3 | epoch 28/50 | batch 31/60 | global_step 4651 | loss_total 0.9378\n",
      "step 2/3 | epoch 28/50 | batch 32/60 | global_step 4652 | loss_total 1.3594\n",
      "step 2/3 | epoch 28/50 | batch 33/60 | global_step 4653 | loss_total 0.9071\n",
      "step 2/3 | epoch 28/50 | batch 34/60 | global_step 4654 | loss_total 1.4666\n",
      "step 2/3 | epoch 28/50 | batch 35/60 | global_step 4655 | loss_total 0.8604\n",
      "step 2/3 | epoch 28/50 | batch 36/60 | global_step 4656 | loss_total 0.8551\n",
      "step 2/3 | epoch 28/50 | batch 37/60 | global_step 4657 | loss_total 0.8970\n",
      "step 2/3 | epoch 28/50 | batch 38/60 | global_step 4658 | loss_total 1.4084\n",
      "step 2/3 | epoch 28/50 | batch 39/60 | global_step 4659 | loss_total 0.8396\n",
      "step 2/3 | epoch 28/50 | batch 40/60 | global_step 4660 | loss_total 0.8929\n",
      "step 2/3 | epoch 28/50 | batch 41/60 | global_step 4661 | loss_total 1.7660\n",
      "step 2/3 | epoch 28/50 | batch 42/60 | global_step 4662 | loss_total 1.5223\n",
      "step 2/3 | epoch 28/50 | batch 43/60 | global_step 4663 | loss_total 0.7704\n",
      "step 2/3 | epoch 28/50 | batch 44/60 | global_step 4664 | loss_total 0.9249\n",
      "step 2/3 | epoch 28/50 | batch 45/60 | global_step 4665 | loss_total 0.8506\n",
      "step 2/3 | epoch 28/50 | batch 46/60 | global_step 4666 | loss_total 1.2750\n",
      "step 2/3 | epoch 28/50 | batch 47/60 | global_step 4667 | loss_total 0.8254\n",
      "step 2/3 | epoch 28/50 | batch 48/60 | global_step 4668 | loss_total 1.5146\n",
      "step 2/3 | epoch 28/50 | batch 49/60 | global_step 4669 | loss_total 1.6410\n",
      "step 2/3 | epoch 28/50 | batch 50/60 | global_step 4670 | loss_total 0.6713\n",
      "step 2/3 | epoch 28/50 | batch 51/60 | global_step 4671 | loss_total 1.6688\n",
      "step 2/3 | epoch 28/50 | batch 52/60 | global_step 4672 | loss_total 0.9100\n",
      "step 2/3 | epoch 28/50 | batch 53/60 | global_step 4673 | loss_total 0.7421\n",
      "step 2/3 | epoch 28/50 | batch 54/60 | global_step 4674 | loss_total 1.6556\n",
      "step 2/3 | epoch 28/50 | batch 55/60 | global_step 4675 | loss_total 1.5582\n",
      "step 2/3 | epoch 28/50 | batch 56/60 | global_step 4676 | loss_total 0.9028\n",
      "step 2/3 | epoch 28/50 | batch 57/60 | global_step 4677 | loss_total 0.8074\n",
      "step 2/3 | epoch 28/50 | batch 58/60 | global_step 4678 | loss_total 0.8916\n",
      "step 2/3 | epoch 28/50 | batch 59/60 | global_step 4679 | loss_total 0.8069\n",
      "step 2/3 | epoch 28/50 | batch 60/60 | global_step 4680 | loss_total 0.7945\n",
      "[epoch done] step 2/3 epoch 28/50 | train_total=1.2108 val_total=0.9639\n",
      "step 2/3 | epoch 29/50 | batch 1/60 | global_step 4681 | loss_total 0.8702\n",
      "step 2/3 | epoch 29/50 | batch 2/60 | global_step 4682 | loss_total 0.7906\n",
      "step 2/3 | epoch 29/50 | batch 3/60 | global_step 4683 | loss_total 1.5953\n",
      "step 2/3 | epoch 29/50 | batch 4/60 | global_step 4684 | loss_total 0.7751\n",
      "step 2/3 | epoch 29/50 | batch 5/60 | global_step 4685 | loss_total 0.7604\n",
      "step 2/3 | epoch 29/50 | batch 6/60 | global_step 4686 | loss_total 2.0324\n",
      "step 2/3 | epoch 29/50 | batch 7/60 | global_step 4687 | loss_total 0.7699\n",
      "step 2/3 | epoch 29/50 | batch 8/60 | global_step 4688 | loss_total 0.9453\n",
      "step 2/3 | epoch 29/50 | batch 9/60 | global_step 4689 | loss_total 1.6927\n",
      "step 2/3 | epoch 29/50 | batch 10/60 | global_step 4690 | loss_total 1.6940\n",
      "step 2/3 | epoch 29/50 | batch 11/60 | global_step 4691 | loss_total 1.4806\n",
      "step 2/3 | epoch 29/50 | batch 12/60 | global_step 4692 | loss_total 1.6304\n",
      "step 2/3 | epoch 29/50 | batch 13/60 | global_step 4693 | loss_total 0.9935\n",
      "step 2/3 | epoch 29/50 | batch 14/60 | global_step 4694 | loss_total 0.7843\n",
      "step 2/3 | epoch 29/50 | batch 15/60 | global_step 4695 | loss_total 0.7896\n",
      "step 2/3 | epoch 29/50 | batch 16/60 | global_step 4696 | loss_total 0.7778\n",
      "step 2/3 | epoch 29/50 | batch 17/60 | global_step 4697 | loss_total 0.7777\n",
      "step 2/3 | epoch 29/50 | batch 18/60 | global_step 4698 | loss_total 0.8264\n",
      "step 2/3 | epoch 29/50 | batch 19/60 | global_step 4699 | loss_total 0.7743\n",
      "step 2/3 | epoch 29/50 | batch 20/60 | global_step 4700 | loss_total 0.8077\n",
      "step 2/3 | epoch 29/50 | batch 21/60 | global_step 4701 | loss_total 0.8395\n",
      "step 2/3 | epoch 29/50 | batch 22/60 | global_step 4702 | loss_total 0.7808\n",
      "step 2/3 | epoch 29/50 | batch 23/60 | global_step 4703 | loss_total 2.5681\n",
      "step 2/3 | epoch 29/50 | batch 24/60 | global_step 4704 | loss_total 0.7779\n",
      "step 2/3 | epoch 29/50 | batch 25/60 | global_step 4705 | loss_total 0.7740\n",
      "step 2/3 | epoch 29/50 | batch 26/60 | global_step 4706 | loss_total 1.8005\n",
      "step 2/3 | epoch 29/50 | batch 27/60 | global_step 4707 | loss_total 1.3830\n",
      "step 2/3 | epoch 29/50 | batch 28/60 | global_step 4708 | loss_total 1.3885\n",
      "step 2/3 | epoch 29/50 | batch 29/60 | global_step 4709 | loss_total 0.7782\n",
      "step 2/3 | epoch 29/50 | batch 30/60 | global_step 4710 | loss_total 0.7573\n",
      "step 2/3 | epoch 29/50 | batch 31/60 | global_step 4711 | loss_total 0.7809\n",
      "step 2/3 | epoch 29/50 | batch 32/60 | global_step 4712 | loss_total 0.9099\n",
      "step 2/3 | epoch 29/50 | batch 33/60 | global_step 4713 | loss_total 0.7855\n",
      "step 2/3 | epoch 29/50 | batch 34/60 | global_step 4714 | loss_total 0.7813\n",
      "step 2/3 | epoch 29/50 | batch 35/60 | global_step 4715 | loss_total 0.7925\n",
      "step 2/3 | epoch 29/50 | batch 36/60 | global_step 4716 | loss_total 0.7659\n",
      "step 2/3 | epoch 29/50 | batch 37/60 | global_step 4717 | loss_total 2.2479\n",
      "step 2/3 | epoch 29/50 | batch 38/60 | global_step 4718 | loss_total 2.5727\n",
      "step 2/3 | epoch 29/50 | batch 39/60 | global_step 4719 | loss_total 0.7923\n",
      "step 2/3 | epoch 29/50 | batch 40/60 | global_step 4720 | loss_total 0.8525\n",
      "step 2/3 | epoch 29/50 | batch 41/60 | global_step 4721 | loss_total 0.8359\n",
      "step 2/3 | epoch 29/50 | batch 42/60 | global_step 4722 | loss_total 0.7900\n",
      "step 2/3 | epoch 29/50 | batch 43/60 | global_step 4723 | loss_total 0.7929\n",
      "step 2/3 | epoch 29/50 | batch 44/60 | global_step 4724 | loss_total 0.7868\n",
      "step 2/3 | epoch 29/50 | batch 45/60 | global_step 4725 | loss_total 0.7958\n",
      "step 2/3 | epoch 29/50 | batch 46/60 | global_step 4726 | loss_total 1.7781\n",
      "step 2/3 | epoch 29/50 | batch 47/60 | global_step 4727 | loss_total 0.8138\n",
      "step 2/3 | epoch 29/50 | batch 48/60 | global_step 4728 | loss_total 0.8037\n",
      "step 2/3 | epoch 29/50 | batch 49/60 | global_step 4729 | loss_total 0.7826\n",
      "step 2/3 | epoch 29/50 | batch 50/60 | global_step 4730 | loss_total 2.2184\n",
      "step 2/3 | epoch 29/50 | batch 51/60 | global_step 4731 | loss_total 0.7917\n",
      "step 2/3 | epoch 29/50 | batch 52/60 | global_step 4732 | loss_total 0.7908\n",
      "step 2/3 | epoch 29/50 | batch 53/60 | global_step 4733 | loss_total 0.7552\n",
      "step 2/3 | epoch 29/50 | batch 54/60 | global_step 4734 | loss_total 0.8247\n",
      "step 2/3 | epoch 29/50 | batch 55/60 | global_step 4735 | loss_total 0.8193\n",
      "step 2/3 | epoch 29/50 | batch 56/60 | global_step 4736 | loss_total 0.8489\n",
      "step 2/3 | epoch 29/50 | batch 57/60 | global_step 4737 | loss_total 1.9597\n",
      "step 2/3 | epoch 29/50 | batch 58/60 | global_step 4738 | loss_total 1.7797\n",
      "step 2/3 | epoch 29/50 | batch 59/60 | global_step 4739 | loss_total 0.7716\n",
      "step 2/3 | epoch 29/50 | batch 60/60 | global_step 4740 | loss_total 0.7709\n",
      "[epoch done] step 2/3 epoch 29/50 | train_total=1.0868 val_total=0.7612\n",
      "step 2/3 | epoch 30/50 | batch 1/60 | global_step 4741 | loss_total 0.7756\n",
      "step 2/3 | epoch 30/50 | batch 2/60 | global_step 4742 | loss_total 2.6583\n",
      "step 2/3 | epoch 30/50 | batch 3/60 | global_step 4743 | loss_total 0.7364\n",
      "step 2/3 | epoch 30/50 | batch 4/60 | global_step 4744 | loss_total 0.7434\n",
      "step 2/3 | epoch 30/50 | batch 5/60 | global_step 4745 | loss_total 0.7667\n",
      "step 2/3 | epoch 30/50 | batch 6/60 | global_step 4746 | loss_total 2.1744\n",
      "step 2/3 | epoch 30/50 | batch 7/60 | global_step 4747 | loss_total 0.7068\n",
      "step 2/3 | epoch 30/50 | batch 8/60 | global_step 4748 | loss_total 1.2380\n",
      "step 2/3 | epoch 30/50 | batch 9/60 | global_step 4749 | loss_total 0.6994\n",
      "step 2/3 | epoch 30/50 | batch 10/60 | global_step 4750 | loss_total 2.6272\n",
      "step 2/3 | epoch 30/50 | batch 11/60 | global_step 4751 | loss_total 1.8336\n",
      "step 2/3 | epoch 30/50 | batch 12/60 | global_step 4752 | loss_total 0.8465\n",
      "step 2/3 | epoch 30/50 | batch 13/60 | global_step 4753 | loss_total 0.7556\n",
      "step 2/3 | epoch 30/50 | batch 14/60 | global_step 4754 | loss_total 2.6791\n",
      "step 2/3 | epoch 30/50 | batch 15/60 | global_step 4755 | loss_total 1.9022\n",
      "step 2/3 | epoch 30/50 | batch 16/60 | global_step 4756 | loss_total 1.6317\n",
      "step 2/3 | epoch 30/50 | batch 17/60 | global_step 4757 | loss_total 1.0819\n",
      "step 2/3 | epoch 30/50 | batch 18/60 | global_step 4758 | loss_total 0.9892\n",
      "step 2/3 | epoch 30/50 | batch 19/60 | global_step 4759 | loss_total 1.7838\n",
      "step 2/3 | epoch 30/50 | batch 20/60 | global_step 4760 | loss_total 0.7740\n",
      "step 2/3 | epoch 30/50 | batch 21/60 | global_step 4761 | loss_total 0.7368\n",
      "step 2/3 | epoch 30/50 | batch 22/60 | global_step 4762 | loss_total 0.8053\n",
      "step 2/3 | epoch 30/50 | batch 23/60 | global_step 4763 | loss_total 0.9814\n",
      "step 2/3 | epoch 30/50 | batch 24/60 | global_step 4764 | loss_total 0.8111\n",
      "step 2/3 | epoch 30/50 | batch 25/60 | global_step 4765 | loss_total 0.7614\n",
      "step 2/3 | epoch 30/50 | batch 26/60 | global_step 4766 | loss_total 1.0189\n",
      "step 2/3 | epoch 30/50 | batch 27/60 | global_step 4767 | loss_total 1.8886\n",
      "step 2/3 | epoch 30/50 | batch 28/60 | global_step 4768 | loss_total 0.8266\n",
      "step 2/3 | epoch 30/50 | batch 29/60 | global_step 4769 | loss_total 0.7676\n",
      "step 2/3 | epoch 30/50 | batch 30/60 | global_step 4770 | loss_total 1.5617\n",
      "step 2/3 | epoch 30/50 | batch 31/60 | global_step 4771 | loss_total 0.7989\n",
      "step 2/3 | epoch 30/50 | batch 32/60 | global_step 4772 | loss_total 1.5521\n",
      "step 2/3 | epoch 30/50 | batch 33/60 | global_step 4773 | loss_total 1.0336\n",
      "step 2/3 | epoch 30/50 | batch 34/60 | global_step 4774 | loss_total 0.7494\n",
      "step 2/3 | epoch 30/50 | batch 35/60 | global_step 4775 | loss_total 0.7627\n",
      "step 2/3 | epoch 30/50 | batch 36/60 | global_step 4776 | loss_total 0.7607\n",
      "step 2/3 | epoch 30/50 | batch 37/60 | global_step 4777 | loss_total 0.7335\n",
      "step 2/3 | epoch 30/50 | batch 38/60 | global_step 4778 | loss_total 0.7212\n",
      "step 2/3 | epoch 30/50 | batch 39/60 | global_step 4779 | loss_total 1.4326\n",
      "step 2/3 | epoch 30/50 | batch 40/60 | global_step 4780 | loss_total 1.8434\n",
      "step 2/3 | epoch 30/50 | batch 41/60 | global_step 4781 | loss_total 0.7428\n",
      "step 2/3 | epoch 30/50 | batch 42/60 | global_step 4782 | loss_total 1.6556\n",
      "step 2/3 | epoch 30/50 | batch 43/60 | global_step 4783 | loss_total 0.7964\n",
      "step 2/3 | epoch 30/50 | batch 44/60 | global_step 4784 | loss_total 2.2687\n",
      "step 2/3 | epoch 30/50 | batch 45/60 | global_step 4785 | loss_total 0.7468\n",
      "step 2/3 | epoch 30/50 | batch 46/60 | global_step 4786 | loss_total 1.7458\n",
      "step 2/3 | epoch 30/50 | batch 47/60 | global_step 4787 | loss_total 0.7465\n",
      "step 2/3 | epoch 30/50 | batch 48/60 | global_step 4788 | loss_total 0.7131\n",
      "step 2/3 | epoch 30/50 | batch 49/60 | global_step 4789 | loss_total 0.7188\n",
      "step 2/3 | epoch 30/50 | batch 50/60 | global_step 4790 | loss_total 0.7452\n",
      "step 2/3 | epoch 30/50 | batch 51/60 | global_step 4791 | loss_total 0.7216\n",
      "step 2/3 | epoch 30/50 | batch 52/60 | global_step 4792 | loss_total 1.6696\n",
      "step 2/3 | epoch 30/50 | batch 53/60 | global_step 4793 | loss_total 0.7264\n",
      "step 2/3 | epoch 30/50 | batch 54/60 | global_step 4794 | loss_total 0.8026\n",
      "step 2/3 | epoch 30/50 | batch 55/60 | global_step 4795 | loss_total 0.7586\n",
      "step 2/3 | epoch 30/50 | batch 56/60 | global_step 4796 | loss_total 1.3755\n",
      "step 2/3 | epoch 30/50 | batch 57/60 | global_step 4797 | loss_total 0.7599\n",
      "step 2/3 | epoch 30/50 | batch 58/60 | global_step 4798 | loss_total 0.7244\n",
      "step 2/3 | epoch 30/50 | batch 59/60 | global_step 4799 | loss_total 1.3714\n",
      "step 2/3 | epoch 30/50 | batch 60/60 | global_step 4800 | loss_total 0.7315\n",
      "[epoch done] step 2/3 epoch 30/50 | train_total=1.1412 val_total=0.8668\n",
      "step 2/3 | epoch 31/50 | batch 1/60 | global_step 4801 | loss_total 0.7305\n",
      "step 2/3 | epoch 31/50 | batch 2/60 | global_step 4802 | loss_total 1.5658\n",
      "step 2/3 | epoch 31/50 | batch 3/60 | global_step 4803 | loss_total 0.7158\n",
      "step 2/3 | epoch 31/50 | batch 4/60 | global_step 4804 | loss_total 0.7720\n",
      "step 2/3 | epoch 31/50 | batch 5/60 | global_step 4805 | loss_total 0.7797\n",
      "step 2/3 | epoch 31/50 | batch 6/60 | global_step 4806 | loss_total 0.8689\n",
      "step 2/3 | epoch 31/50 | batch 7/60 | global_step 4807 | loss_total 0.8751\n",
      "step 2/3 | epoch 31/50 | batch 8/60 | global_step 4808 | loss_total 2.2860\n",
      "step 2/3 | epoch 31/50 | batch 9/60 | global_step 4809 | loss_total 1.5244\n",
      "step 2/3 | epoch 31/50 | batch 10/60 | global_step 4810 | loss_total 0.7916\n",
      "step 2/3 | epoch 31/50 | batch 11/60 | global_step 4811 | loss_total 1.8008\n",
      "step 2/3 | epoch 31/50 | batch 12/60 | global_step 4812 | loss_total 0.6315\n",
      "step 2/3 | epoch 31/50 | batch 13/60 | global_step 4813 | loss_total 1.3998\n",
      "step 2/3 | epoch 31/50 | batch 14/60 | global_step 4814 | loss_total 0.8122\n",
      "step 2/3 | epoch 31/50 | batch 15/60 | global_step 4815 | loss_total 0.8790\n",
      "step 2/3 | epoch 31/50 | batch 16/60 | global_step 4816 | loss_total 0.8118\n",
      "step 2/3 | epoch 31/50 | batch 17/60 | global_step 4817 | loss_total 1.5363\n",
      "step 2/3 | epoch 31/50 | batch 18/60 | global_step 4818 | loss_total 2.9443\n",
      "step 2/3 | epoch 31/50 | batch 19/60 | global_step 4819 | loss_total 0.8623\n",
      "step 2/3 | epoch 31/50 | batch 20/60 | global_step 4820 | loss_total 0.8996\n",
      "step 2/3 | epoch 31/50 | batch 21/60 | global_step 4821 | loss_total 1.5678\n",
      "step 2/3 | epoch 31/50 | batch 22/60 | global_step 4822 | loss_total 0.8540\n",
      "step 2/3 | epoch 31/50 | batch 23/60 | global_step 4823 | loss_total 0.8641\n",
      "step 2/3 | epoch 31/50 | batch 24/60 | global_step 4824 | loss_total 0.8482\n",
      "step 2/3 | epoch 31/50 | batch 25/60 | global_step 4825 | loss_total 0.8389\n",
      "step 2/3 | epoch 31/50 | batch 26/60 | global_step 4826 | loss_total 0.8044\n",
      "step 2/3 | epoch 31/50 | batch 27/60 | global_step 4827 | loss_total 1.6393\n",
      "step 2/3 | epoch 31/50 | batch 28/60 | global_step 4828 | loss_total 0.8413\n",
      "step 2/3 | epoch 31/50 | batch 29/60 | global_step 4829 | loss_total 0.8424\n",
      "step 2/3 | epoch 31/50 | batch 30/60 | global_step 4830 | loss_total 0.8167\n",
      "step 2/3 | epoch 31/50 | batch 31/60 | global_step 4831 | loss_total 1.4149\n",
      "step 2/3 | epoch 31/50 | batch 32/60 | global_step 4832 | loss_total 0.8230\n",
      "step 2/3 | epoch 31/50 | batch 33/60 | global_step 4833 | loss_total 0.7817\n",
      "step 2/3 | epoch 31/50 | batch 34/60 | global_step 4834 | loss_total 1.4324\n",
      "step 2/3 | epoch 31/50 | batch 35/60 | global_step 4835 | loss_total 2.7993\n",
      "step 2/3 | epoch 31/50 | batch 36/60 | global_step 4836 | loss_total 0.7913\n",
      "step 2/3 | epoch 31/50 | batch 37/60 | global_step 4837 | loss_total 0.8452\n",
      "step 2/3 | epoch 31/50 | batch 38/60 | global_step 4838 | loss_total 0.8193\n",
      "step 2/3 | epoch 31/50 | batch 39/60 | global_step 4839 | loss_total 1.8340\n",
      "step 2/3 | epoch 31/50 | batch 40/60 | global_step 4840 | loss_total 0.7648\n",
      "step 2/3 | epoch 31/50 | batch 41/60 | global_step 4841 | loss_total 1.7447\n",
      "step 2/3 | epoch 31/50 | batch 42/60 | global_step 4842 | loss_total 1.7503\n",
      "step 2/3 | epoch 31/50 | batch 43/60 | global_step 4843 | loss_total 1.5931\n",
      "step 2/3 | epoch 31/50 | batch 44/60 | global_step 4844 | loss_total 1.3911\n",
      "step 2/3 | epoch 31/50 | batch 45/60 | global_step 4845 | loss_total 1.7825\n",
      "step 2/3 | epoch 31/50 | batch 46/60 | global_step 4846 | loss_total 0.8153\n",
      "step 2/3 | epoch 31/50 | batch 47/60 | global_step 4847 | loss_total 1.4071\n",
      "step 2/3 | epoch 31/50 | batch 48/60 | global_step 4848 | loss_total 0.8203\n",
      "step 2/3 | epoch 31/50 | batch 49/60 | global_step 4849 | loss_total 1.8310\n",
      "step 2/3 | epoch 31/50 | batch 50/60 | global_step 4850 | loss_total 1.6096\n",
      "step 2/3 | epoch 31/50 | batch 51/60 | global_step 4851 | loss_total 0.7177\n",
      "step 2/3 | epoch 31/50 | batch 52/60 | global_step 4852 | loss_total 1.7093\n",
      "step 2/3 | epoch 31/50 | batch 53/60 | global_step 4853 | loss_total 1.5231\n",
      "step 2/3 | epoch 31/50 | batch 54/60 | global_step 4854 | loss_total 0.8002\n",
      "step 2/3 | epoch 31/50 | batch 55/60 | global_step 4855 | loss_total 1.0025\n",
      "step 2/3 | epoch 31/50 | batch 56/60 | global_step 4856 | loss_total 0.8365\n",
      "step 2/3 | epoch 31/50 | batch 57/60 | global_step 4857 | loss_total 0.9803\n",
      "step 2/3 | epoch 31/50 | batch 58/60 | global_step 4858 | loss_total 0.8321\n",
      "step 2/3 | epoch 31/50 | batch 59/60 | global_step 4859 | loss_total 0.9897\n",
      "step 2/3 | epoch 31/50 | batch 60/60 | global_step 4860 | loss_total 0.8229\n",
      "[epoch done] step 2/3 epoch 31/50 | train_total=1.1778 val_total=1.0970\n",
      "step 2/3 | epoch 32/50 | batch 1/60 | global_step 4861 | loss_total 1.2062\n",
      "step 2/3 | epoch 32/50 | batch 2/60 | global_step 4862 | loss_total 1.4448\n",
      "step 2/3 | epoch 32/50 | batch 3/60 | global_step 4863 | loss_total 1.4626\n",
      "step 2/3 | epoch 32/50 | batch 4/60 | global_step 4864 | loss_total 1.8537\n",
      "step 2/3 | epoch 32/50 | batch 5/60 | global_step 4865 | loss_total 0.7531\n",
      "step 2/3 | epoch 32/50 | batch 6/60 | global_step 4866 | loss_total 0.7568\n",
      "step 2/3 | epoch 32/50 | batch 7/60 | global_step 4867 | loss_total 1.6466\n",
      "step 2/3 | epoch 32/50 | batch 8/60 | global_step 4868 | loss_total 1.6583\n",
      "step 2/3 | epoch 32/50 | batch 9/60 | global_step 4869 | loss_total 0.7875\n",
      "step 2/3 | epoch 32/50 | batch 10/60 | global_step 4870 | loss_total 0.7875\n",
      "step 2/3 | epoch 32/50 | batch 11/60 | global_step 4871 | loss_total 0.8216\n",
      "step 2/3 | epoch 32/50 | batch 12/60 | global_step 4872 | loss_total 0.8430\n",
      "step 2/3 | epoch 32/50 | batch 13/60 | global_step 4873 | loss_total 0.8144\n",
      "step 2/3 | epoch 32/50 | batch 14/60 | global_step 4874 | loss_total 0.7834\n",
      "step 2/3 | epoch 32/50 | batch 15/60 | global_step 4875 | loss_total 1.4673\n",
      "step 2/3 | epoch 32/50 | batch 16/60 | global_step 4876 | loss_total 0.7824\n",
      "step 2/3 | epoch 32/50 | batch 17/60 | global_step 4877 | loss_total 1.6482\n",
      "step 2/3 | epoch 32/50 | batch 18/60 | global_step 4878 | loss_total 1.5725\n",
      "step 2/3 | epoch 32/50 | batch 19/60 | global_step 4879 | loss_total 0.8166\n",
      "step 2/3 | epoch 32/50 | batch 20/60 | global_step 4880 | loss_total 1.3691\n",
      "step 2/3 | epoch 32/50 | batch 21/60 | global_step 4881 | loss_total 1.7082\n",
      "step 2/3 | epoch 32/50 | batch 22/60 | global_step 4882 | loss_total 0.8507\n",
      "step 2/3 | epoch 32/50 | batch 23/60 | global_step 4883 | loss_total 0.7918\n",
      "step 2/3 | epoch 32/50 | batch 24/60 | global_step 4884 | loss_total 0.7735\n",
      "step 2/3 | epoch 32/50 | batch 25/60 | global_step 4885 | loss_total 1.3805\n",
      "step 2/3 | epoch 32/50 | batch 26/60 | global_step 4886 | loss_total 0.7696\n",
      "step 2/3 | epoch 32/50 | batch 27/60 | global_step 4887 | loss_total 0.7714\n",
      "step 2/3 | epoch 32/50 | batch 28/60 | global_step 4888 | loss_total 0.8143\n",
      "step 2/3 | epoch 32/50 | batch 29/60 | global_step 4889 | loss_total 1.6159\n",
      "step 2/3 | epoch 32/50 | batch 30/60 | global_step 4890 | loss_total 0.7582\n",
      "step 2/3 | epoch 32/50 | batch 31/60 | global_step 4891 | loss_total 0.7859\n",
      "step 2/3 | epoch 32/50 | batch 32/60 | global_step 4892 | loss_total 2.1208\n",
      "step 2/3 | epoch 32/50 | batch 33/60 | global_step 4893 | loss_total 1.6054\n",
      "step 2/3 | epoch 32/50 | batch 34/60 | global_step 4894 | loss_total 1.3692\n",
      "step 2/3 | epoch 32/50 | batch 35/60 | global_step 4895 | loss_total 0.7378\n",
      "step 2/3 | epoch 32/50 | batch 36/60 | global_step 4896 | loss_total 0.7355\n",
      "step 2/3 | epoch 32/50 | batch 37/60 | global_step 4897 | loss_total 1.2765\n",
      "step 2/3 | epoch 32/50 | batch 38/60 | global_step 4898 | loss_total 1.4869\n",
      "step 2/3 | epoch 32/50 | batch 39/60 | global_step 4899 | loss_total 1.4741\n",
      "step 2/3 | epoch 32/50 | batch 40/60 | global_step 4900 | loss_total 0.9171\n",
      "step 2/3 | epoch 32/50 | batch 41/60 | global_step 4901 | loss_total 0.9271\n",
      "step 2/3 | epoch 32/50 | batch 42/60 | global_step 4902 | loss_total 0.8281\n",
      "step 2/3 | epoch 32/50 | batch 43/60 | global_step 4903 | loss_total 0.7255\n",
      "step 2/3 | epoch 32/50 | batch 44/60 | global_step 4904 | loss_total 0.8896\n",
      "step 2/3 | epoch 32/50 | batch 45/60 | global_step 4905 | loss_total 1.4669\n",
      "step 2/3 | epoch 32/50 | batch 46/60 | global_step 4906 | loss_total 1.9962\n",
      "step 2/3 | epoch 32/50 | batch 47/60 | global_step 4907 | loss_total 2.8017\n",
      "step 2/3 | epoch 32/50 | batch 48/60 | global_step 4908 | loss_total 0.8049\n",
      "step 2/3 | epoch 32/50 | batch 49/60 | global_step 4909 | loss_total 0.8172\n",
      "step 2/3 | epoch 32/50 | batch 50/60 | global_step 4910 | loss_total 1.6242\n",
      "step 2/3 | epoch 32/50 | batch 51/60 | global_step 4911 | loss_total 0.7326\n",
      "step 2/3 | epoch 32/50 | batch 52/60 | global_step 4912 | loss_total 1.5256\n",
      "step 2/3 | epoch 32/50 | batch 53/60 | global_step 4913 | loss_total 0.9765\n",
      "step 2/3 | epoch 32/50 | batch 54/60 | global_step 4914 | loss_total 0.8336\n",
      "step 2/3 | epoch 32/50 | batch 55/60 | global_step 4915 | loss_total 0.8184\n",
      "step 2/3 | epoch 32/50 | batch 56/60 | global_step 4916 | loss_total 1.1119\n",
      "step 2/3 | epoch 32/50 | batch 57/60 | global_step 4917 | loss_total 0.8279\n",
      "step 2/3 | epoch 32/50 | batch 58/60 | global_step 4918 | loss_total 0.8979\n",
      "step 2/3 | epoch 32/50 | batch 59/60 | global_step 4919 | loss_total 1.4573\n",
      "step 2/3 | epoch 32/50 | batch 60/60 | global_step 4920 | loss_total 0.8156\n",
      "[epoch done] step 2/3 epoch 32/50 | train_total=1.1483 val_total=0.9511\n",
      "step 2/3 | epoch 33/50 | batch 1/60 | global_step 4921 | loss_total 1.6104\n",
      "step 2/3 | epoch 33/50 | batch 2/60 | global_step 4922 | loss_total 0.7779\n",
      "step 2/3 | epoch 33/50 | batch 3/60 | global_step 4923 | loss_total 1.6436\n",
      "step 2/3 | epoch 33/50 | batch 4/60 | global_step 4924 | loss_total 1.4929\n",
      "step 2/3 | epoch 33/50 | batch 5/60 | global_step 4925 | loss_total 0.7758\n",
      "step 2/3 | epoch 33/50 | batch 6/60 | global_step 4926 | loss_total 1.1122\n",
      "step 2/3 | epoch 33/50 | batch 7/60 | global_step 4927 | loss_total 0.8281\n",
      "step 2/3 | epoch 33/50 | batch 8/60 | global_step 4928 | loss_total 0.8105\n",
      "step 2/3 | epoch 33/50 | batch 9/60 | global_step 4929 | loss_total 0.8478\n",
      "step 2/3 | epoch 33/50 | batch 10/60 | global_step 4930 | loss_total 0.7805\n",
      "step 2/3 | epoch 33/50 | batch 11/60 | global_step 4931 | loss_total 0.8198\n",
      "step 2/3 | epoch 33/50 | batch 12/60 | global_step 4932 | loss_total 2.7231\n",
      "step 2/3 | epoch 33/50 | batch 13/60 | global_step 4933 | loss_total 0.8122\n",
      "step 2/3 | epoch 33/50 | batch 14/60 | global_step 4934 | loss_total 1.6788\n",
      "step 2/3 | epoch 33/50 | batch 15/60 | global_step 4935 | loss_total 0.7735\n",
      "step 2/3 | epoch 33/50 | batch 16/60 | global_step 4936 | loss_total 1.7302\n",
      "step 2/3 | epoch 33/50 | batch 17/60 | global_step 4937 | loss_total 0.8443\n",
      "step 2/3 | epoch 33/50 | batch 18/60 | global_step 4938 | loss_total 0.8586\n",
      "step 2/3 | epoch 33/50 | batch 19/60 | global_step 4939 | loss_total 0.7843\n",
      "step 2/3 | epoch 33/50 | batch 20/60 | global_step 4940 | loss_total 0.8046\n",
      "step 2/3 | epoch 33/50 | batch 21/60 | global_step 4941 | loss_total 0.7873\n",
      "step 2/3 | epoch 33/50 | batch 22/60 | global_step 4942 | loss_total 1.4240\n",
      "step 2/3 | epoch 33/50 | batch 23/60 | global_step 4943 | loss_total 0.8569\n",
      "step 2/3 | epoch 33/50 | batch 24/60 | global_step 4944 | loss_total 1.6201\n",
      "step 2/3 | epoch 33/50 | batch 25/60 | global_step 4945 | loss_total 0.7885\n",
      "step 2/3 | epoch 33/50 | batch 26/60 | global_step 4946 | loss_total 2.2528\n",
      "step 2/3 | epoch 33/50 | batch 27/60 | global_step 4947 | loss_total 2.1849\n",
      "step 2/3 | epoch 33/50 | batch 28/60 | global_step 4948 | loss_total 0.8035\n",
      "step 2/3 | epoch 33/50 | batch 29/60 | global_step 4949 | loss_total 0.7887\n",
      "step 2/3 | epoch 33/50 | batch 30/60 | global_step 4950 | loss_total 1.2701\n",
      "step 2/3 | epoch 33/50 | batch 31/60 | global_step 4951 | loss_total 0.8096\n",
      "step 2/3 | epoch 33/50 | batch 32/60 | global_step 4952 | loss_total 1.6172\n",
      "step 2/3 | epoch 33/50 | batch 33/60 | global_step 4953 | loss_total 1.3133\n",
      "step 2/3 | epoch 33/50 | batch 34/60 | global_step 4954 | loss_total 0.7809\n",
      "step 2/3 | epoch 33/50 | batch 35/60 | global_step 4955 | loss_total 0.7836\n",
      "step 2/3 | epoch 33/50 | batch 36/60 | global_step 4956 | loss_total 0.7734\n",
      "step 2/3 | epoch 33/50 | batch 37/60 | global_step 4957 | loss_total 1.6494\n",
      "step 2/3 | epoch 33/50 | batch 38/60 | global_step 4958 | loss_total 1.3550\n",
      "step 2/3 | epoch 33/50 | batch 39/60 | global_step 4959 | loss_total 1.6496\n",
      "step 2/3 | epoch 33/50 | batch 40/60 | global_step 4960 | loss_total 0.8085\n",
      "step 2/3 | epoch 33/50 | batch 41/60 | global_step 4961 | loss_total 0.9641\n",
      "step 2/3 | epoch 33/50 | batch 42/60 | global_step 4962 | loss_total 0.8980\n",
      "step 2/3 | epoch 33/50 | batch 43/60 | global_step 4963 | loss_total 0.8142\n",
      "step 2/3 | epoch 33/50 | batch 44/60 | global_step 4964 | loss_total 0.7920\n",
      "step 2/3 | epoch 33/50 | batch 45/60 | global_step 4965 | loss_total 0.7839\n",
      "step 2/3 | epoch 33/50 | batch 46/60 | global_step 4966 | loss_total 2.0286\n",
      "step 2/3 | epoch 33/50 | batch 47/60 | global_step 4967 | loss_total 0.7909\n",
      "step 2/3 | epoch 33/50 | batch 48/60 | global_step 4968 | loss_total 0.7771\n",
      "step 2/3 | epoch 33/50 | batch 49/60 | global_step 4969 | loss_total 0.7948\n",
      "step 2/3 | epoch 33/50 | batch 50/60 | global_step 4970 | loss_total 1.4049\n",
      "step 2/3 | epoch 33/50 | batch 51/60 | global_step 4971 | loss_total 1.4230\n",
      "step 2/3 | epoch 33/50 | batch 52/60 | global_step 4972 | loss_total 1.6330\n",
      "step 2/3 | epoch 33/50 | batch 53/60 | global_step 4973 | loss_total 0.7979\n",
      "step 2/3 | epoch 33/50 | batch 54/60 | global_step 4974 | loss_total 0.7798\n",
      "step 2/3 | epoch 33/50 | batch 55/60 | global_step 4975 | loss_total 0.7819\n",
      "step 2/3 | epoch 33/50 | batch 56/60 | global_step 4976 | loss_total 2.1621\n",
      "step 2/3 | epoch 33/50 | batch 57/60 | global_step 4977 | loss_total 0.8319\n",
      "step 2/3 | epoch 33/50 | batch 58/60 | global_step 4978 | loss_total 0.7599\n",
      "step 2/3 | epoch 33/50 | batch 59/60 | global_step 4979 | loss_total 0.7850\n",
      "step 2/3 | epoch 33/50 | batch 60/60 | global_step 4980 | loss_total 1.5662\n",
      "[epoch done] step 2/3 epoch 33/50 | train_total=1.1399 val_total=0.8941\n",
      "step 2/3 | epoch 34/50 | batch 1/60 | global_step 4981 | loss_total 0.8399\n",
      "step 2/3 | epoch 34/50 | batch 2/60 | global_step 4982 | loss_total 1.2919\n",
      "step 2/3 | epoch 34/50 | batch 3/60 | global_step 4983 | loss_total 0.9541\n",
      "step 2/3 | epoch 34/50 | batch 4/60 | global_step 4984 | loss_total 1.3121\n",
      "step 2/3 | epoch 34/50 | batch 5/60 | global_step 4985 | loss_total 0.7445\n",
      "step 2/3 | epoch 34/50 | batch 6/60 | global_step 4986 | loss_total 0.6674\n",
      "step 2/3 | epoch 34/50 | batch 7/60 | global_step 4987 | loss_total 1.1133\n",
      "step 2/3 | epoch 34/50 | batch 8/60 | global_step 4988 | loss_total 0.8077\n",
      "step 2/3 | epoch 34/50 | batch 9/60 | global_step 4989 | loss_total 0.7282\n",
      "step 2/3 | epoch 34/50 | batch 10/60 | global_step 4990 | loss_total 0.7236\n",
      "step 2/3 | epoch 34/50 | batch 11/60 | global_step 4991 | loss_total 1.3813\n",
      "step 2/3 | epoch 34/50 | batch 12/60 | global_step 4992 | loss_total 0.7190\n",
      "step 2/3 | epoch 34/50 | batch 13/60 | global_step 4993 | loss_total 1.4774\n",
      "step 2/3 | epoch 34/50 | batch 14/60 | global_step 4994 | loss_total 1.5214\n",
      "step 2/3 | epoch 34/50 | batch 15/60 | global_step 4995 | loss_total 0.7076\n",
      "step 2/3 | epoch 34/50 | batch 16/60 | global_step 4996 | loss_total 0.8192\n",
      "step 2/3 | epoch 34/50 | batch 17/60 | global_step 4997 | loss_total 1.4531\n",
      "step 2/3 | epoch 34/50 | batch 18/60 | global_step 4998 | loss_total 0.7016\n",
      "step 2/3 | epoch 34/50 | batch 19/60 | global_step 4999 | loss_total 0.9778\n",
      "step 2/3 | epoch 34/50 | batch 20/60 | global_step 5000 | loss_total 0.9665\n",
      "step 2/3 | epoch 34/50 | batch 21/60 | global_step 5001 | loss_total 1.6999\n",
      "step 2/3 | epoch 34/50 | batch 22/60 | global_step 5002 | loss_total 0.8258\n",
      "step 2/3 | epoch 34/50 | batch 23/60 | global_step 5003 | loss_total 0.8798\n",
      "step 2/3 | epoch 34/50 | batch 24/60 | global_step 5004 | loss_total 1.3324\n",
      "step 2/3 | epoch 34/50 | batch 25/60 | global_step 5005 | loss_total 1.6671\n",
      "step 2/3 | epoch 34/50 | batch 26/60 | global_step 5006 | loss_total 0.8452\n",
      "step 2/3 | epoch 34/50 | batch 27/60 | global_step 5007 | loss_total 0.8286\n",
      "step 2/3 | epoch 34/50 | batch 28/60 | global_step 5008 | loss_total 1.0917\n",
      "step 2/3 | epoch 34/50 | batch 29/60 | global_step 5009 | loss_total 0.9379\n",
      "step 2/3 | epoch 34/50 | batch 30/60 | global_step 5010 | loss_total 0.9163\n",
      "step 2/3 | epoch 34/50 | batch 31/60 | global_step 5011 | loss_total 0.8430\n",
      "step 2/3 | epoch 34/50 | batch 32/60 | global_step 5012 | loss_total 0.9107\n",
      "step 2/3 | epoch 34/50 | batch 33/60 | global_step 5013 | loss_total 0.8820\n",
      "step 2/3 | epoch 34/50 | batch 34/60 | global_step 5014 | loss_total 0.8005\n",
      "step 2/3 | epoch 34/50 | batch 35/60 | global_step 5015 | loss_total 0.7924\n",
      "step 2/3 | epoch 34/50 | batch 36/60 | global_step 5016 | loss_total 0.8314\n",
      "step 2/3 | epoch 34/50 | batch 37/60 | global_step 5017 | loss_total 2.5917\n",
      "step 2/3 | epoch 34/50 | batch 38/60 | global_step 5018 | loss_total 1.6811\n",
      "step 2/3 | epoch 34/50 | batch 39/60 | global_step 5019 | loss_total 0.7859\n",
      "step 2/3 | epoch 34/50 | batch 40/60 | global_step 5020 | loss_total 1.5345\n",
      "step 2/3 | epoch 34/50 | batch 41/60 | global_step 5021 | loss_total 0.7875\n",
      "step 2/3 | epoch 34/50 | batch 42/60 | global_step 5022 | loss_total 0.8077\n",
      "step 2/3 | epoch 34/50 | batch 43/60 | global_step 5023 | loss_total 1.0077\n",
      "step 2/3 | epoch 34/50 | batch 44/60 | global_step 5024 | loss_total 1.7423\n",
      "step 2/3 | epoch 34/50 | batch 45/60 | global_step 5025 | loss_total 0.8439\n",
      "step 2/3 | epoch 34/50 | batch 46/60 | global_step 5026 | loss_total 0.8396\n",
      "step 2/3 | epoch 34/50 | batch 47/60 | global_step 5027 | loss_total 1.3946\n",
      "step 2/3 | epoch 34/50 | batch 48/60 | global_step 5028 | loss_total 0.8222\n",
      "step 2/3 | epoch 34/50 | batch 49/60 | global_step 5029 | loss_total 1.8976\n",
      "step 2/3 | epoch 34/50 | batch 50/60 | global_step 5030 | loss_total 0.8010\n",
      "step 2/3 | epoch 34/50 | batch 51/60 | global_step 5031 | loss_total 1.4633\n",
      "step 2/3 | epoch 34/50 | batch 52/60 | global_step 5032 | loss_total 1.6275\n",
      "step 2/3 | epoch 34/50 | batch 53/60 | global_step 5033 | loss_total 0.7975\n",
      "step 2/3 | epoch 34/50 | batch 54/60 | global_step 5034 | loss_total 0.7962\n",
      "step 2/3 | epoch 34/50 | batch 55/60 | global_step 5035 | loss_total 2.3765\n",
      "step 2/3 | epoch 34/50 | batch 56/60 | global_step 5036 | loss_total 0.8096\n",
      "step 2/3 | epoch 34/50 | batch 57/60 | global_step 5037 | loss_total 0.8485\n",
      "step 2/3 | epoch 34/50 | batch 58/60 | global_step 5038 | loss_total 0.9546\n",
      "step 2/3 | epoch 34/50 | batch 59/60 | global_step 5039 | loss_total 0.7934\n",
      "step 2/3 | epoch 34/50 | batch 60/60 | global_step 5040 | loss_total 1.6660\n",
      "[epoch done] step 2/3 epoch 34/50 | train_total=1.0944 val_total=0.9091\n",
      "step 2/3 | epoch 35/50 | batch 1/60 | global_step 5041 | loss_total 0.8009\n",
      "step 2/3 | epoch 35/50 | batch 2/60 | global_step 5042 | loss_total 1.6834\n",
      "step 2/3 | epoch 35/50 | batch 3/60 | global_step 5043 | loss_total 0.7849\n",
      "step 2/3 | epoch 35/50 | batch 4/60 | global_step 5044 | loss_total 2.0497\n",
      "step 2/3 | epoch 35/50 | batch 5/60 | global_step 5045 | loss_total 0.7916\n",
      "step 2/3 | epoch 35/50 | batch 6/60 | global_step 5046 | loss_total 0.7822\n",
      "step 2/3 | epoch 35/50 | batch 7/60 | global_step 5047 | loss_total 2.6896\n",
      "step 2/3 | epoch 35/50 | batch 8/60 | global_step 5048 | loss_total 0.7381\n",
      "step 2/3 | epoch 35/50 | batch 9/60 | global_step 5049 | loss_total 0.7348\n",
      "step 2/3 | epoch 35/50 | batch 10/60 | global_step 5050 | loss_total 1.4876\n",
      "step 2/3 | epoch 35/50 | batch 11/60 | global_step 5051 | loss_total 0.8528\n",
      "step 2/3 | epoch 35/50 | batch 12/60 | global_step 5052 | loss_total 1.6971\n",
      "step 2/3 | epoch 35/50 | batch 13/60 | global_step 5053 | loss_total 0.7798\n",
      "step 2/3 | epoch 35/50 | batch 14/60 | global_step 5054 | loss_total 0.8721\n",
      "step 2/3 | epoch 35/50 | batch 15/60 | global_step 5055 | loss_total 0.9740\n",
      "step 2/3 | epoch 35/50 | batch 16/60 | global_step 5056 | loss_total 0.7229\n",
      "step 2/3 | epoch 35/50 | batch 17/60 | global_step 5057 | loss_total 0.7560\n",
      "step 2/3 | epoch 35/50 | batch 18/60 | global_step 5058 | loss_total 0.7372\n",
      "step 2/3 | epoch 35/50 | batch 19/60 | global_step 5059 | loss_total 1.6239\n",
      "step 2/3 | epoch 35/50 | batch 20/60 | global_step 5060 | loss_total 0.8646\n",
      "step 2/3 | epoch 35/50 | batch 21/60 | global_step 5061 | loss_total 0.7767\n",
      "step 2/3 | epoch 35/50 | batch 22/60 | global_step 5062 | loss_total 1.5186\n",
      "step 2/3 | epoch 35/50 | batch 23/60 | global_step 5063 | loss_total 0.7528\n",
      "step 2/3 | epoch 35/50 | batch 24/60 | global_step 5064 | loss_total 1.4003\n",
      "step 2/3 | epoch 35/50 | batch 25/60 | global_step 5065 | loss_total 1.4568\n",
      "step 2/3 | epoch 35/50 | batch 26/60 | global_step 5066 | loss_total 0.8830\n",
      "step 2/3 | epoch 35/50 | batch 27/60 | global_step 5067 | loss_total 1.6459\n",
      "step 2/3 | epoch 35/50 | batch 28/60 | global_step 5068 | loss_total 1.3977\n",
      "step 2/3 | epoch 35/50 | batch 29/60 | global_step 5069 | loss_total 1.0592\n",
      "step 2/3 | epoch 35/50 | batch 30/60 | global_step 5070 | loss_total 1.0448\n",
      "step 2/3 | epoch 35/50 | batch 31/60 | global_step 5071 | loss_total 0.7231\n",
      "step 2/3 | epoch 35/50 | batch 32/60 | global_step 5072 | loss_total 0.7825\n",
      "step 2/3 | epoch 35/50 | batch 33/60 | global_step 5073 | loss_total 1.9792\n",
      "step 2/3 | epoch 35/50 | batch 34/60 | global_step 5074 | loss_total 1.5464\n",
      "step 2/3 | epoch 35/50 | batch 35/60 | global_step 5075 | loss_total 0.7517\n",
      "step 2/3 | epoch 35/50 | batch 36/60 | global_step 5076 | loss_total 0.7885\n",
      "step 2/3 | epoch 35/50 | batch 37/60 | global_step 5077 | loss_total 0.8256\n",
      "step 2/3 | epoch 35/50 | batch 38/60 | global_step 5078 | loss_total 0.8677\n",
      "step 2/3 | epoch 35/50 | batch 39/60 | global_step 5079 | loss_total 1.6231\n",
      "step 2/3 | epoch 35/50 | batch 40/60 | global_step 5080 | loss_total 0.7607\n",
      "step 2/3 | epoch 35/50 | batch 41/60 | global_step 5081 | loss_total 0.8558\n",
      "step 2/3 | epoch 35/50 | batch 42/60 | global_step 5082 | loss_total 0.7933\n",
      "step 2/3 | epoch 35/50 | batch 43/60 | global_step 5083 | loss_total 2.3896\n",
      "step 2/3 | epoch 35/50 | batch 44/60 | global_step 5084 | loss_total 1.6252\n",
      "step 2/3 | epoch 35/50 | batch 45/60 | global_step 5085 | loss_total 0.7946\n",
      "step 2/3 | epoch 35/50 | batch 46/60 | global_step 5086 | loss_total 0.7907\n",
      "step 2/3 | epoch 35/50 | batch 47/60 | global_step 5087 | loss_total 1.5743\n",
      "step 2/3 | epoch 35/50 | batch 48/60 | global_step 5088 | loss_total 1.9643\n",
      "step 2/3 | epoch 35/50 | batch 49/60 | global_step 5089 | loss_total 0.8417\n",
      "step 2/3 | epoch 35/50 | batch 50/60 | global_step 5090 | loss_total 0.9054\n",
      "step 2/3 | epoch 35/50 | batch 51/60 | global_step 5091 | loss_total 0.8281\n",
      "step 2/3 | epoch 35/50 | batch 52/60 | global_step 5092 | loss_total 0.7755\n",
      "step 2/3 | epoch 35/50 | batch 53/60 | global_step 5093 | loss_total 1.3367\n",
      "step 2/3 | epoch 35/50 | batch 54/60 | global_step 5094 | loss_total 0.7820\n",
      "step 2/3 | epoch 35/50 | batch 55/60 | global_step 5095 | loss_total 1.3214\n",
      "step 2/3 | epoch 35/50 | batch 56/60 | global_step 5096 | loss_total 0.8674\n",
      "step 2/3 | epoch 35/50 | batch 57/60 | global_step 5097 | loss_total 1.7188\n",
      "step 2/3 | epoch 35/50 | batch 58/60 | global_step 5098 | loss_total 0.8138\n",
      "step 2/3 | epoch 35/50 | batch 59/60 | global_step 5099 | loss_total 0.7531\n",
      "step 2/3 | epoch 35/50 | batch 60/60 | global_step 5100 | loss_total 1.4614\n",
      "[epoch done] step 2/3 epoch 35/50 | train_total=1.1367 val_total=0.9597\n",
      "step 2/3 | epoch 36/50 | batch 1/60 | global_step 5101 | loss_total 0.8139\n",
      "step 2/3 | epoch 36/50 | batch 2/60 | global_step 5102 | loss_total 0.8869\n",
      "step 2/3 | epoch 36/50 | batch 3/60 | global_step 5103 | loss_total 1.2538\n",
      "step 2/3 | epoch 36/50 | batch 4/60 | global_step 5104 | loss_total 1.3780\n",
      "step 2/3 | epoch 36/50 | batch 5/60 | global_step 5105 | loss_total 0.7529\n",
      "step 2/3 | epoch 36/50 | batch 6/60 | global_step 5106 | loss_total 0.7566\n",
      "step 2/3 | epoch 36/50 | batch 7/60 | global_step 5107 | loss_total 0.9343\n",
      "step 2/3 | epoch 36/50 | batch 8/60 | global_step 5108 | loss_total 0.7599\n",
      "step 2/3 | epoch 36/50 | batch 9/60 | global_step 5109 | loss_total 0.8310\n",
      "step 2/3 | epoch 36/50 | batch 10/60 | global_step 5110 | loss_total 1.2515\n",
      "step 2/3 | epoch 36/50 | batch 11/60 | global_step 5111 | loss_total 1.6427\n",
      "step 2/3 | epoch 36/50 | batch 12/60 | global_step 5112 | loss_total 1.0393\n",
      "step 2/3 | epoch 36/50 | batch 13/60 | global_step 5113 | loss_total 0.8168\n",
      "step 2/3 | epoch 36/50 | batch 14/60 | global_step 5114 | loss_total 0.8146\n",
      "step 2/3 | epoch 36/50 | batch 15/60 | global_step 5115 | loss_total 0.6848\n",
      "step 2/3 | epoch 36/50 | batch 16/60 | global_step 5116 | loss_total 0.8089\n",
      "step 2/3 | epoch 36/50 | batch 17/60 | global_step 5117 | loss_total 2.0899\n",
      "step 2/3 | epoch 36/50 | batch 18/60 | global_step 5118 | loss_total 0.8826\n",
      "step 2/3 | epoch 36/50 | batch 19/60 | global_step 5119 | loss_total 0.7415\n",
      "step 2/3 | epoch 36/50 | batch 20/60 | global_step 5120 | loss_total 0.8752\n",
      "step 2/3 | epoch 36/50 | batch 21/60 | global_step 5121 | loss_total 1.6262\n",
      "step 2/3 | epoch 36/50 | batch 22/60 | global_step 5122 | loss_total 0.8420\n",
      "step 2/3 | epoch 36/50 | batch 23/60 | global_step 5123 | loss_total 0.8376\n",
      "step 2/3 | epoch 36/50 | batch 24/60 | global_step 5124 | loss_total 0.7595\n",
      "step 2/3 | epoch 36/50 | batch 25/60 | global_step 5125 | loss_total 0.7406\n",
      "step 2/3 | epoch 36/50 | batch 26/60 | global_step 5126 | loss_total 1.5181\n",
      "step 2/3 | epoch 36/50 | batch 27/60 | global_step 5127 | loss_total 0.7583\n",
      "step 2/3 | epoch 36/50 | batch 28/60 | global_step 5128 | loss_total 0.7589\n",
      "step 2/3 | epoch 36/50 | batch 29/60 | global_step 5129 | loss_total 0.8187\n",
      "step 2/3 | epoch 36/50 | batch 30/60 | global_step 5130 | loss_total 1.5663\n",
      "step 2/3 | epoch 36/50 | batch 31/60 | global_step 5131 | loss_total 0.7846\n",
      "step 2/3 | epoch 36/50 | batch 32/60 | global_step 5132 | loss_total 2.1703\n",
      "step 2/3 | epoch 36/50 | batch 33/60 | global_step 5133 | loss_total 1.4038\n",
      "step 2/3 | epoch 36/50 | batch 34/60 | global_step 5134 | loss_total 0.7878\n",
      "step 2/3 | epoch 36/50 | batch 35/60 | global_step 5135 | loss_total 0.7848\n",
      "step 2/3 | epoch 36/50 | batch 36/60 | global_step 5136 | loss_total 0.7936\n",
      "step 2/3 | epoch 36/50 | batch 37/60 | global_step 5137 | loss_total 0.7336\n",
      "step 2/3 | epoch 36/50 | batch 38/60 | global_step 5138 | loss_total 1.3764\n",
      "step 2/3 | epoch 36/50 | batch 39/60 | global_step 5139 | loss_total 0.9540\n",
      "step 2/3 | epoch 36/50 | batch 40/60 | global_step 5140 | loss_total 0.8261\n",
      "step 2/3 | epoch 36/50 | batch 41/60 | global_step 5141 | loss_total 2.1683\n",
      "step 2/3 | epoch 36/50 | batch 42/60 | global_step 5142 | loss_total 1.4811\n",
      "step 2/3 | epoch 36/50 | batch 43/60 | global_step 5143 | loss_total 0.8037\n",
      "step 2/3 | epoch 36/50 | batch 44/60 | global_step 5144 | loss_total 0.7591\n",
      "step 2/3 | epoch 36/50 | batch 45/60 | global_step 5145 | loss_total 2.5390\n",
      "step 2/3 | epoch 36/50 | batch 46/60 | global_step 5146 | loss_total 0.7750\n",
      "step 2/3 | epoch 36/50 | batch 47/60 | global_step 5147 | loss_total 0.7881\n",
      "step 2/3 | epoch 36/50 | batch 48/60 | global_step 5148 | loss_total 0.7854\n",
      "step 2/3 | epoch 36/50 | batch 49/60 | global_step 5149 | loss_total 0.7933\n",
      "step 2/3 | epoch 36/50 | batch 50/60 | global_step 5150 | loss_total 0.7761\n",
      "step 2/3 | epoch 36/50 | batch 51/60 | global_step 5151 | loss_total 0.7830\n",
      "step 2/3 | epoch 36/50 | batch 52/60 | global_step 5152 | loss_total 0.7948\n",
      "step 2/3 | epoch 36/50 | batch 53/60 | global_step 5153 | loss_total 0.7820\n",
      "step 2/3 | epoch 36/50 | batch 54/60 | global_step 5154 | loss_total 0.8287\n",
      "step 2/3 | epoch 36/50 | batch 55/60 | global_step 5155 | loss_total 1.4962\n",
      "step 2/3 | epoch 36/50 | batch 56/60 | global_step 5156 | loss_total 1.6709\n",
      "step 2/3 | epoch 36/50 | batch 57/60 | global_step 5157 | loss_total 2.0402\n",
      "step 2/3 | epoch 36/50 | batch 58/60 | global_step 5158 | loss_total 1.7797\n",
      "step 2/3 | epoch 36/50 | batch 59/60 | global_step 5159 | loss_total 2.5952\n",
      "step 2/3 | epoch 36/50 | batch 60/60 | global_step 5160 | loss_total 0.7891\n",
      "[epoch done] step 2/3 epoch 36/50 | train_total=1.1014 val_total=0.7409\n",
      "step 2/3 | epoch 37/50 | batch 1/60 | global_step 5161 | loss_total 1.3445\n",
      "step 2/3 | epoch 37/50 | batch 2/60 | global_step 5162 | loss_total 1.5163\n",
      "step 2/3 | epoch 37/50 | batch 3/60 | global_step 5163 | loss_total 1.7636\n",
      "step 2/3 | epoch 37/50 | batch 4/60 | global_step 5164 | loss_total 0.7962\n",
      "step 2/3 | epoch 37/50 | batch 5/60 | global_step 5165 | loss_total 0.7892\n",
      "step 2/3 | epoch 37/50 | batch 6/60 | global_step 5166 | loss_total 1.3080\n",
      "step 2/3 | epoch 37/50 | batch 7/60 | global_step 5167 | loss_total 0.7876\n",
      "step 2/3 | epoch 37/50 | batch 8/60 | global_step 5168 | loss_total 0.7956\n",
      "step 2/3 | epoch 37/50 | batch 9/60 | global_step 5169 | loss_total 1.5968\n",
      "step 2/3 | epoch 37/50 | batch 10/60 | global_step 5170 | loss_total 0.7748\n",
      "step 2/3 | epoch 37/50 | batch 11/60 | global_step 5171 | loss_total 0.8300\n",
      "step 2/3 | epoch 37/50 | batch 12/60 | global_step 5172 | loss_total 2.4502\n",
      "step 2/3 | epoch 37/50 | batch 13/60 | global_step 5173 | loss_total 0.8225\n",
      "step 2/3 | epoch 37/50 | batch 14/60 | global_step 5174 | loss_total 0.7774\n",
      "step 2/3 | epoch 37/50 | batch 15/60 | global_step 5175 | loss_total 0.8136\n",
      "step 2/3 | epoch 37/50 | batch 16/60 | global_step 5176 | loss_total 0.7819\n",
      "step 2/3 | epoch 37/50 | batch 17/60 | global_step 5177 | loss_total 0.7734\n",
      "step 2/3 | epoch 37/50 | batch 18/60 | global_step 5178 | loss_total 0.8539\n",
      "step 2/3 | epoch 37/50 | batch 19/60 | global_step 5179 | loss_total 0.8161\n",
      "step 2/3 | epoch 37/50 | batch 20/60 | global_step 5180 | loss_total 1.5621\n",
      "step 2/3 | epoch 37/50 | batch 21/60 | global_step 5181 | loss_total 0.7869\n",
      "step 2/3 | epoch 37/50 | batch 22/60 | global_step 5182 | loss_total 0.7937\n",
      "step 2/3 | epoch 37/50 | batch 23/60 | global_step 5183 | loss_total 0.8042\n",
      "step 2/3 | epoch 37/50 | batch 24/60 | global_step 5184 | loss_total 1.3979\n",
      "step 2/3 | epoch 37/50 | batch 25/60 | global_step 5185 | loss_total 0.8016\n",
      "step 2/3 | epoch 37/50 | batch 26/60 | global_step 5186 | loss_total 0.7921\n",
      "step 2/3 | epoch 37/50 | batch 27/60 | global_step 5187 | loss_total 1.2778\n",
      "step 2/3 | epoch 37/50 | batch 28/60 | global_step 5188 | loss_total 1.3356\n",
      "step 2/3 | epoch 37/50 | batch 29/60 | global_step 5189 | loss_total 0.7908\n",
      "step 2/3 | epoch 37/50 | batch 30/60 | global_step 5190 | loss_total 0.8545\n",
      "step 2/3 | epoch 37/50 | batch 31/60 | global_step 5191 | loss_total 0.8015\n",
      "step 2/3 | epoch 37/50 | batch 32/60 | global_step 5192 | loss_total 0.7881\n",
      "step 2/3 | epoch 37/50 | batch 33/60 | global_step 5193 | loss_total 0.7953\n",
      "step 2/3 | epoch 37/50 | batch 34/60 | global_step 5194 | loss_total 0.7846\n",
      "step 2/3 | epoch 37/50 | batch 35/60 | global_step 5195 | loss_total 1.9068\n",
      "step 2/3 | epoch 37/50 | batch 36/60 | global_step 5196 | loss_total 1.0143\n",
      "step 2/3 | epoch 37/50 | batch 37/60 | global_step 5197 | loss_total 2.5019\n",
      "step 2/3 | epoch 37/50 | batch 38/60 | global_step 5198 | loss_total 0.7881\n",
      "step 2/3 | epoch 37/50 | batch 39/60 | global_step 5199 | loss_total 0.7755\n",
      "step 2/3 | epoch 37/50 | batch 40/60 | global_step 5200 | loss_total 0.7824\n",
      "step 2/3 | epoch 37/50 | batch 41/60 | global_step 5201 | loss_total 1.3165\n",
      "step 2/3 | epoch 37/50 | batch 42/60 | global_step 5202 | loss_total 0.8657\n",
      "step 2/3 | epoch 37/50 | batch 43/60 | global_step 5203 | loss_total 0.7649\n",
      "step 2/3 | epoch 37/50 | batch 44/60 | global_step 5204 | loss_total 1.5614\n",
      "step 2/3 | epoch 37/50 | batch 45/60 | global_step 5205 | loss_total 1.8627\n",
      "step 2/3 | epoch 37/50 | batch 46/60 | global_step 5206 | loss_total 0.7819\n",
      "step 2/3 | epoch 37/50 | batch 47/60 | global_step 5207 | loss_total 0.7906\n",
      "step 2/3 | epoch 37/50 | batch 48/60 | global_step 5208 | loss_total 0.7802\n",
      "step 2/3 | epoch 37/50 | batch 49/60 | global_step 5209 | loss_total 0.7631\n",
      "step 2/3 | epoch 37/50 | batch 50/60 | global_step 5210 | loss_total 0.7833\n",
      "step 2/3 | epoch 37/50 | batch 51/60 | global_step 5211 | loss_total 0.7798\n",
      "step 2/3 | epoch 37/50 | batch 52/60 | global_step 5212 | loss_total 0.7682\n",
      "step 2/3 | epoch 37/50 | batch 53/60 | global_step 5213 | loss_total 1.3710\n",
      "step 2/3 | epoch 37/50 | batch 54/60 | global_step 5214 | loss_total 0.7519\n",
      "step 2/3 | epoch 37/50 | batch 55/60 | global_step 5215 | loss_total 1.4224\n",
      "step 2/3 | epoch 37/50 | batch 56/60 | global_step 5216 | loss_total 2.0430\n",
      "step 2/3 | epoch 37/50 | batch 57/60 | global_step 5217 | loss_total 0.8366\n",
      "step 2/3 | epoch 37/50 | batch 58/60 | global_step 5218 | loss_total 1.6096\n",
      "step 2/3 | epoch 37/50 | batch 59/60 | global_step 5219 | loss_total 0.7442\n",
      "step 2/3 | epoch 37/50 | batch 60/60 | global_step 5220 | loss_total 1.3486\n",
      "[epoch done] step 2/3 epoch 37/50 | train_total=1.0745 val_total=0.9235\n",
      "step 2/3 | epoch 38/50 | batch 1/60 | global_step 5221 | loss_total 0.6778\n",
      "step 2/3 | epoch 38/50 | batch 2/60 | global_step 5222 | loss_total 0.6727\n",
      "step 2/3 | epoch 38/50 | batch 3/60 | global_step 5223 | loss_total 1.5898\n",
      "step 2/3 | epoch 38/50 | batch 4/60 | global_step 5224 | loss_total 0.7984\n",
      "step 2/3 | epoch 38/50 | batch 5/60 | global_step 5225 | loss_total 0.6394\n",
      "step 2/3 | epoch 38/50 | batch 6/60 | global_step 5226 | loss_total 2.8213\n",
      "step 2/3 | epoch 38/50 | batch 7/60 | global_step 5227 | loss_total 0.8172\n",
      "step 2/3 | epoch 38/50 | batch 8/60 | global_step 5228 | loss_total 1.8492\n",
      "step 2/3 | epoch 38/50 | batch 9/60 | global_step 5229 | loss_total 0.8100\n",
      "step 2/3 | epoch 38/50 | batch 10/60 | global_step 5230 | loss_total 0.7342\n",
      "step 2/3 | epoch 38/50 | batch 11/60 | global_step 5231 | loss_total 0.9038\n",
      "step 2/3 | epoch 38/50 | batch 12/60 | global_step 5232 | loss_total 1.7393\n",
      "step 2/3 | epoch 38/50 | batch 13/60 | global_step 5233 | loss_total 0.6229\n",
      "step 2/3 | epoch 38/50 | batch 14/60 | global_step 5234 | loss_total 0.9001\n",
      "step 2/3 | epoch 38/50 | batch 15/60 | global_step 5235 | loss_total 0.8053\n",
      "step 2/3 | epoch 38/50 | batch 16/60 | global_step 5236 | loss_total 0.7994\n",
      "step 2/3 | epoch 38/50 | batch 17/60 | global_step 5237 | loss_total 0.8862\n",
      "step 2/3 | epoch 38/50 | batch 18/60 | global_step 5238 | loss_total 0.7165\n",
      "step 2/3 | epoch 38/50 | batch 19/60 | global_step 5239 | loss_total 1.3704\n",
      "step 2/3 | epoch 38/50 | batch 20/60 | global_step 5240 | loss_total 0.9956\n",
      "step 2/3 | epoch 38/50 | batch 21/60 | global_step 5241 | loss_total 1.4473\n",
      "step 2/3 | epoch 38/50 | batch 22/60 | global_step 5242 | loss_total 1.8067\n",
      "step 2/3 | epoch 38/50 | batch 23/60 | global_step 5243 | loss_total 0.7329\n",
      "step 2/3 | epoch 38/50 | batch 24/60 | global_step 5244 | loss_total 0.6926\n",
      "step 2/3 | epoch 38/50 | batch 25/60 | global_step 5245 | loss_total 1.3663\n",
      "step 2/3 | epoch 38/50 | batch 26/60 | global_step 5246 | loss_total 0.8390\n",
      "step 2/3 | epoch 38/50 | batch 27/60 | global_step 5247 | loss_total 1.7795\n",
      "step 2/3 | epoch 38/50 | batch 28/60 | global_step 5248 | loss_total 1.6997\n",
      "step 2/3 | epoch 38/50 | batch 29/60 | global_step 5249 | loss_total 0.7743\n",
      "step 2/3 | epoch 38/50 | batch 30/60 | global_step 5250 | loss_total 0.8481\n",
      "step 2/3 | epoch 38/50 | batch 31/60 | global_step 5251 | loss_total 0.8957\n",
      "step 2/3 | epoch 38/50 | batch 32/60 | global_step 5252 | loss_total 0.7573\n",
      "step 2/3 | epoch 38/50 | batch 33/60 | global_step 5253 | loss_total 0.8187\n",
      "step 2/3 | epoch 38/50 | batch 34/60 | global_step 5254 | loss_total 0.8017\n",
      "step 2/3 | epoch 38/50 | batch 35/60 | global_step 5255 | loss_total 0.8634\n",
      "step 2/3 | epoch 38/50 | batch 36/60 | global_step 5256 | loss_total 1.3426\n",
      "step 2/3 | epoch 38/50 | batch 37/60 | global_step 5257 | loss_total 0.7492\n",
      "step 2/3 | epoch 38/50 | batch 38/60 | global_step 5258 | loss_total 1.3833\n",
      "step 2/3 | epoch 38/50 | batch 39/60 | global_step 5259 | loss_total 1.8585\n",
      "step 2/3 | epoch 38/50 | batch 40/60 | global_step 5260 | loss_total 0.7213\n",
      "step 2/3 | epoch 38/50 | batch 41/60 | global_step 5261 | loss_total 0.8021\n",
      "step 2/3 | epoch 38/50 | batch 42/60 | global_step 5262 | loss_total 1.6134\n",
      "step 2/3 | epoch 38/50 | batch 43/60 | global_step 5263 | loss_total 0.8077\n",
      "step 2/3 | epoch 38/50 | batch 44/60 | global_step 5264 | loss_total 0.7439\n",
      "step 2/3 | epoch 38/50 | batch 45/60 | global_step 5265 | loss_total 1.9546\n",
      "step 2/3 | epoch 38/50 | batch 46/60 | global_step 5266 | loss_total 0.8013\n",
      "step 2/3 | epoch 38/50 | batch 47/60 | global_step 5267 | loss_total 1.4972\n",
      "step 2/3 | epoch 38/50 | batch 48/60 | global_step 5268 | loss_total 0.7350\n",
      "step 2/3 | epoch 38/50 | batch 49/60 | global_step 5269 | loss_total 2.0048\n",
      "step 2/3 | epoch 38/50 | batch 50/60 | global_step 5270 | loss_total 1.2775\n",
      "step 2/3 | epoch 38/50 | batch 51/60 | global_step 5271 | loss_total 1.4530\n",
      "step 2/3 | epoch 38/50 | batch 52/60 | global_step 5272 | loss_total 0.7314\n",
      "step 2/3 | epoch 38/50 | batch 53/60 | global_step 5273 | loss_total 1.8211\n",
      "step 2/3 | epoch 38/50 | batch 54/60 | global_step 5274 | loss_total 1.5158\n",
      "step 2/3 | epoch 38/50 | batch 55/60 | global_step 5275 | loss_total 0.8232\n",
      "step 2/3 | epoch 38/50 | batch 56/60 | global_step 5276 | loss_total 1.4979\n",
      "step 2/3 | epoch 38/50 | batch 57/60 | global_step 5277 | loss_total 1.4047\n",
      "step 2/3 | epoch 38/50 | batch 58/60 | global_step 5278 | loss_total 0.8566\n",
      "step 2/3 | epoch 38/50 | batch 59/60 | global_step 5279 | loss_total 1.2651\n",
      "step 2/3 | epoch 38/50 | batch 60/60 | global_step 5280 | loss_total 1.0041\n",
      "[epoch done] step 2/3 epoch 38/50 | train_total=1.1323 val_total=1.0384\n",
      "step 2/3 | epoch 39/50 | batch 1/60 | global_step 5281 | loss_total 0.7009\n",
      "step 2/3 | epoch 39/50 | batch 2/60 | global_step 5282 | loss_total 0.8369\n",
      "step 2/3 | epoch 39/50 | batch 3/60 | global_step 5283 | loss_total 0.7555\n",
      "step 2/3 | epoch 39/50 | batch 4/60 | global_step 5284 | loss_total 0.9150\n",
      "step 2/3 | epoch 39/50 | batch 5/60 | global_step 5285 | loss_total 0.8026\n",
      "step 2/3 | epoch 39/50 | batch 6/60 | global_step 5286 | loss_total 0.7572\n",
      "step 2/3 | epoch 39/50 | batch 7/60 | global_step 5287 | loss_total 0.8488\n",
      "step 2/3 | epoch 39/50 | batch 8/60 | global_step 5288 | loss_total 0.7937\n",
      "step 2/3 | epoch 39/50 | batch 9/60 | global_step 5289 | loss_total 0.7905\n",
      "step 2/3 | epoch 39/50 | batch 10/60 | global_step 5290 | loss_total 1.3565\n",
      "step 2/3 | epoch 39/50 | batch 11/60 | global_step 5291 | loss_total 0.7852\n",
      "step 2/3 | epoch 39/50 | batch 12/60 | global_step 5292 | loss_total 0.7826\n",
      "step 2/3 | epoch 39/50 | batch 13/60 | global_step 5293 | loss_total 0.7929\n",
      "step 2/3 | epoch 39/50 | batch 14/60 | global_step 5294 | loss_total 1.7542\n",
      "step 2/3 | epoch 39/50 | batch 15/60 | global_step 5295 | loss_total 0.8727\n",
      "step 2/3 | epoch 39/50 | batch 16/60 | global_step 5296 | loss_total 0.7782\n",
      "step 2/3 | epoch 39/50 | batch 17/60 | global_step 5297 | loss_total 0.8026\n",
      "step 2/3 | epoch 39/50 | batch 18/60 | global_step 5298 | loss_total 1.8831\n",
      "step 2/3 | epoch 39/50 | batch 19/60 | global_step 5299 | loss_total 0.7411\n",
      "step 2/3 | epoch 39/50 | batch 20/60 | global_step 5300 | loss_total 1.8815\n",
      "step 2/3 | epoch 39/50 | batch 21/60 | global_step 5301 | loss_total 1.7174\n",
      "step 2/3 | epoch 39/50 | batch 22/60 | global_step 5302 | loss_total 0.7741\n",
      "step 2/3 | epoch 39/50 | batch 23/60 | global_step 5303 | loss_total 0.7254\n",
      "step 2/3 | epoch 39/50 | batch 24/60 | global_step 5304 | loss_total 1.4506\n",
      "step 2/3 | epoch 39/50 | batch 25/60 | global_step 5305 | loss_total 1.5822\n",
      "step 2/3 | epoch 39/50 | batch 26/60 | global_step 5306 | loss_total 0.8476\n",
      "step 2/3 | epoch 39/50 | batch 27/60 | global_step 5307 | loss_total 0.7806\n",
      "step 2/3 | epoch 39/50 | batch 28/60 | global_step 5308 | loss_total 0.6946\n",
      "step 2/3 | epoch 39/50 | batch 29/60 | global_step 5309 | loss_total 0.7297\n",
      "step 2/3 | epoch 39/50 | batch 30/60 | global_step 5310 | loss_total 0.8687\n",
      "step 2/3 | epoch 39/50 | batch 31/60 | global_step 5311 | loss_total 0.7774\n",
      "step 2/3 | epoch 39/50 | batch 32/60 | global_step 5312 | loss_total 0.7224\n",
      "step 2/3 | epoch 39/50 | batch 33/60 | global_step 5313 | loss_total 0.7740\n",
      "step 2/3 | epoch 39/50 | batch 34/60 | global_step 5314 | loss_total 1.8696\n",
      "step 2/3 | epoch 39/50 | batch 35/60 | global_step 5315 | loss_total 0.7722\n",
      "step 2/3 | epoch 39/50 | batch 36/60 | global_step 5316 | loss_total 0.7710\n",
      "step 2/3 | epoch 39/50 | batch 37/60 | global_step 5317 | loss_total 1.6671\n",
      "step 2/3 | epoch 39/50 | batch 38/60 | global_step 5318 | loss_total 0.8203\n",
      "step 2/3 | epoch 39/50 | batch 39/60 | global_step 5319 | loss_total 0.7764\n",
      "step 2/3 | epoch 39/50 | batch 40/60 | global_step 5320 | loss_total 0.8108\n",
      "step 2/3 | epoch 39/50 | batch 41/60 | global_step 5321 | loss_total 0.7964\n",
      "step 2/3 | epoch 39/50 | batch 42/60 | global_step 5322 | loss_total 1.7189\n",
      "step 2/3 | epoch 39/50 | batch 43/60 | global_step 5323 | loss_total 0.7749\n",
      "step 2/3 | epoch 39/50 | batch 44/60 | global_step 5324 | loss_total 0.7663\n",
      "step 2/3 | epoch 39/50 | batch 45/60 | global_step 5325 | loss_total 0.7790\n",
      "step 2/3 | epoch 39/50 | batch 46/60 | global_step 5326 | loss_total 2.5139\n",
      "step 2/3 | epoch 39/50 | batch 47/60 | global_step 5327 | loss_total 1.3663\n",
      "step 2/3 | epoch 39/50 | batch 48/60 | global_step 5328 | loss_total 0.7633\n",
      "step 2/3 | epoch 39/50 | batch 49/60 | global_step 5329 | loss_total 0.7609\n",
      "step 2/3 | epoch 39/50 | batch 50/60 | global_step 5330 | loss_total 1.6937\n",
      "step 2/3 | epoch 39/50 | batch 51/60 | global_step 5331 | loss_total 2.5721\n",
      "step 2/3 | epoch 39/50 | batch 52/60 | global_step 5332 | loss_total 2.0944\n",
      "step 2/3 | epoch 39/50 | batch 53/60 | global_step 5333 | loss_total 0.8527\n",
      "step 2/3 | epoch 39/50 | batch 54/60 | global_step 5334 | loss_total 0.7170\n",
      "step 2/3 | epoch 39/50 | batch 55/60 | global_step 5335 | loss_total 1.4115\n",
      "step 2/3 | epoch 39/50 | batch 56/60 | global_step 5336 | loss_total 0.8457\n",
      "step 2/3 | epoch 39/50 | batch 57/60 | global_step 5337 | loss_total 0.7388\n",
      "step 2/3 | epoch 39/50 | batch 58/60 | global_step 5338 | loss_total 1.6529\n",
      "step 2/3 | epoch 39/50 | batch 59/60 | global_step 5339 | loss_total 0.7424\n",
      "step 2/3 | epoch 39/50 | batch 60/60 | global_step 5340 | loss_total 0.8297\n",
      "[epoch done] step 2/3 epoch 39/50 | train_total=1.0659 val_total=0.9256\n",
      "step 2/3 | epoch 40/50 | batch 1/60 | global_step 5341 | loss_total 0.8710\n",
      "step 2/3 | epoch 40/50 | batch 2/60 | global_step 5342 | loss_total 0.8235\n",
      "step 2/3 | epoch 40/50 | batch 3/60 | global_step 5343 | loss_total 1.3621\n",
      "step 2/3 | epoch 40/50 | batch 4/60 | global_step 5344 | loss_total 0.7810\n",
      "step 2/3 | epoch 40/50 | batch 5/60 | global_step 5345 | loss_total 0.8524\n",
      "step 2/3 | epoch 40/50 | batch 6/60 | global_step 5346 | loss_total 0.7324\n",
      "step 2/3 | epoch 40/50 | batch 7/60 | global_step 5347 | loss_total 0.7795\n",
      "step 2/3 | epoch 40/50 | batch 8/60 | global_step 5348 | loss_total 0.9024\n",
      "step 2/3 | epoch 40/50 | batch 9/60 | global_step 5349 | loss_total 2.5896\n",
      "step 2/3 | epoch 40/50 | batch 10/60 | global_step 5350 | loss_total 0.7424\n",
      "step 2/3 | epoch 40/50 | batch 11/60 | global_step 5351 | loss_total 2.1969\n",
      "step 2/3 | epoch 40/50 | batch 12/60 | global_step 5352 | loss_total 0.7696\n",
      "step 2/3 | epoch 40/50 | batch 13/60 | global_step 5353 | loss_total 1.6601\n",
      "step 2/3 | epoch 40/50 | batch 14/60 | global_step 5354 | loss_total 0.7704\n",
      "step 2/3 | epoch 40/50 | batch 15/60 | global_step 5355 | loss_total 0.7448\n",
      "step 2/3 | epoch 40/50 | batch 16/60 | global_step 5356 | loss_total 0.7705\n",
      "step 2/3 | epoch 40/50 | batch 17/60 | global_step 5357 | loss_total 0.7722\n",
      "step 2/3 | epoch 40/50 | batch 18/60 | global_step 5358 | loss_total 0.8143\n",
      "step 2/3 | epoch 40/50 | batch 19/60 | global_step 5359 | loss_total 0.7386\n",
      "step 2/3 | epoch 40/50 | batch 20/60 | global_step 5360 | loss_total 1.3124\n",
      "step 2/3 | epoch 40/50 | batch 21/60 | global_step 5361 | loss_total 1.3419\n",
      "step 2/3 | epoch 40/50 | batch 22/60 | global_step 5362 | loss_total 0.8097\n",
      "step 2/3 | epoch 40/50 | batch 23/60 | global_step 5363 | loss_total 1.5243\n",
      "step 2/3 | epoch 40/50 | batch 24/60 | global_step 5364 | loss_total 0.7379\n",
      "step 2/3 | epoch 40/50 | batch 25/60 | global_step 5365 | loss_total 0.7712\n",
      "step 2/3 | epoch 40/50 | batch 26/60 | global_step 5366 | loss_total 1.7300\n",
      "step 2/3 | epoch 40/50 | batch 27/60 | global_step 5367 | loss_total 2.1143\n",
      "step 2/3 | epoch 40/50 | batch 28/60 | global_step 5368 | loss_total 0.7791\n",
      "step 2/3 | epoch 40/50 | batch 29/60 | global_step 5369 | loss_total 0.7503\n",
      "step 2/3 | epoch 40/50 | batch 30/60 | global_step 5370 | loss_total 0.8129\n",
      "step 2/3 | epoch 40/50 | batch 31/60 | global_step 5371 | loss_total 0.9232\n",
      "step 2/3 | epoch 40/50 | batch 32/60 | global_step 5372 | loss_total 0.9004\n",
      "step 2/3 | epoch 40/50 | batch 33/60 | global_step 5373 | loss_total 0.7049\n",
      "step 2/3 | epoch 40/50 | batch 34/60 | global_step 5374 | loss_total 1.3288\n",
      "step 2/3 | epoch 40/50 | batch 35/60 | global_step 5375 | loss_total 0.7341\n",
      "step 2/3 | epoch 40/50 | batch 36/60 | global_step 5376 | loss_total 0.8478\n",
      "step 2/3 | epoch 40/50 | batch 37/60 | global_step 5377 | loss_total 0.7863\n",
      "step 2/3 | epoch 40/50 | batch 38/60 | global_step 5378 | loss_total 1.5721\n",
      "step 2/3 | epoch 40/50 | batch 39/60 | global_step 5379 | loss_total 0.7760\n",
      "step 2/3 | epoch 40/50 | batch 40/60 | global_step 5380 | loss_total 0.7240\n",
      "step 2/3 | epoch 40/50 | batch 41/60 | global_step 5381 | loss_total 1.4575\n",
      "step 2/3 | epoch 40/50 | batch 42/60 | global_step 5382 | loss_total 0.7919\n",
      "step 2/3 | epoch 40/50 | batch 43/60 | global_step 5383 | loss_total 0.7059\n",
      "step 2/3 | epoch 40/50 | batch 44/60 | global_step 5384 | loss_total 0.7925\n",
      "step 2/3 | epoch 40/50 | batch 45/60 | global_step 5385 | loss_total 1.4683\n",
      "step 2/3 | epoch 40/50 | batch 46/60 | global_step 5386 | loss_total 0.8309\n",
      "step 2/3 | epoch 40/50 | batch 47/60 | global_step 5387 | loss_total 0.6330\n",
      "step 2/3 | epoch 40/50 | batch 48/60 | global_step 5388 | loss_total 0.7129\n",
      "step 2/3 | epoch 40/50 | batch 49/60 | global_step 5389 | loss_total 1.3011\n",
      "step 2/3 | epoch 40/50 | batch 50/60 | global_step 5390 | loss_total 2.2190\n",
      "step 2/3 | epoch 40/50 | batch 51/60 | global_step 5391 | loss_total 0.8078\n",
      "step 2/3 | epoch 40/50 | batch 52/60 | global_step 5392 | loss_total 0.8749\n",
      "step 2/3 | epoch 40/50 | batch 53/60 | global_step 5393 | loss_total 0.7802\n",
      "step 2/3 | epoch 40/50 | batch 54/60 | global_step 5394 | loss_total 0.7835\n",
      "step 2/3 | epoch 40/50 | batch 55/60 | global_step 5395 | loss_total 0.7026\n",
      "step 2/3 | epoch 40/50 | batch 56/60 | global_step 5396 | loss_total 0.8509\n",
      "step 2/3 | epoch 40/50 | batch 57/60 | global_step 5397 | loss_total 0.7803\n",
      "step 2/3 | epoch 40/50 | batch 58/60 | global_step 5398 | loss_total 0.8336\n",
      "step 2/3 | epoch 40/50 | batch 59/60 | global_step 5399 | loss_total 2.2249\n",
      "step 2/3 | epoch 40/50 | batch 60/60 | global_step 5400 | loss_total 0.7715\n",
      "[epoch done] step 2/3 epoch 40/50 | train_total=1.0330 val_total=0.8659\n",
      "step 2/3 | epoch 41/50 | batch 1/60 | global_step 5401 | loss_total 2.0821\n",
      "step 2/3 | epoch 41/50 | batch 2/60 | global_step 5402 | loss_total 0.7627\n",
      "step 2/3 | epoch 41/50 | batch 3/60 | global_step 5403 | loss_total 0.9267\n",
      "step 2/3 | epoch 41/50 | batch 4/60 | global_step 5404 | loss_total 0.7655\n",
      "step 2/3 | epoch 41/50 | batch 5/60 | global_step 5405 | loss_total 1.8294\n",
      "step 2/3 | epoch 41/50 | batch 6/60 | global_step 5406 | loss_total 1.4300\n",
      "step 2/3 | epoch 41/50 | batch 7/60 | global_step 5407 | loss_total 1.8790\n",
      "step 2/3 | epoch 41/50 | batch 8/60 | global_step 5408 | loss_total 0.7673\n",
      "step 2/3 | epoch 41/50 | batch 9/60 | global_step 5409 | loss_total 1.4484\n",
      "step 2/3 | epoch 41/50 | batch 10/60 | global_step 5410 | loss_total 1.8466\n",
      "step 2/3 | epoch 41/50 | batch 11/60 | global_step 5411 | loss_total 0.7676\n",
      "step 2/3 | epoch 41/50 | batch 12/60 | global_step 5412 | loss_total 1.9590\n",
      "step 2/3 | epoch 41/50 | batch 13/60 | global_step 5413 | loss_total 0.7732\n",
      "step 2/3 | epoch 41/50 | batch 14/60 | global_step 5414 | loss_total 0.7670\n",
      "step 2/3 | epoch 41/50 | batch 15/60 | global_step 5415 | loss_total 0.7695\n",
      "step 2/3 | epoch 41/50 | batch 16/60 | global_step 5416 | loss_total 0.7594\n",
      "step 2/3 | epoch 41/50 | batch 17/60 | global_step 5417 | loss_total 0.7560\n",
      "step 2/3 | epoch 41/50 | batch 18/60 | global_step 5418 | loss_total 0.8183\n",
      "step 2/3 | epoch 41/50 | batch 19/60 | global_step 5419 | loss_total 0.8697\n",
      "step 2/3 | epoch 41/50 | batch 20/60 | global_step 5420 | loss_total 1.9980\n",
      "step 2/3 | epoch 41/50 | batch 21/60 | global_step 5421 | loss_total 1.3250\n",
      "step 2/3 | epoch 41/50 | batch 22/60 | global_step 5422 | loss_total 0.7394\n",
      "step 2/3 | epoch 41/50 | batch 23/60 | global_step 5423 | loss_total 1.7023\n",
      "step 2/3 | epoch 41/50 | batch 24/60 | global_step 5424 | loss_total 0.8627\n",
      "step 2/3 | epoch 41/50 | batch 25/60 | global_step 5425 | loss_total 1.5421\n",
      "step 2/3 | epoch 41/50 | batch 26/60 | global_step 5426 | loss_total 0.8060\n",
      "step 2/3 | epoch 41/50 | batch 27/60 | global_step 5427 | loss_total 1.2872\n",
      "step 2/3 | epoch 41/50 | batch 28/60 | global_step 5428 | loss_total 1.5727\n",
      "step 2/3 | epoch 41/50 | batch 29/60 | global_step 5429 | loss_total 0.8210\n",
      "step 2/3 | epoch 41/50 | batch 30/60 | global_step 5430 | loss_total 0.8237\n",
      "step 2/3 | epoch 41/50 | batch 31/60 | global_step 5431 | loss_total 0.9070\n",
      "step 2/3 | epoch 41/50 | batch 32/60 | global_step 5432 | loss_total 0.6809\n",
      "step 2/3 | epoch 41/50 | batch 33/60 | global_step 5433 | loss_total 0.9737\n",
      "step 2/3 | epoch 41/50 | batch 34/60 | global_step 5434 | loss_total 1.2705\n",
      "step 2/3 | epoch 41/50 | batch 35/60 | global_step 5435 | loss_total 1.4136\n",
      "step 2/3 | epoch 41/50 | batch 36/60 | global_step 5436 | loss_total 0.7187\n",
      "step 2/3 | epoch 41/50 | batch 37/60 | global_step 5437 | loss_total 0.8103\n",
      "step 2/3 | epoch 41/50 | batch 38/60 | global_step 5438 | loss_total 0.7188\n",
      "step 2/3 | epoch 41/50 | batch 39/60 | global_step 5439 | loss_total 0.9007\n",
      "step 2/3 | epoch 41/50 | batch 40/60 | global_step 5440 | loss_total 0.7216\n",
      "step 2/3 | epoch 41/50 | batch 41/60 | global_step 5441 | loss_total 0.7180\n",
      "step 2/3 | epoch 41/50 | batch 42/60 | global_step 5442 | loss_total 0.7069\n",
      "step 2/3 | epoch 41/50 | batch 43/60 | global_step 5443 | loss_total 0.7028\n",
      "step 2/3 | epoch 41/50 | batch 44/60 | global_step 5444 | loss_total 0.8056\n",
      "step 2/3 | epoch 41/50 | batch 45/60 | global_step 5445 | loss_total 0.6893\n",
      "step 2/3 | epoch 41/50 | batch 46/60 | global_step 5446 | loss_total 0.7985\n",
      "step 2/3 | epoch 41/50 | batch 47/60 | global_step 5447 | loss_total 0.7995\n",
      "step 2/3 | epoch 41/50 | batch 48/60 | global_step 5448 | loss_total 0.7959\n",
      "step 2/3 | epoch 41/50 | batch 49/60 | global_step 5449 | loss_total 1.5026\n",
      "step 2/3 | epoch 41/50 | batch 50/60 | global_step 5450 | loss_total 1.2153\n",
      "step 2/3 | epoch 41/50 | batch 51/60 | global_step 5451 | loss_total 0.9240\n",
      "step 2/3 | epoch 41/50 | batch 52/60 | global_step 5452 | loss_total 0.6366\n",
      "step 2/3 | epoch 41/50 | batch 53/60 | global_step 5453 | loss_total 1.9608\n",
      "step 2/3 | epoch 41/50 | batch 54/60 | global_step 5454 | loss_total 0.7816\n",
      "step 2/3 | epoch 41/50 | batch 55/60 | global_step 5455 | loss_total 2.7500\n",
      "step 2/3 | epoch 41/50 | batch 56/60 | global_step 5456 | loss_total 0.6065\n",
      "step 2/3 | epoch 41/50 | batch 57/60 | global_step 5457 | loss_total 0.9287\n",
      "step 2/3 | epoch 41/50 | batch 58/60 | global_step 5458 | loss_total 0.6912\n",
      "step 2/3 | epoch 41/50 | batch 59/60 | global_step 5459 | loss_total 0.7799\n",
      "step 2/3 | epoch 41/50 | batch 60/60 | global_step 5460 | loss_total 0.8793\n",
      "[epoch done] step 2/3 epoch 41/50 | train_total=1.0708 val_total=0.9970\n",
      "step 2/3 | epoch 42/50 | batch 1/60 | global_step 5461 | loss_total 0.7769\n",
      "step 2/3 | epoch 42/50 | batch 2/60 | global_step 5462 | loss_total 0.7705\n",
      "step 2/3 | epoch 42/50 | batch 3/60 | global_step 5463 | loss_total 1.7151\n",
      "step 2/3 | epoch 42/50 | batch 4/60 | global_step 5464 | loss_total 2.4555\n",
      "step 2/3 | epoch 42/50 | batch 5/60 | global_step 5465 | loss_total 0.8563\n",
      "step 2/3 | epoch 42/50 | batch 6/60 | global_step 5466 | loss_total 1.7887\n",
      "step 2/3 | epoch 42/50 | batch 7/60 | global_step 5467 | loss_total 0.6981\n",
      "step 2/3 | epoch 42/50 | batch 8/60 | global_step 5468 | loss_total 0.8121\n",
      "step 2/3 | epoch 42/50 | batch 9/60 | global_step 5469 | loss_total 1.6501\n",
      "step 2/3 | epoch 42/50 | batch 10/60 | global_step 5470 | loss_total 1.7171\n",
      "step 2/3 | epoch 42/50 | batch 11/60 | global_step 5471 | loss_total 1.5806\n",
      "step 2/3 | epoch 42/50 | batch 12/60 | global_step 5472 | loss_total 0.8223\n",
      "step 2/3 | epoch 42/50 | batch 13/60 | global_step 5473 | loss_total 0.7894\n",
      "step 2/3 | epoch 42/50 | batch 14/60 | global_step 5474 | loss_total 0.7863\n",
      "step 2/3 | epoch 42/50 | batch 15/60 | global_step 5475 | loss_total 1.3744\n",
      "step 2/3 | epoch 42/50 | batch 16/60 | global_step 5476 | loss_total 0.7804\n",
      "step 2/3 | epoch 42/50 | batch 17/60 | global_step 5477 | loss_total 0.7833\n",
      "step 2/3 | epoch 42/50 | batch 18/60 | global_step 5478 | loss_total 0.7980\n",
      "step 2/3 | epoch 42/50 | batch 19/60 | global_step 5479 | loss_total 0.8037\n",
      "step 2/3 | epoch 42/50 | batch 20/60 | global_step 5480 | loss_total 0.7858\n",
      "step 2/3 | epoch 42/50 | batch 21/60 | global_step 5481 | loss_total 0.7786\n",
      "step 2/3 | epoch 42/50 | batch 22/60 | global_step 5482 | loss_total 0.9914\n",
      "step 2/3 | epoch 42/50 | batch 23/60 | global_step 5483 | loss_total 0.7759\n",
      "step 2/3 | epoch 42/50 | batch 24/60 | global_step 5484 | loss_total 0.7753\n",
      "step 2/3 | epoch 42/50 | batch 25/60 | global_step 5485 | loss_total 0.7690\n",
      "step 2/3 | epoch 42/50 | batch 26/60 | global_step 5486 | loss_total 1.6728\n",
      "step 2/3 | epoch 42/50 | batch 27/60 | global_step 5487 | loss_total 1.4202\n",
      "step 2/3 | epoch 42/50 | batch 28/60 | global_step 5488 | loss_total 0.7316\n",
      "step 2/3 | epoch 42/50 | batch 29/60 | global_step 5489 | loss_total 0.8095\n",
      "step 2/3 | epoch 42/50 | batch 30/60 | global_step 5490 | loss_total 0.7610\n",
      "step 2/3 | epoch 42/50 | batch 31/60 | global_step 5491 | loss_total 0.7939\n",
      "step 2/3 | epoch 42/50 | batch 32/60 | global_step 5492 | loss_total 0.7565\n",
      "step 2/3 | epoch 42/50 | batch 33/60 | global_step 5493 | loss_total 1.4138\n",
      "step 2/3 | epoch 42/50 | batch 34/60 | global_step 5494 | loss_total 2.5627\n",
      "step 2/3 | epoch 42/50 | batch 35/60 | global_step 5495 | loss_total 1.7511\n",
      "step 2/3 | epoch 42/50 | batch 36/60 | global_step 5496 | loss_total 2.3209\n",
      "step 2/3 | epoch 42/50 | batch 37/60 | global_step 5497 | loss_total 1.8868\n",
      "step 2/3 | epoch 42/50 | batch 38/60 | global_step 5498 | loss_total 1.8371\n",
      "step 2/3 | epoch 42/50 | batch 39/60 | global_step 5499 | loss_total 2.2863\n",
      "step 2/3 | epoch 42/50 | batch 40/60 | global_step 5500 | loss_total 0.7646\n",
      "step 2/3 | epoch 42/50 | batch 41/60 | global_step 5501 | loss_total 1.6133\n",
      "step 2/3 | epoch 42/50 | batch 42/60 | global_step 5502 | loss_total 0.7711\n",
      "step 2/3 | epoch 42/50 | batch 43/60 | global_step 5503 | loss_total 1.4520\n",
      "step 2/3 | epoch 42/50 | batch 44/60 | global_step 5504 | loss_total 0.7451\n",
      "step 2/3 | epoch 42/50 | batch 45/60 | global_step 5505 | loss_total 0.8190\n",
      "step 2/3 | epoch 42/50 | batch 46/60 | global_step 5506 | loss_total 0.7502\n",
      "step 2/3 | epoch 42/50 | batch 47/60 | global_step 5507 | loss_total 1.2810\n",
      "step 2/3 | epoch 42/50 | batch 48/60 | global_step 5508 | loss_total 0.8389\n",
      "step 2/3 | epoch 42/50 | batch 49/60 | global_step 5509 | loss_total 1.4559\n",
      "step 2/3 | epoch 42/50 | batch 50/60 | global_step 5510 | loss_total 0.8504\n",
      "step 2/3 | epoch 42/50 | batch 51/60 | global_step 5511 | loss_total 1.6704\n",
      "step 2/3 | epoch 42/50 | batch 52/60 | global_step 5512 | loss_total 0.7468\n",
      "step 2/3 | epoch 42/50 | batch 53/60 | global_step 5513 | loss_total 0.7901\n",
      "step 2/3 | epoch 42/50 | batch 54/60 | global_step 5514 | loss_total 0.7471\n",
      "step 2/3 | epoch 42/50 | batch 55/60 | global_step 5515 | loss_total 1.2991\n",
      "step 2/3 | epoch 42/50 | batch 56/60 | global_step 5516 | loss_total 0.7905\n",
      "step 2/3 | epoch 42/50 | batch 57/60 | global_step 5517 | loss_total 0.7935\n",
      "step 2/3 | epoch 42/50 | batch 58/60 | global_step 5518 | loss_total 0.7896\n",
      "step 2/3 | epoch 42/50 | batch 59/60 | global_step 5519 | loss_total 0.8746\n",
      "step 2/3 | epoch 42/50 | batch 60/60 | global_step 5520 | loss_total 0.7483\n",
      "[epoch done] step 2/3 epoch 42/50 | train_total=1.1372 val_total=0.9322\n",
      "step 2/3 | epoch 43/50 | batch 1/60 | global_step 5521 | loss_total 1.3569\n",
      "step 2/3 | epoch 43/50 | batch 2/60 | global_step 5522 | loss_total 1.7203\n",
      "step 2/3 | epoch 43/50 | batch 3/60 | global_step 5523 | loss_total 0.7879\n",
      "step 2/3 | epoch 43/50 | batch 4/60 | global_step 5524 | loss_total 0.8355\n",
      "step 2/3 | epoch 43/50 | batch 5/60 | global_step 5525 | loss_total 1.6281\n",
      "step 2/3 | epoch 43/50 | batch 6/60 | global_step 5526 | loss_total 0.7889\n",
      "step 2/3 | epoch 43/50 | batch 7/60 | global_step 5527 | loss_total 2.2106\n",
      "step 2/3 | epoch 43/50 | batch 8/60 | global_step 5528 | loss_total 1.3683\n",
      "step 2/3 | epoch 43/50 | batch 9/60 | global_step 5529 | loss_total 1.3344\n",
      "step 2/3 | epoch 43/50 | batch 10/60 | global_step 5530 | loss_total 0.8087\n",
      "step 2/3 | epoch 43/50 | batch 11/60 | global_step 5531 | loss_total 0.7874\n",
      "step 2/3 | epoch 43/50 | batch 12/60 | global_step 5532 | loss_total 0.8019\n",
      "step 2/3 | epoch 43/50 | batch 13/60 | global_step 5533 | loss_total 0.7942\n",
      "step 2/3 | epoch 43/50 | batch 14/60 | global_step 5534 | loss_total 1.3378\n",
      "step 2/3 | epoch 43/50 | batch 15/60 | global_step 5535 | loss_total 0.8184\n",
      "step 2/3 | epoch 43/50 | batch 16/60 | global_step 5536 | loss_total 1.3734\n",
      "step 2/3 | epoch 43/50 | batch 17/60 | global_step 5537 | loss_total 0.7875\n",
      "step 2/3 | epoch 43/50 | batch 18/60 | global_step 5538 | loss_total 0.7818\n",
      "step 2/3 | epoch 43/50 | batch 19/60 | global_step 5539 | loss_total 1.6305\n",
      "step 2/3 | epoch 43/50 | batch 20/60 | global_step 5540 | loss_total 0.7854\n",
      "step 2/3 | epoch 43/50 | batch 21/60 | global_step 5541 | loss_total 0.7929\n",
      "step 2/3 | epoch 43/50 | batch 22/60 | global_step 5542 | loss_total 1.8100\n",
      "step 2/3 | epoch 43/50 | batch 23/60 | global_step 5543 | loss_total 1.6000\n",
      "step 2/3 | epoch 43/50 | batch 24/60 | global_step 5544 | loss_total 1.4298\n",
      "step 2/3 | epoch 43/50 | batch 25/60 | global_step 5545 | loss_total 0.7549\n",
      "step 2/3 | epoch 43/50 | batch 26/60 | global_step 5546 | loss_total 1.5219\n",
      "step 2/3 | epoch 43/50 | batch 27/60 | global_step 5547 | loss_total 2.5584\n",
      "step 2/3 | epoch 43/50 | batch 28/60 | global_step 5548 | loss_total 0.7918\n",
      "step 2/3 | epoch 43/50 | batch 29/60 | global_step 5549 | loss_total 0.7536\n",
      "step 2/3 | epoch 43/50 | batch 30/60 | global_step 5550 | loss_total 0.7438\n",
      "step 2/3 | epoch 43/50 | batch 31/60 | global_step 5551 | loss_total 1.9661\n",
      "step 2/3 | epoch 43/50 | batch 32/60 | global_step 5552 | loss_total 0.8006\n",
      "step 2/3 | epoch 43/50 | batch 33/60 | global_step 5553 | loss_total 1.3796\n",
      "step 2/3 | epoch 43/50 | batch 34/60 | global_step 5554 | loss_total 0.9740\n",
      "step 2/3 | epoch 43/50 | batch 35/60 | global_step 5555 | loss_total 1.7559\n",
      "step 2/3 | epoch 43/50 | batch 36/60 | global_step 5556 | loss_total 1.7610\n",
      "step 2/3 | epoch 43/50 | batch 37/60 | global_step 5557 | loss_total 1.0772\n",
      "step 2/3 | epoch 43/50 | batch 38/60 | global_step 5558 | loss_total 0.8187\n",
      "step 2/3 | epoch 43/50 | batch 39/60 | global_step 5559 | loss_total 0.8060\n",
      "step 2/3 | epoch 43/50 | batch 40/60 | global_step 5560 | loss_total 0.7651\n",
      "step 2/3 | epoch 43/50 | batch 41/60 | global_step 5561 | loss_total 0.8060\n",
      "step 2/3 | epoch 43/50 | batch 42/60 | global_step 5562 | loss_total 0.8179\n",
      "step 2/3 | epoch 43/50 | batch 43/60 | global_step 5563 | loss_total 0.8095\n",
      "step 2/3 | epoch 43/50 | batch 44/60 | global_step 5564 | loss_total 0.8841\n",
      "step 2/3 | epoch 43/50 | batch 45/60 | global_step 5565 | loss_total 1.9429\n",
      "step 2/3 | epoch 43/50 | batch 46/60 | global_step 5566 | loss_total 0.7750\n",
      "step 2/3 | epoch 43/50 | batch 47/60 | global_step 5567 | loss_total 0.8178\n",
      "step 2/3 | epoch 43/50 | batch 48/60 | global_step 5568 | loss_total 0.7951\n",
      "step 2/3 | epoch 43/50 | batch 49/60 | global_step 5569 | loss_total 0.7904\n",
      "step 2/3 | epoch 43/50 | batch 50/60 | global_step 5570 | loss_total 0.9018\n",
      "step 2/3 | epoch 43/50 | batch 51/60 | global_step 5571 | loss_total 0.8471\n",
      "step 2/3 | epoch 43/50 | batch 52/60 | global_step 5572 | loss_total 0.7976\n",
      "step 2/3 | epoch 43/50 | batch 53/60 | global_step 5573 | loss_total 0.7815\n",
      "step 2/3 | epoch 43/50 | batch 54/60 | global_step 5574 | loss_total 0.7873\n",
      "step 2/3 | epoch 43/50 | batch 55/60 | global_step 5575 | loss_total 2.3705\n",
      "step 2/3 | epoch 43/50 | batch 56/60 | global_step 5576 | loss_total 0.7825\n",
      "step 2/3 | epoch 43/50 | batch 57/60 | global_step 5577 | loss_total 1.7334\n",
      "step 2/3 | epoch 43/50 | batch 58/60 | global_step 5578 | loss_total 0.7761\n",
      "step 2/3 | epoch 43/50 | batch 59/60 | global_step 5579 | loss_total 0.7837\n",
      "step 2/3 | epoch 43/50 | batch 60/60 | global_step 5580 | loss_total 0.7850\n",
      "[epoch done] step 2/3 epoch 43/50 | train_total=1.1231 val_total=0.8736\n",
      "step 2/3 | epoch 44/50 | batch 1/60 | global_step 5581 | loss_total 1.3948\n",
      "step 2/3 | epoch 44/50 | batch 2/60 | global_step 5582 | loss_total 1.8077\n",
      "step 2/3 | epoch 44/50 | batch 3/60 | global_step 5583 | loss_total 0.8019\n",
      "step 2/3 | epoch 44/50 | batch 4/60 | global_step 5584 | loss_total 1.6006\n",
      "step 2/3 | epoch 44/50 | batch 5/60 | global_step 5585 | loss_total 0.8003\n",
      "step 2/3 | epoch 44/50 | batch 6/60 | global_step 5586 | loss_total 1.3551\n",
      "step 2/3 | epoch 44/50 | batch 7/60 | global_step 5587 | loss_total 0.9292\n",
      "step 2/3 | epoch 44/50 | batch 8/60 | global_step 5588 | loss_total 0.7927\n",
      "step 2/3 | epoch 44/50 | batch 9/60 | global_step 5589 | loss_total 0.7833\n",
      "step 2/3 | epoch 44/50 | batch 10/60 | global_step 5590 | loss_total 1.7781\n",
      "step 2/3 | epoch 44/50 | batch 11/60 | global_step 5591 | loss_total 0.7797\n",
      "step 2/3 | epoch 44/50 | batch 12/60 | global_step 5592 | loss_total 1.3157\n",
      "step 2/3 | epoch 44/50 | batch 13/60 | global_step 5593 | loss_total 1.7826\n",
      "step 2/3 | epoch 44/50 | batch 14/60 | global_step 5594 | loss_total 1.3766\n",
      "step 2/3 | epoch 44/50 | batch 15/60 | global_step 5595 | loss_total 0.7711\n",
      "step 2/3 | epoch 44/50 | batch 16/60 | global_step 5596 | loss_total 0.7406\n",
      "step 2/3 | epoch 44/50 | batch 17/60 | global_step 5597 | loss_total 0.8297\n",
      "step 2/3 | epoch 44/50 | batch 18/60 | global_step 5598 | loss_total 1.6942\n",
      "step 2/3 | epoch 44/50 | batch 19/60 | global_step 5599 | loss_total 0.8509\n",
      "step 2/3 | epoch 44/50 | batch 20/60 | global_step 5600 | loss_total 0.8482\n",
      "step 2/3 | epoch 44/50 | batch 21/60 | global_step 5601 | loss_total 1.6391\n",
      "step 2/3 | epoch 44/50 | batch 22/60 | global_step 5602 | loss_total 0.7990\n",
      "step 2/3 | epoch 44/50 | batch 23/60 | global_step 5603 | loss_total 0.8753\n",
      "step 2/3 | epoch 44/50 | batch 24/60 | global_step 5604 | loss_total 0.8964\n",
      "step 2/3 | epoch 44/50 | batch 25/60 | global_step 5605 | loss_total 0.8116\n",
      "step 2/3 | epoch 44/50 | batch 26/60 | global_step 5606 | loss_total 0.8096\n",
      "step 2/3 | epoch 44/50 | batch 27/60 | global_step 5607 | loss_total 0.9363\n",
      "step 2/3 | epoch 44/50 | batch 28/60 | global_step 5608 | loss_total 0.7960\n",
      "step 2/3 | epoch 44/50 | batch 29/60 | global_step 5609 | loss_total 0.7934\n",
      "step 2/3 | epoch 44/50 | batch 30/60 | global_step 5610 | loss_total 0.7507\n",
      "step 2/3 | epoch 44/50 | batch 31/60 | global_step 5611 | loss_total 0.8031\n",
      "step 2/3 | epoch 44/50 | batch 32/60 | global_step 5612 | loss_total 0.7469\n",
      "step 2/3 | epoch 44/50 | batch 33/60 | global_step 5613 | loss_total 0.8349\n",
      "step 2/3 | epoch 44/50 | batch 34/60 | global_step 5614 | loss_total 0.8003\n",
      "step 2/3 | epoch 44/50 | batch 35/60 | global_step 5615 | loss_total 0.6972\n",
      "step 2/3 | epoch 44/50 | batch 36/60 | global_step 5616 | loss_total 0.8200\n",
      "step 2/3 | epoch 44/50 | batch 37/60 | global_step 5617 | loss_total 1.3582\n",
      "step 2/3 | epoch 44/50 | batch 38/60 | global_step 5618 | loss_total 3.2062\n",
      "step 2/3 | epoch 44/50 | batch 39/60 | global_step 5619 | loss_total 1.6023\n",
      "step 2/3 | epoch 44/50 | batch 40/60 | global_step 5620 | loss_total 0.7648\n",
      "step 2/3 | epoch 44/50 | batch 41/60 | global_step 5621 | loss_total 1.6495\n",
      "step 2/3 | epoch 44/50 | batch 42/60 | global_step 5622 | loss_total 0.7637\n",
      "step 2/3 | epoch 44/50 | batch 43/60 | global_step 5623 | loss_total 0.7948\n",
      "step 2/3 | epoch 44/50 | batch 44/60 | global_step 5624 | loss_total 0.7235\n",
      "step 2/3 | epoch 44/50 | batch 45/60 | global_step 5625 | loss_total 1.0649\n",
      "step 2/3 | epoch 44/50 | batch 46/60 | global_step 5626 | loss_total 0.7408\n",
      "step 2/3 | epoch 44/50 | batch 47/60 | global_step 5627 | loss_total 1.7968\n",
      "step 2/3 | epoch 44/50 | batch 48/60 | global_step 5628 | loss_total 0.7460\n",
      "step 2/3 | epoch 44/50 | batch 49/60 | global_step 5629 | loss_total 2.1286\n",
      "step 2/3 | epoch 44/50 | batch 50/60 | global_step 5630 | loss_total 1.7436\n",
      "step 2/3 | epoch 44/50 | batch 51/60 | global_step 5631 | loss_total 0.7453\n",
      "step 2/3 | epoch 44/50 | batch 52/60 | global_step 5632 | loss_total 2.2259\n",
      "step 2/3 | epoch 44/50 | batch 53/60 | global_step 5633 | loss_total 0.7625\n",
      "step 2/3 | epoch 44/50 | batch 54/60 | global_step 5634 | loss_total 0.7650\n",
      "step 2/3 | epoch 44/50 | batch 55/60 | global_step 5635 | loss_total 0.9274\n",
      "step 2/3 | epoch 44/50 | batch 56/60 | global_step 5636 | loss_total 0.8038\n",
      "step 2/3 | epoch 44/50 | batch 57/60 | global_step 5637 | loss_total 0.7421\n",
      "step 2/3 | epoch 44/50 | batch 58/60 | global_step 5638 | loss_total 0.7774\n",
      "step 2/3 | epoch 44/50 | batch 59/60 | global_step 5639 | loss_total 1.6947\n",
      "step 2/3 | epoch 44/50 | batch 60/60 | global_step 5640 | loss_total 0.7274\n",
      "[epoch done] step 2/3 epoch 44/50 | train_total=1.1016 val_total=0.9052\n",
      "step 2/3 | epoch 45/50 | batch 1/60 | global_step 5641 | loss_total 0.7299\n",
      "step 2/3 | epoch 45/50 | batch 2/60 | global_step 5642 | loss_total 0.7129\n",
      "step 2/3 | epoch 45/50 | batch 3/60 | global_step 5643 | loss_total 0.7091\n",
      "step 2/3 | epoch 45/50 | batch 4/60 | global_step 5644 | loss_total 0.7016\n",
      "step 2/3 | epoch 45/50 | batch 5/60 | global_step 5645 | loss_total 0.8636\n",
      "step 2/3 | epoch 45/50 | batch 6/60 | global_step 5646 | loss_total 0.6855\n",
      "step 2/3 | epoch 45/50 | batch 7/60 | global_step 5647 | loss_total 0.6759\n",
      "step 2/3 | epoch 45/50 | batch 8/60 | global_step 5648 | loss_total 0.6671\n",
      "step 2/3 | epoch 45/50 | batch 9/60 | global_step 5649 | loss_total 1.0826\n",
      "step 2/3 | epoch 45/50 | batch 10/60 | global_step 5650 | loss_total 1.8015\n",
      "step 2/3 | epoch 45/50 | batch 11/60 | global_step 5651 | loss_total 2.3764\n",
      "step 2/3 | epoch 45/50 | batch 12/60 | global_step 5652 | loss_total 1.5291\n",
      "step 2/3 | epoch 45/50 | batch 13/60 | global_step 5653 | loss_total 2.2526\n",
      "step 2/3 | epoch 45/50 | batch 14/60 | global_step 5654 | loss_total 0.8018\n",
      "step 2/3 | epoch 45/50 | batch 15/60 | global_step 5655 | loss_total 0.7950\n",
      "step 2/3 | epoch 45/50 | batch 16/60 | global_step 5656 | loss_total 0.6947\n",
      "step 2/3 | epoch 45/50 | batch 17/60 | global_step 5657 | loss_total 0.6684\n",
      "step 2/3 | epoch 45/50 | batch 18/60 | global_step 5658 | loss_total 0.6700\n",
      "step 2/3 | epoch 45/50 | batch 19/60 | global_step 5659 | loss_total 1.8943\n",
      "step 2/3 | epoch 45/50 | batch 20/60 | global_step 5660 | loss_total 1.4298\n",
      "step 2/3 | epoch 45/50 | batch 21/60 | global_step 5661 | loss_total 2.2805\n",
      "step 2/3 | epoch 45/50 | batch 22/60 | global_step 5662 | loss_total 0.8005\n",
      "step 2/3 | epoch 45/50 | batch 23/60 | global_step 5663 | loss_total 0.7881\n",
      "step 2/3 | epoch 45/50 | batch 24/60 | global_step 5664 | loss_total 1.7649\n",
      "step 2/3 | epoch 45/50 | batch 25/60 | global_step 5665 | loss_total 2.2280\n",
      "step 2/3 | epoch 45/50 | batch 26/60 | global_step 5666 | loss_total 0.6692\n",
      "step 2/3 | epoch 45/50 | batch 27/60 | global_step 5667 | loss_total 1.4215\n",
      "step 2/3 | epoch 45/50 | batch 28/60 | global_step 5668 | loss_total 2.1263\n",
      "step 2/3 | epoch 45/50 | batch 29/60 | global_step 5669 | loss_total 0.9146\n",
      "step 2/3 | epoch 45/50 | batch 30/60 | global_step 5670 | loss_total 0.7811\n",
      "step 2/3 | epoch 45/50 | batch 31/60 | global_step 5671 | loss_total 0.7097\n",
      "step 2/3 | epoch 45/50 | batch 32/60 | global_step 5672 | loss_total 0.7077\n",
      "step 2/3 | epoch 45/50 | batch 33/60 | global_step 5673 | loss_total 0.8898\n",
      "step 2/3 | epoch 45/50 | batch 34/60 | global_step 5674 | loss_total 1.3567\n",
      "step 2/3 | epoch 45/50 | batch 35/60 | global_step 5675 | loss_total 1.3897\n",
      "step 2/3 | epoch 45/50 | batch 36/60 | global_step 5676 | loss_total 0.8720\n",
      "step 2/3 | epoch 45/50 | batch 37/60 | global_step 5677 | loss_total 0.8885\n",
      "step 2/3 | epoch 45/50 | batch 38/60 | global_step 5678 | loss_total 0.8509\n",
      "step 2/3 | epoch 45/50 | batch 39/60 | global_step 5679 | loss_total 1.4535\n",
      "step 2/3 | epoch 45/50 | batch 40/60 | global_step 5680 | loss_total 1.6349\n",
      "step 2/3 | epoch 45/50 | batch 41/60 | global_step 5681 | loss_total 1.2742\n",
      "step 2/3 | epoch 45/50 | batch 42/60 | global_step 5682 | loss_total 0.8186\n",
      "step 2/3 | epoch 45/50 | batch 43/60 | global_step 5683 | loss_total 0.7996\n",
      "step 2/3 | epoch 45/50 | batch 44/60 | global_step 5684 | loss_total 0.7944\n",
      "step 2/3 | epoch 45/50 | batch 45/60 | global_step 5685 | loss_total 1.2713\n",
      "step 2/3 | epoch 45/50 | batch 46/60 | global_step 5686 | loss_total 1.7106\n",
      "step 2/3 | epoch 45/50 | batch 47/60 | global_step 5687 | loss_total 0.7963\n",
      "step 2/3 | epoch 45/50 | batch 48/60 | global_step 5688 | loss_total 1.7100\n",
      "step 2/3 | epoch 45/50 | batch 49/60 | global_step 5689 | loss_total 0.7864\n",
      "step 2/3 | epoch 45/50 | batch 50/60 | global_step 5690 | loss_total 0.8013\n",
      "step 2/3 | epoch 45/50 | batch 51/60 | global_step 5691 | loss_total 0.8037\n",
      "step 2/3 | epoch 45/50 | batch 52/60 | global_step 5692 | loss_total 0.8371\n",
      "step 2/3 | epoch 45/50 | batch 53/60 | global_step 5693 | loss_total 0.8046\n",
      "step 2/3 | epoch 45/50 | batch 54/60 | global_step 5694 | loss_total 0.7743\n",
      "step 2/3 | epoch 45/50 | batch 55/60 | global_step 5695 | loss_total 2.0920\n",
      "step 2/3 | epoch 45/50 | batch 56/60 | global_step 5696 | loss_total 1.6438\n",
      "step 2/3 | epoch 45/50 | batch 57/60 | global_step 5697 | loss_total 0.8461\n",
      "step 2/3 | epoch 45/50 | batch 58/60 | global_step 5698 | loss_total 1.6379\n",
      "step 2/3 | epoch 45/50 | batch 59/60 | global_step 5699 | loss_total 1.6392\n",
      "step 2/3 | epoch 45/50 | batch 60/60 | global_step 5700 | loss_total 0.8365\n",
      "[epoch done] step 2/3 epoch 45/50 | train_total=1.1491 val_total=0.7755\n",
      "step 2/3 | epoch 46/50 | batch 1/60 | global_step 5701 | loss_total 0.8322\n",
      "step 2/3 | epoch 46/50 | batch 2/60 | global_step 5702 | loss_total 0.8518\n",
      "step 2/3 | epoch 46/50 | batch 3/60 | global_step 5703 | loss_total 1.5152\n",
      "step 2/3 | epoch 46/50 | batch 4/60 | global_step 5704 | loss_total 0.8125\n",
      "step 2/3 | epoch 46/50 | batch 5/60 | global_step 5705 | loss_total 0.8178\n",
      "step 2/3 | epoch 46/50 | batch 6/60 | global_step 5706 | loss_total 0.8199\n",
      "step 2/3 | epoch 46/50 | batch 7/60 | global_step 5707 | loss_total 0.8112\n",
      "step 2/3 | epoch 46/50 | batch 8/60 | global_step 5708 | loss_total 0.8209\n",
      "step 2/3 | epoch 46/50 | batch 9/60 | global_step 5709 | loss_total 0.7722\n",
      "step 2/3 | epoch 46/50 | batch 10/60 | global_step 5710 | loss_total 1.2953\n",
      "step 2/3 | epoch 46/50 | batch 11/60 | global_step 5711 | loss_total 1.2364\n",
      "step 2/3 | epoch 46/50 | batch 12/60 | global_step 5712 | loss_total 0.8201\n",
      "step 2/3 | epoch 46/50 | batch 13/60 | global_step 5713 | loss_total 0.8074\n",
      "step 2/3 | epoch 46/50 | batch 14/60 | global_step 5714 | loss_total 0.9526\n",
      "step 2/3 | epoch 46/50 | batch 15/60 | global_step 5715 | loss_total 0.8580\n",
      "step 2/3 | epoch 46/50 | batch 16/60 | global_step 5716 | loss_total 0.7971\n",
      "step 2/3 | epoch 46/50 | batch 17/60 | global_step 5717 | loss_total 0.7968\n",
      "step 2/3 | epoch 46/50 | batch 18/60 | global_step 5718 | loss_total 1.4343\n",
      "step 2/3 | epoch 46/50 | batch 19/60 | global_step 5719 | loss_total 0.8305\n",
      "step 2/3 | epoch 46/50 | batch 20/60 | global_step 5720 | loss_total 1.6088\n",
      "step 2/3 | epoch 46/50 | batch 21/60 | global_step 5721 | loss_total 1.4594\n",
      "step 2/3 | epoch 46/50 | batch 22/60 | global_step 5722 | loss_total 0.7607\n",
      "step 2/3 | epoch 46/50 | batch 23/60 | global_step 5723 | loss_total 0.8056\n",
      "step 2/3 | epoch 46/50 | batch 24/60 | global_step 5724 | loss_total 0.7812\n",
      "step 2/3 | epoch 46/50 | batch 25/60 | global_step 5725 | loss_total 1.3237\n",
      "step 2/3 | epoch 46/50 | batch 26/60 | global_step 5726 | loss_total 1.2899\n",
      "step 2/3 | epoch 46/50 | batch 27/60 | global_step 5727 | loss_total 1.7163\n",
      "step 2/3 | epoch 46/50 | batch 28/60 | global_step 5728 | loss_total 1.6767\n",
      "step 2/3 | epoch 46/50 | batch 29/60 | global_step 5729 | loss_total 0.7767\n",
      "step 2/3 | epoch 46/50 | batch 30/60 | global_step 5730 | loss_total 0.7911\n",
      "step 2/3 | epoch 46/50 | batch 31/60 | global_step 5731 | loss_total 0.8103\n",
      "step 2/3 | epoch 46/50 | batch 32/60 | global_step 5732 | loss_total 0.7768\n",
      "step 2/3 | epoch 46/50 | batch 33/60 | global_step 5733 | loss_total 1.3646\n",
      "step 2/3 | epoch 46/50 | batch 34/60 | global_step 5734 | loss_total 0.8245\n",
      "step 2/3 | epoch 46/50 | batch 35/60 | global_step 5735 | loss_total 0.7956\n",
      "step 2/3 | epoch 46/50 | batch 36/60 | global_step 5736 | loss_total 0.8371\n",
      "step 2/3 | epoch 46/50 | batch 37/60 | global_step 5737 | loss_total 1.4877\n",
      "step 2/3 | epoch 46/50 | batch 38/60 | global_step 5738 | loss_total 1.4796\n",
      "step 2/3 | epoch 46/50 | batch 39/60 | global_step 5739 | loss_total 1.3465\n",
      "step 2/3 | epoch 46/50 | batch 40/60 | global_step 5740 | loss_total 0.8073\n",
      "step 2/3 | epoch 46/50 | batch 41/60 | global_step 5741 | loss_total 0.8283\n",
      "step 2/3 | epoch 46/50 | batch 42/60 | global_step 5742 | loss_total 0.7935\n",
      "step 2/3 | epoch 46/50 | batch 43/60 | global_step 5743 | loss_total 1.7282\n",
      "step 2/3 | epoch 46/50 | batch 44/60 | global_step 5744 | loss_total 1.2700\n",
      "step 2/3 | epoch 46/50 | batch 45/60 | global_step 5745 | loss_total 1.6446\n",
      "step 2/3 | epoch 46/50 | batch 46/60 | global_step 5746 | loss_total 2.1518\n",
      "step 2/3 | epoch 46/50 | batch 47/60 | global_step 5747 | loss_total 0.8195\n",
      "step 2/3 | epoch 46/50 | batch 48/60 | global_step 5748 | loss_total 1.7761\n",
      "step 2/3 | epoch 46/50 | batch 49/60 | global_step 5749 | loss_total 0.7891\n",
      "step 2/3 | epoch 46/50 | batch 50/60 | global_step 5750 | loss_total 0.8098\n",
      "step 2/3 | epoch 46/50 | batch 51/60 | global_step 5751 | loss_total 0.8255\n",
      "step 2/3 | epoch 46/50 | batch 52/60 | global_step 5752 | loss_total 0.8039\n",
      "step 2/3 | epoch 46/50 | batch 53/60 | global_step 5753 | loss_total 0.9156\n",
      "step 2/3 | epoch 46/50 | batch 54/60 | global_step 5754 | loss_total 1.2392\n",
      "step 2/3 | epoch 46/50 | batch 55/60 | global_step 5755 | loss_total 1.0635\n",
      "step 2/3 | epoch 46/50 | batch 56/60 | global_step 5756 | loss_total 1.4845\n",
      "step 2/3 | epoch 46/50 | batch 57/60 | global_step 5757 | loss_total 1.3901\n",
      "step 2/3 | epoch 46/50 | batch 58/60 | global_step 5758 | loss_total 0.7399\n",
      "step 2/3 | epoch 46/50 | batch 59/60 | global_step 5759 | loss_total 0.9306\n",
      "step 2/3 | epoch 46/50 | batch 60/60 | global_step 5760 | loss_total 0.7952\n",
      "[epoch done] step 2/3 epoch 46/50 | train_total=1.0700 val_total=0.9209\n",
      "step 2/3 | epoch 47/50 | batch 1/60 | global_step 5761 | loss_total 2.3305\n",
      "step 2/3 | epoch 47/50 | batch 2/60 | global_step 5762 | loss_total 1.3998\n",
      "step 2/3 | epoch 47/50 | batch 3/60 | global_step 5763 | loss_total 0.7665\n",
      "step 2/3 | epoch 47/50 | batch 4/60 | global_step 5764 | loss_total 0.7946\n",
      "step 2/3 | epoch 47/50 | batch 5/60 | global_step 5765 | loss_total 0.8468\n",
      "step 2/3 | epoch 47/50 | batch 6/60 | global_step 5766 | loss_total 0.7512\n",
      "step 2/3 | epoch 47/50 | batch 7/60 | global_step 5767 | loss_total 0.8466\n",
      "step 2/3 | epoch 47/50 | batch 8/60 | global_step 5768 | loss_total 0.7550\n",
      "step 2/3 | epoch 47/50 | batch 9/60 | global_step 5769 | loss_total 0.7926\n",
      "step 2/3 | epoch 47/50 | batch 10/60 | global_step 5770 | loss_total 0.8322\n",
      "step 2/3 | epoch 47/50 | batch 11/60 | global_step 5771 | loss_total 1.7269\n",
      "step 2/3 | epoch 47/50 | batch 12/60 | global_step 5772 | loss_total 0.8417\n",
      "step 2/3 | epoch 47/50 | batch 13/60 | global_step 5773 | loss_total 1.7549\n",
      "step 2/3 | epoch 47/50 | batch 14/60 | global_step 5774 | loss_total 0.7550\n",
      "step 2/3 | epoch 47/50 | batch 15/60 | global_step 5775 | loss_total 0.7699\n",
      "step 2/3 | epoch 47/50 | batch 16/60 | global_step 5776 | loss_total 0.7962\n",
      "step 2/3 | epoch 47/50 | batch 17/60 | global_step 5777 | loss_total 0.8255\n",
      "step 2/3 | epoch 47/50 | batch 18/60 | global_step 5778 | loss_total 0.7832\n",
      "step 2/3 | epoch 47/50 | batch 19/60 | global_step 5779 | loss_total 2.2843\n",
      "step 2/3 | epoch 47/50 | batch 20/60 | global_step 5780 | loss_total 0.7858\n",
      "step 2/3 | epoch 47/50 | batch 21/60 | global_step 5781 | loss_total 0.8249\n",
      "step 2/3 | epoch 47/50 | batch 22/60 | global_step 5782 | loss_total 0.7791\n",
      "step 2/3 | epoch 47/50 | batch 23/60 | global_step 5783 | loss_total 0.7949\n",
      "step 2/3 | epoch 47/50 | batch 24/60 | global_step 5784 | loss_total 0.7765\n",
      "step 2/3 | epoch 47/50 | batch 25/60 | global_step 5785 | loss_total 0.8022\n",
      "step 2/3 | epoch 47/50 | batch 26/60 | global_step 5786 | loss_total 0.7573\n",
      "step 2/3 | epoch 47/50 | batch 27/60 | global_step 5787 | loss_total 0.7632\n",
      "step 2/3 | epoch 47/50 | batch 28/60 | global_step 5788 | loss_total 0.7464\n",
      "step 2/3 | epoch 47/50 | batch 29/60 | global_step 5789 | loss_total 1.5128\n",
      "step 2/3 | epoch 47/50 | batch 30/60 | global_step 5790 | loss_total 0.8067\n",
      "step 2/3 | epoch 47/50 | batch 31/60 | global_step 5791 | loss_total 2.4005\n",
      "step 2/3 | epoch 47/50 | batch 32/60 | global_step 5792 | loss_total 0.8058\n",
      "step 2/3 | epoch 47/50 | batch 33/60 | global_step 5793 | loss_total 0.8590\n",
      "step 2/3 | epoch 47/50 | batch 34/60 | global_step 5794 | loss_total 0.8049\n",
      "step 2/3 | epoch 47/50 | batch 35/60 | global_step 5795 | loss_total 0.7206\n",
      "step 2/3 | epoch 47/50 | batch 36/60 | global_step 5796 | loss_total 0.7288\n",
      "step 2/3 | epoch 47/50 | batch 37/60 | global_step 5797 | loss_total 0.7825\n",
      "step 2/3 | epoch 47/50 | batch 38/60 | global_step 5798 | loss_total 0.8667\n",
      "step 2/3 | epoch 47/50 | batch 39/60 | global_step 5799 | loss_total 0.6077\n",
      "step 2/3 | epoch 47/50 | batch 40/60 | global_step 5800 | loss_total 0.7890\n",
      "step 2/3 | epoch 47/50 | batch 41/60 | global_step 5801 | loss_total 1.7486\n",
      "step 2/3 | epoch 47/50 | batch 42/60 | global_step 5802 | loss_total 0.8716\n",
      "step 2/3 | epoch 47/50 | batch 43/60 | global_step 5803 | loss_total 0.7039\n",
      "step 2/3 | epoch 47/50 | batch 44/60 | global_step 5804 | loss_total 1.3138\n",
      "step 2/3 | epoch 47/50 | batch 45/60 | global_step 5805 | loss_total 0.5853\n",
      "step 2/3 | epoch 47/50 | batch 46/60 | global_step 5806 | loss_total 0.7869\n",
      "step 2/3 | epoch 47/50 | batch 47/60 | global_step 5807 | loss_total 1.6624\n",
      "step 2/3 | epoch 47/50 | batch 48/60 | global_step 5808 | loss_total 1.4177\n",
      "step 2/3 | epoch 47/50 | batch 49/60 | global_step 5809 | loss_total 0.7824\n",
      "step 2/3 | epoch 47/50 | batch 50/60 | global_step 5810 | loss_total 0.6751\n",
      "step 2/3 | epoch 47/50 | batch 51/60 | global_step 5811 | loss_total 1.0511\n",
      "step 2/3 | epoch 47/50 | batch 52/60 | global_step 5812 | loss_total 0.6762\n",
      "step 2/3 | epoch 47/50 | batch 53/60 | global_step 5813 | loss_total 2.2054\n",
      "step 2/3 | epoch 47/50 | batch 54/60 | global_step 5814 | loss_total 2.4090\n",
      "step 2/3 | epoch 47/50 | batch 55/60 | global_step 5815 | loss_total 0.7812\n",
      "step 2/3 | epoch 47/50 | batch 56/60 | global_step 5816 | loss_total 0.6675\n",
      "step 2/3 | epoch 47/50 | batch 57/60 | global_step 5817 | loss_total 0.5603\n",
      "step 2/3 | epoch 47/50 | batch 58/60 | global_step 5818 | loss_total 0.7965\n",
      "step 2/3 | epoch 47/50 | batch 59/60 | global_step 5819 | loss_total 0.6650\n",
      "step 2/3 | epoch 47/50 | batch 60/60 | global_step 5820 | loss_total 1.4305\n",
      "[epoch done] step 2/3 epoch 47/50 | train_total=1.0193 val_total=1.0451\n",
      "step 2/3 | epoch 48/50 | batch 1/60 | global_step 5821 | loss_total 1.3481\n",
      "step 2/3 | epoch 48/50 | batch 2/60 | global_step 5822 | loss_total 0.9555\n",
      "step 2/3 | epoch 48/50 | batch 3/60 | global_step 5823 | loss_total 0.7862\n",
      "step 2/3 | epoch 48/50 | batch 4/60 | global_step 5824 | loss_total 0.6578\n",
      "step 2/3 | epoch 48/50 | batch 5/60 | global_step 5825 | loss_total 0.8548\n",
      "step 2/3 | epoch 48/50 | batch 6/60 | global_step 5826 | loss_total 0.7768\n",
      "step 2/3 | epoch 48/50 | batch 7/60 | global_step 5827 | loss_total 0.9431\n",
      "step 2/3 | epoch 48/50 | batch 8/60 | global_step 5828 | loss_total 1.7156\n",
      "step 2/3 | epoch 48/50 | batch 9/60 | global_step 5829 | loss_total 1.3960\n",
      "step 2/3 | epoch 48/50 | batch 10/60 | global_step 5830 | loss_total 1.5309\n",
      "step 2/3 | epoch 48/50 | batch 11/60 | global_step 5831 | loss_total 0.8964\n",
      "step 2/3 | epoch 48/50 | batch 12/60 | global_step 5832 | loss_total 0.7971\n",
      "step 2/3 | epoch 48/50 | batch 13/60 | global_step 5833 | loss_total 1.7331\n",
      "step 2/3 | epoch 48/50 | batch 14/60 | global_step 5834 | loss_total 0.8313\n",
      "step 2/3 | epoch 48/50 | batch 15/60 | global_step 5835 | loss_total 2.1722\n",
      "step 2/3 | epoch 48/50 | batch 16/60 | global_step 5836 | loss_total 1.5964\n",
      "step 2/3 | epoch 48/50 | batch 17/60 | global_step 5837 | loss_total 0.6943\n",
      "step 2/3 | epoch 48/50 | batch 18/60 | global_step 5838 | loss_total 0.8294\n",
      "step 2/3 | epoch 48/50 | batch 19/60 | global_step 5839 | loss_total 0.9615\n",
      "step 2/3 | epoch 48/50 | batch 20/60 | global_step 5840 | loss_total 0.7575\n",
      "step 2/3 | epoch 48/50 | batch 21/60 | global_step 5841 | loss_total 1.8343\n",
      "step 2/3 | epoch 48/50 | batch 22/60 | global_step 5842 | loss_total 0.7564\n",
      "step 2/3 | epoch 48/50 | batch 23/60 | global_step 5843 | loss_total 2.2330\n",
      "step 2/3 | epoch 48/50 | batch 24/60 | global_step 5844 | loss_total 0.8356\n",
      "step 2/3 | epoch 48/50 | batch 25/60 | global_step 5845 | loss_total 0.7780\n",
      "step 2/3 | epoch 48/50 | batch 26/60 | global_step 5846 | loss_total 0.7691\n",
      "step 2/3 | epoch 48/50 | batch 27/60 | global_step 5847 | loss_total 0.8163\n",
      "step 2/3 | epoch 48/50 | batch 28/60 | global_step 5848 | loss_total 0.7682\n",
      "step 2/3 | epoch 48/50 | batch 29/60 | global_step 5849 | loss_total 0.8408\n",
      "step 2/3 | epoch 48/50 | batch 30/60 | global_step 5850 | loss_total 0.7657\n",
      "step 2/3 | epoch 48/50 | batch 31/60 | global_step 5851 | loss_total 0.7789\n",
      "step 2/3 | epoch 48/50 | batch 32/60 | global_step 5852 | loss_total 0.7606\n",
      "step 2/3 | epoch 48/50 | batch 33/60 | global_step 5853 | loss_total 1.5135\n",
      "step 2/3 | epoch 48/50 | batch 34/60 | global_step 5854 | loss_total 2.9127\n",
      "step 2/3 | epoch 48/50 | batch 35/60 | global_step 5855 | loss_total 0.9656\n",
      "step 2/3 | epoch 48/50 | batch 36/60 | global_step 5856 | loss_total 1.8173\n",
      "step 2/3 | epoch 48/50 | batch 37/60 | global_step 5857 | loss_total 0.7669\n",
      "step 2/3 | epoch 48/50 | batch 38/60 | global_step 5858 | loss_total 0.7598\n",
      "step 2/3 | epoch 48/50 | batch 39/60 | global_step 5859 | loss_total 0.7607\n",
      "step 2/3 | epoch 48/50 | batch 40/60 | global_step 5860 | loss_total 0.7718\n",
      "step 2/3 | epoch 48/50 | batch 41/60 | global_step 5861 | loss_total 1.8404\n",
      "step 2/3 | epoch 48/50 | batch 42/60 | global_step 5862 | loss_total 2.3574\n",
      "step 2/3 | epoch 48/50 | batch 43/60 | global_step 5863 | loss_total 1.0895\n",
      "step 2/3 | epoch 48/50 | batch 44/60 | global_step 5864 | loss_total 0.7616\n",
      "step 2/3 | epoch 48/50 | batch 45/60 | global_step 5865 | loss_total 2.4738\n",
      "step 2/3 | epoch 48/50 | batch 46/60 | global_step 5866 | loss_total 0.7379\n",
      "step 2/3 | epoch 48/50 | batch 47/60 | global_step 5867 | loss_total 0.7644\n",
      "step 2/3 | epoch 48/50 | batch 48/60 | global_step 5868 | loss_total 0.8801\n",
      "step 2/3 | epoch 48/50 | batch 49/60 | global_step 5869 | loss_total 0.8394\n",
      "step 2/3 | epoch 48/50 | batch 50/60 | global_step 5870 | loss_total 1.0565\n",
      "step 2/3 | epoch 48/50 | batch 51/60 | global_step 5871 | loss_total 1.5953\n",
      "step 2/3 | epoch 48/50 | batch 52/60 | global_step 5872 | loss_total 1.2889\n",
      "step 2/3 | epoch 48/50 | batch 53/60 | global_step 5873 | loss_total 1.7596\n",
      "step 2/3 | epoch 48/50 | batch 54/60 | global_step 5874 | loss_total 1.5914\n",
      "step 2/3 | epoch 48/50 | batch 55/60 | global_step 5875 | loss_total 0.7574\n",
      "step 2/3 | epoch 48/50 | batch 56/60 | global_step 5876 | loss_total 0.8488\n",
      "step 2/3 | epoch 48/50 | batch 57/60 | global_step 5877 | loss_total 0.8278\n",
      "step 2/3 | epoch 48/50 | batch 58/60 | global_step 5878 | loss_total 0.7878\n",
      "step 2/3 | epoch 48/50 | batch 59/60 | global_step 5879 | loss_total 0.7801\n",
      "step 2/3 | epoch 48/50 | batch 60/60 | global_step 5880 | loss_total 0.9593\n",
      "[epoch done] step 2/3 epoch 48/50 | train_total=1.1406 val_total=0.9079\n",
      "step 2/3 | epoch 49/50 | batch 1/60 | global_step 5881 | loss_total 0.7750\n",
      "step 2/3 | epoch 49/50 | batch 2/60 | global_step 5882 | loss_total 0.7674\n",
      "step 2/3 | epoch 49/50 | batch 3/60 | global_step 5883 | loss_total 0.7685\n",
      "step 2/3 | epoch 49/50 | batch 4/60 | global_step 5884 | loss_total 0.7913\n",
      "step 2/3 | epoch 49/50 | batch 5/60 | global_step 5885 | loss_total 0.7843\n",
      "step 2/3 | epoch 49/50 | batch 6/60 | global_step 5886 | loss_total 2.3669\n",
      "step 2/3 | epoch 49/50 | batch 7/60 | global_step 5887 | loss_total 0.7633\n",
      "step 2/3 | epoch 49/50 | batch 8/60 | global_step 5888 | loss_total 1.6612\n",
      "step 2/3 | epoch 49/50 | batch 9/60 | global_step 5889 | loss_total 2.0051\n",
      "step 2/3 | epoch 49/50 | batch 10/60 | global_step 5890 | loss_total 1.2722\n",
      "step 2/3 | epoch 49/50 | batch 11/60 | global_step 5891 | loss_total 0.8007\n",
      "step 2/3 | epoch 49/50 | batch 12/60 | global_step 5892 | loss_total 0.7445\n",
      "step 2/3 | epoch 49/50 | batch 13/60 | global_step 5893 | loss_total 1.6124\n",
      "step 2/3 | epoch 49/50 | batch 14/60 | global_step 5894 | loss_total 1.5330\n",
      "step 2/3 | epoch 49/50 | batch 15/60 | global_step 5895 | loss_total 2.1047\n",
      "step 2/3 | epoch 49/50 | batch 16/60 | global_step 5896 | loss_total 0.9639\n",
      "step 2/3 | epoch 49/50 | batch 17/60 | global_step 5897 | loss_total 0.8255\n",
      "step 2/3 | epoch 49/50 | batch 18/60 | global_step 5898 | loss_total 0.7981\n",
      "step 2/3 | epoch 49/50 | batch 19/60 | global_step 5899 | loss_total 0.8094\n",
      "step 2/3 | epoch 49/50 | batch 20/60 | global_step 5900 | loss_total 0.8070\n",
      "step 2/3 | epoch 49/50 | batch 21/60 | global_step 5901 | loss_total 0.7991\n",
      "step 2/3 | epoch 49/50 | batch 22/60 | global_step 5902 | loss_total 0.8668\n",
      "step 2/3 | epoch 49/50 | batch 23/60 | global_step 5903 | loss_total 1.3064\n",
      "step 2/3 | epoch 49/50 | batch 24/60 | global_step 5904 | loss_total 0.8196\n",
      "step 2/3 | epoch 49/50 | batch 25/60 | global_step 5905 | loss_total 2.0534\n",
      "step 2/3 | epoch 49/50 | batch 26/60 | global_step 5906 | loss_total 2.2637\n",
      "step 2/3 | epoch 49/50 | batch 27/60 | global_step 5907 | loss_total 0.7767\n",
      "step 2/3 | epoch 49/50 | batch 28/60 | global_step 5908 | loss_total 0.7794\n",
      "step 2/3 | epoch 49/50 | batch 29/60 | global_step 5909 | loss_total 0.8504\n",
      "step 2/3 | epoch 49/50 | batch 30/60 | global_step 5910 | loss_total 0.7765\n",
      "step 2/3 | epoch 49/50 | batch 31/60 | global_step 5911 | loss_total 0.8067\n",
      "step 2/3 | epoch 49/50 | batch 32/60 | global_step 5912 | loss_total 1.3752\n",
      "step 2/3 | epoch 49/50 | batch 33/60 | global_step 5913 | loss_total 0.7826\n",
      "step 2/3 | epoch 49/50 | batch 34/60 | global_step 5914 | loss_total 1.7593\n",
      "step 2/3 | epoch 49/50 | batch 35/60 | global_step 5915 | loss_total 1.4461\n",
      "step 2/3 | epoch 49/50 | batch 36/60 | global_step 5916 | loss_total 1.7185\n",
      "step 2/3 | epoch 49/50 | batch 37/60 | global_step 5917 | loss_total 1.3811\n",
      "step 2/3 | epoch 49/50 | batch 38/60 | global_step 5918 | loss_total 1.6102\n",
      "step 2/3 | epoch 49/50 | batch 39/60 | global_step 5919 | loss_total 1.0287\n",
      "step 2/3 | epoch 49/50 | batch 40/60 | global_step 5920 | loss_total 0.7858\n",
      "step 2/3 | epoch 49/50 | batch 41/60 | global_step 5921 | loss_total 0.7804\n",
      "step 2/3 | epoch 49/50 | batch 42/60 | global_step 5922 | loss_total 0.7952\n",
      "step 2/3 | epoch 49/50 | batch 43/60 | global_step 5923 | loss_total 0.7878\n",
      "step 2/3 | epoch 49/50 | batch 44/60 | global_step 5924 | loss_total 1.2442\n",
      "step 2/3 | epoch 49/50 | batch 45/60 | global_step 5925 | loss_total 0.7930\n",
      "step 2/3 | epoch 49/50 | batch 46/60 | global_step 5926 | loss_total 1.2446\n",
      "step 2/3 | epoch 49/50 | batch 47/60 | global_step 5927 | loss_total 0.7809\n",
      "step 2/3 | epoch 49/50 | batch 48/60 | global_step 5928 | loss_total 1.6917\n",
      "step 2/3 | epoch 49/50 | batch 49/60 | global_step 5929 | loss_total 0.7795\n",
      "step 2/3 | epoch 49/50 | batch 50/60 | global_step 5930 | loss_total 0.7884\n",
      "step 2/3 | epoch 49/50 | batch 51/60 | global_step 5931 | loss_total 0.7836\n",
      "step 2/3 | epoch 49/50 | batch 52/60 | global_step 5932 | loss_total 0.7912\n",
      "step 2/3 | epoch 49/50 | batch 53/60 | global_step 5933 | loss_total 2.7017\n",
      "step 2/3 | epoch 49/50 | batch 54/60 | global_step 5934 | loss_total 0.7808\n",
      "step 2/3 | epoch 49/50 | batch 55/60 | global_step 5935 | loss_total 0.7812\n",
      "step 2/3 | epoch 49/50 | batch 56/60 | global_step 5936 | loss_total 0.7506\n",
      "step 2/3 | epoch 49/50 | batch 57/60 | global_step 5937 | loss_total 1.2329\n",
      "step 2/3 | epoch 49/50 | batch 58/60 | global_step 5938 | loss_total 0.7461\n",
      "step 2/3 | epoch 49/50 | batch 59/60 | global_step 5939 | loss_total 1.2341\n",
      "step 2/3 | epoch 49/50 | batch 60/60 | global_step 5940 | loss_total 0.7940\n",
      "[epoch done] step 2/3 epoch 49/50 | train_total=1.1204 val_total=0.9146\n",
      "step 2/3 | epoch 50/50 | batch 1/60 | global_step 5941 | loss_total 0.7861\n",
      "step 2/3 | epoch 50/50 | batch 2/60 | global_step 5942 | loss_total 2.4268\n",
      "step 2/3 | epoch 50/50 | batch 3/60 | global_step 5943 | loss_total 0.7811\n",
      "step 2/3 | epoch 50/50 | batch 4/60 | global_step 5944 | loss_total 1.2629\n",
      "step 2/3 | epoch 50/50 | batch 5/60 | global_step 5945 | loss_total 0.7966\n",
      "step 2/3 | epoch 50/50 | batch 6/60 | global_step 5946 | loss_total 1.6008\n",
      "step 2/3 | epoch 50/50 | batch 7/60 | global_step 5947 | loss_total 1.4579\n",
      "step 2/3 | epoch 50/50 | batch 8/60 | global_step 5948 | loss_total 0.7878\n",
      "step 2/3 | epoch 50/50 | batch 9/60 | global_step 5949 | loss_total 1.5365\n",
      "step 2/3 | epoch 50/50 | batch 10/60 | global_step 5950 | loss_total 1.3502\n",
      "step 2/3 | epoch 50/50 | batch 11/60 | global_step 5951 | loss_total 0.7399\n",
      "step 2/3 | epoch 50/50 | batch 12/60 | global_step 5952 | loss_total 1.3963\n",
      "step 2/3 | epoch 50/50 | batch 13/60 | global_step 5953 | loss_total 1.3282\n",
      "step 2/3 | epoch 50/50 | batch 14/60 | global_step 5954 | loss_total 0.9527\n",
      "step 2/3 | epoch 50/50 | batch 15/60 | global_step 5955 | loss_total 0.7433\n",
      "step 2/3 | epoch 50/50 | batch 16/60 | global_step 5956 | loss_total 1.3413\n",
      "step 2/3 | epoch 50/50 | batch 17/60 | global_step 5957 | loss_total 1.2133\n",
      "step 2/3 | epoch 50/50 | batch 18/60 | global_step 5958 | loss_total 0.8391\n",
      "step 2/3 | epoch 50/50 | batch 19/60 | global_step 5959 | loss_total 0.9321\n",
      "step 2/3 | epoch 50/50 | batch 20/60 | global_step 5960 | loss_total 0.8094\n",
      "step 2/3 | epoch 50/50 | batch 21/60 | global_step 5961 | loss_total 0.7695\n",
      "step 2/3 | epoch 50/50 | batch 22/60 | global_step 5962 | loss_total 0.8405\n",
      "step 2/3 | epoch 50/50 | batch 23/60 | global_step 5963 | loss_total 1.3895\n",
      "step 2/3 | epoch 50/50 | batch 24/60 | global_step 5964 | loss_total 0.7753\n",
      "step 2/3 | epoch 50/50 | batch 25/60 | global_step 5965 | loss_total 0.7506\n",
      "step 2/3 | epoch 50/50 | batch 26/60 | global_step 5966 | loss_total 0.7804\n",
      "step 2/3 | epoch 50/50 | batch 27/60 | global_step 5967 | loss_total 0.8102\n",
      "step 2/3 | epoch 50/50 | batch 28/60 | global_step 5968 | loss_total 1.6724\n",
      "step 2/3 | epoch 50/50 | batch 29/60 | global_step 5969 | loss_total 0.9939\n",
      "step 2/3 | epoch 50/50 | batch 30/60 | global_step 5970 | loss_total 0.8060\n",
      "step 2/3 | epoch 50/50 | batch 31/60 | global_step 5971 | loss_total 0.7715\n",
      "step 2/3 | epoch 50/50 | batch 32/60 | global_step 5972 | loss_total 1.6094\n",
      "step 2/3 | epoch 50/50 | batch 33/60 | global_step 5973 | loss_total 1.5342\n",
      "step 2/3 | epoch 50/50 | batch 34/60 | global_step 5974 | loss_total 0.8407\n",
      "step 2/3 | epoch 50/50 | batch 35/60 | global_step 5975 | loss_total 0.9174\n",
      "step 2/3 | epoch 50/50 | batch 36/60 | global_step 5976 | loss_total 0.8096\n",
      "step 2/3 | epoch 50/50 | batch 37/60 | global_step 5977 | loss_total 0.8070\n",
      "step 2/3 | epoch 50/50 | batch 38/60 | global_step 5978 | loss_total 0.7787\n",
      "step 2/3 | epoch 50/50 | batch 39/60 | global_step 5979 | loss_total 1.3048\n",
      "step 2/3 | epoch 50/50 | batch 40/60 | global_step 5980 | loss_total 0.7710\n",
      "step 2/3 | epoch 50/50 | batch 41/60 | global_step 5981 | loss_total 0.8101\n",
      "step 2/3 | epoch 50/50 | batch 42/60 | global_step 5982 | loss_total 1.6354\n",
      "step 2/3 | epoch 50/50 | batch 43/60 | global_step 5983 | loss_total 1.7708\n",
      "step 2/3 | epoch 50/50 | batch 44/60 | global_step 5984 | loss_total 1.7047\n",
      "step 2/3 | epoch 50/50 | batch 45/60 | global_step 5985 | loss_total 1.7238\n",
      "step 2/3 | epoch 50/50 | batch 46/60 | global_step 5986 | loss_total 0.7622\n",
      "step 2/3 | epoch 50/50 | batch 47/60 | global_step 5987 | loss_total 0.8464\n",
      "step 2/3 | epoch 50/50 | batch 48/60 | global_step 5988 | loss_total 1.4342\n",
      "step 2/3 | epoch 50/50 | batch 49/60 | global_step 5989 | loss_total 0.8160\n",
      "step 2/3 | epoch 50/50 | batch 50/60 | global_step 5990 | loss_total 0.8912\n",
      "step 2/3 | epoch 50/50 | batch 51/60 | global_step 5991 | loss_total 1.0169\n",
      "step 2/3 | epoch 50/50 | batch 52/60 | global_step 5992 | loss_total 0.8086\n",
      "step 2/3 | epoch 50/50 | batch 53/60 | global_step 5993 | loss_total 1.3437\n",
      "step 2/3 | epoch 50/50 | batch 54/60 | global_step 5994 | loss_total 0.8899\n",
      "step 2/3 | epoch 50/50 | batch 55/60 | global_step 5995 | loss_total 1.3836\n",
      "step 2/3 | epoch 50/50 | batch 56/60 | global_step 5996 | loss_total 0.7910\n",
      "step 2/3 | epoch 50/50 | batch 57/60 | global_step 5997 | loss_total 1.6043\n",
      "step 2/3 | epoch 50/50 | batch 58/60 | global_step 5998 | loss_total 1.2881\n",
      "step 2/3 | epoch 50/50 | batch 59/60 | global_step 5999 | loss_total 0.7461\n",
      "step 2/3 | epoch 50/50 | batch 60/60 | global_step 6000 | loss_total 0.8416\n",
      "[epoch done] step 2/3 epoch 50/50 | train_total=1.0987 val_total=0.9078\n",
      "\n",
      "[Phase 3] files=5,406 train_chunks=1,675 val_chunks=325 epochs=20\n",
      "phase3 balance (is_music): {1: 2703, 0: 2703}\n",
      "trainable params tensors: 26/26\n",
      "[phase3] epoch 1/20 hard_negatives=OFF train_chunks=1675\n",
      "step 3/3 | epoch 1/20 | batch 1/60 | global_step 6001 | loss_total 10.7773\n",
      "step 3/3 | epoch 1/20 | batch 2/60 | global_step 6002 | loss_total 0.4518\n",
      "step 3/3 | epoch 1/20 | batch 3/60 | global_step 6003 | loss_total 0.4549\n",
      "step 3/3 | epoch 1/20 | batch 4/60 | global_step 6004 | loss_total 9.9648\n",
      "step 3/3 | epoch 1/20 | batch 5/60 | global_step 6005 | loss_total 0.4517\n",
      "step 3/3 | epoch 1/20 | batch 6/60 | global_step 6006 | loss_total 0.7389\n",
      "step 3/3 | epoch 1/20 | batch 7/60 | global_step 6007 | loss_total 0.4230\n",
      "step 3/3 | epoch 1/20 | batch 8/60 | global_step 6008 | loss_total 0.4747\n",
      "step 3/3 | epoch 1/20 | batch 9/60 | global_step 6009 | loss_total 10.2607\n",
      "step 3/3 | epoch 1/20 | batch 10/60 | global_step 6010 | loss_total 1.0198\n",
      "step 3/3 | epoch 1/20 | batch 11/60 | global_step 6011 | loss_total 4.8945\n",
      "step 3/3 | epoch 1/20 | batch 12/60 | global_step 6012 | loss_total 4.6598\n",
      "step 3/3 | epoch 1/20 | batch 13/60 | global_step 6013 | loss_total 0.5009\n",
      "step 3/3 | epoch 1/20 | batch 14/60 | global_step 6014 | loss_total 5.7530\n",
      "step 3/3 | epoch 1/20 | batch 15/60 | global_step 6015 | loss_total 4.4768\n",
      "step 3/3 | epoch 1/20 | batch 16/60 | global_step 6016 | loss_total 0.7264\n",
      "step 3/3 | epoch 1/20 | batch 17/60 | global_step 6017 | loss_total 9.8596\n",
      "step 3/3 | epoch 1/20 | batch 18/60 | global_step 6018 | loss_total 0.6788\n",
      "step 3/3 | epoch 1/20 | batch 19/60 | global_step 6019 | loss_total 4.6023\n",
      "step 3/3 | epoch 1/20 | batch 20/60 | global_step 6020 | loss_total 3.7310\n",
      "step 3/3 | epoch 1/20 | batch 21/60 | global_step 6021 | loss_total 0.6303\n",
      "step 3/3 | epoch 1/20 | batch 22/60 | global_step 6022 | loss_total 0.4862\n",
      "step 3/3 | epoch 1/20 | batch 23/60 | global_step 6023 | loss_total 4.6483\n",
      "step 3/3 | epoch 1/20 | batch 24/60 | global_step 6024 | loss_total 5.1986\n",
      "step 3/3 | epoch 1/20 | batch 25/60 | global_step 6025 | loss_total 0.4576\n",
      "step 3/3 | epoch 1/20 | batch 26/60 | global_step 6026 | loss_total 4.9219\n",
      "step 3/3 | epoch 1/20 | batch 27/60 | global_step 6027 | loss_total 9.5339\n",
      "step 3/3 | epoch 1/20 | batch 28/60 | global_step 6028 | loss_total 0.4863\n",
      "step 3/3 | epoch 1/20 | batch 29/60 | global_step 6029 | loss_total 0.5355\n",
      "step 3/3 | epoch 1/20 | batch 30/60 | global_step 6030 | loss_total 0.4291\n",
      "step 3/3 | epoch 1/20 | batch 31/60 | global_step 6031 | loss_total 12.1117\n",
      "step 3/3 | epoch 1/20 | batch 32/60 | global_step 6032 | loss_total 0.4903\n",
      "step 3/3 | epoch 1/20 | batch 33/60 | global_step 6033 | loss_total 11.8241\n",
      "step 3/3 | epoch 1/20 | batch 34/60 | global_step 6034 | loss_total 6.2264\n",
      "step 3/3 | epoch 1/20 | batch 35/60 | global_step 6035 | loss_total 0.8422\n",
      "step 3/3 | epoch 1/20 | batch 36/60 | global_step 6036 | loss_total 12.2546\n",
      "step 3/3 | epoch 1/20 | batch 37/60 | global_step 6037 | loss_total 0.4822\n",
      "step 3/3 | epoch 1/20 | batch 38/60 | global_step 6038 | loss_total 3.9722\n",
      "step 3/3 | epoch 1/20 | batch 39/60 | global_step 6039 | loss_total 0.4807\n",
      "step 3/3 | epoch 1/20 | batch 40/60 | global_step 6040 | loss_total 0.4828\n",
      "step 3/3 | epoch 1/20 | batch 41/60 | global_step 6041 | loss_total 0.4234\n",
      "step 3/3 | epoch 1/20 | batch 42/60 | global_step 6042 | loss_total 7.6773\n",
      "step 3/3 | epoch 1/20 | batch 43/60 | global_step 6043 | loss_total 9.6169\n",
      "step 3/3 | epoch 1/20 | batch 44/60 | global_step 6044 | loss_total 0.9570\n",
      "step 3/3 | epoch 1/20 | batch 45/60 | global_step 6045 | loss_total 1.4068\n",
      "step 3/3 | epoch 1/20 | batch 46/60 | global_step 6046 | loss_total 7.8924\n",
      "step 3/3 | epoch 1/20 | batch 47/60 | global_step 6047 | loss_total 7.5162\n",
      "step 3/3 | epoch 1/20 | batch 48/60 | global_step 6048 | loss_total 1.0474\n",
      "step 3/3 | epoch 1/20 | batch 49/60 | global_step 6049 | loss_total 0.4748\n",
      "step 3/3 | epoch 1/20 | batch 50/60 | global_step 6050 | loss_total 6.0519\n",
      "step 3/3 | epoch 1/20 | batch 51/60 | global_step 6051 | loss_total 3.8607\n",
      "step 3/3 | epoch 1/20 | batch 52/60 | global_step 6052 | loss_total 0.7826\n",
      "step 3/3 | epoch 1/20 | batch 53/60 | global_step 6053 | loss_total 3.9694\n",
      "step 3/3 | epoch 1/20 | batch 54/60 | global_step 6054 | loss_total 0.9701\n",
      "step 3/3 | epoch 1/20 | batch 55/60 | global_step 6055 | loss_total 7.3748\n",
      "step 3/3 | epoch 1/20 | batch 56/60 | global_step 6056 | loss_total 0.7395\n",
      "step 3/3 | epoch 1/20 | batch 57/60 | global_step 6057 | loss_total 0.7503\n",
      "step 3/3 | epoch 1/20 | batch 58/60 | global_step 6058 | loss_total 6.2316\n",
      "step 3/3 | epoch 1/20 | batch 59/60 | global_step 6059 | loss_total 0.4872\n",
      "step 3/3 | epoch 1/20 | batch 60/60 | global_step 6060 | loss_total 0.4881\n",
      "[epoch done] step 3/3 epoch 1/20 | train_total=3.6686 val_total=0.6381\n",
      "step 3/3 | epoch 2/20 | batch 1/60 | global_step 6061 | loss_total 2.8634\n",
      "step 3/3 | epoch 2/20 | batch 2/60 | global_step 6062 | loss_total 2.9752\n",
      "step 3/3 | epoch 2/20 | batch 3/60 | global_step 6063 | loss_total 3.6080\n",
      "step 3/3 | epoch 2/20 | batch 4/60 | global_step 6064 | loss_total 0.9325\n",
      "step 3/3 | epoch 2/20 | batch 5/60 | global_step 6065 | loss_total 3.4721\n",
      "step 3/3 | epoch 2/20 | batch 6/60 | global_step 6066 | loss_total 2.7702\n",
      "step 3/3 | epoch 2/20 | batch 7/60 | global_step 6067 | loss_total 1.1457\n",
      "step 3/3 | epoch 2/20 | batch 8/60 | global_step 6068 | loss_total 0.4783\n",
      "step 3/3 | epoch 2/20 | batch 9/60 | global_step 6069 | loss_total 4.5577\n",
      "step 3/3 | epoch 2/20 | batch 10/60 | global_step 6070 | loss_total 0.4799\n",
      "step 3/3 | epoch 2/20 | batch 11/60 | global_step 6071 | loss_total 3.1671\n",
      "step 3/3 | epoch 2/20 | batch 12/60 | global_step 6072 | loss_total 1.1058\n",
      "step 3/3 | epoch 2/20 | batch 13/60 | global_step 6073 | loss_total 0.5499\n",
      "step 3/3 | epoch 2/20 | batch 14/60 | global_step 6074 | loss_total 3.6570\n",
      "step 3/3 | epoch 2/20 | batch 15/60 | global_step 6075 | loss_total 0.7890\n",
      "step 3/3 | epoch 2/20 | batch 16/60 | global_step 6076 | loss_total 0.8614\n",
      "step 3/3 | epoch 2/20 | batch 17/60 | global_step 6077 | loss_total 5.2035\n",
      "step 3/3 | epoch 2/20 | batch 18/60 | global_step 6078 | loss_total 0.5283\n",
      "step 3/3 | epoch 2/20 | batch 19/60 | global_step 6079 | loss_total 0.5266\n",
      "step 3/3 | epoch 2/20 | batch 20/60 | global_step 6080 | loss_total 2.9184\n",
      "step 3/3 | epoch 2/20 | batch 21/60 | global_step 6081 | loss_total 3.1768\n",
      "step 3/3 | epoch 2/20 | batch 22/60 | global_step 6082 | loss_total 4.6547\n",
      "step 3/3 | epoch 2/20 | batch 23/60 | global_step 6083 | loss_total 0.5236\n",
      "step 3/3 | epoch 2/20 | batch 24/60 | global_step 6084 | loss_total 0.4897\n",
      "step 3/3 | epoch 2/20 | batch 25/60 | global_step 6085 | loss_total 3.9426\n",
      "step 3/3 | epoch 2/20 | batch 26/60 | global_step 6086 | loss_total 6.3463\n",
      "step 3/3 | epoch 2/20 | batch 27/60 | global_step 6087 | loss_total 5.4229\n",
      "step 3/3 | epoch 2/20 | batch 28/60 | global_step 6088 | loss_total 0.5348\n",
      "step 3/3 | epoch 2/20 | batch 29/60 | global_step 6089 | loss_total 0.8380\n",
      "step 3/3 | epoch 2/20 | batch 30/60 | global_step 6090 | loss_total 0.7717\n",
      "step 3/3 | epoch 2/20 | batch 31/60 | global_step 6091 | loss_total 1.3397\n",
      "step 3/3 | epoch 2/20 | batch 32/60 | global_step 6092 | loss_total 0.5442\n",
      "step 3/3 | epoch 2/20 | batch 33/60 | global_step 6093 | loss_total 0.7685\n",
      "step 3/3 | epoch 2/20 | batch 34/60 | global_step 6094 | loss_total 2.8450\n",
      "step 3/3 | epoch 2/20 | batch 35/60 | global_step 6095 | loss_total 0.4995\n",
      "step 3/3 | epoch 2/20 | batch 36/60 | global_step 6096 | loss_total 0.5896\n",
      "step 3/3 | epoch 2/20 | batch 37/60 | global_step 6097 | loss_total 0.8176\n",
      "step 3/3 | epoch 2/20 | batch 38/60 | global_step 6098 | loss_total 3.1426\n",
      "step 3/3 | epoch 2/20 | batch 39/60 | global_step 6099 | loss_total 2.3813\n",
      "step 3/3 | epoch 2/20 | batch 40/60 | global_step 6100 | loss_total 0.4889\n",
      "step 3/3 | epoch 2/20 | batch 41/60 | global_step 6101 | loss_total 2.2660\n",
      "step 3/3 | epoch 2/20 | batch 42/60 | global_step 6102 | loss_total 0.5307\n",
      "step 3/3 | epoch 2/20 | batch 43/60 | global_step 6103 | loss_total 0.4923\n",
      "step 3/3 | epoch 2/20 | batch 44/60 | global_step 6104 | loss_total 0.4867\n",
      "step 3/3 | epoch 2/20 | batch 45/60 | global_step 6105 | loss_total 3.1169\n",
      "step 3/3 | epoch 2/20 | batch 46/60 | global_step 6106 | loss_total 2.5062\n",
      "step 3/3 | epoch 2/20 | batch 47/60 | global_step 6107 | loss_total 3.9630\n",
      "step 3/3 | epoch 2/20 | batch 48/60 | global_step 6108 | loss_total 0.6062\n",
      "step 3/3 | epoch 2/20 | batch 49/60 | global_step 6109 | loss_total 0.5167\n",
      "step 3/3 | epoch 2/20 | batch 50/60 | global_step 6110 | loss_total 2.4711\n",
      "step 3/3 | epoch 2/20 | batch 51/60 | global_step 6111 | loss_total 0.4993\n",
      "step 3/3 | epoch 2/20 | batch 52/60 | global_step 6112 | loss_total 0.7427\n",
      "step 3/3 | epoch 2/20 | batch 53/60 | global_step 6113 | loss_total 0.7315\n",
      "step 3/3 | epoch 2/20 | batch 54/60 | global_step 6114 | loss_total 0.7417\n",
      "step 3/3 | epoch 2/20 | batch 55/60 | global_step 6115 | loss_total 0.5143\n",
      "step 3/3 | epoch 2/20 | batch 56/60 | global_step 6116 | loss_total 0.5572\n",
      "step 3/3 | epoch 2/20 | batch 57/60 | global_step 6117 | loss_total 3.6879\n",
      "step 3/3 | epoch 2/20 | batch 58/60 | global_step 6118 | loss_total 0.5124\n",
      "step 3/3 | epoch 2/20 | batch 59/60 | global_step 6119 | loss_total 6.7876\n",
      "step 3/3 | epoch 2/20 | batch 60/60 | global_step 6120 | loss_total 2.4573\n",
      "[epoch done] step 3/3 epoch 2/20 | train_total=1.9483 val_total=0.6088\n",
      "step 3/3 | epoch 3/20 | batch 1/60 | global_step 6121 | loss_total 0.9508\n",
      "step 3/3 | epoch 3/20 | batch 2/60 | global_step 6122 | loss_total 1.8378\n",
      "step 3/3 | epoch 3/20 | batch 3/60 | global_step 6123 | loss_total 3.1878\n",
      "step 3/3 | epoch 3/20 | batch 4/60 | global_step 6124 | loss_total 0.9232\n",
      "step 3/3 | epoch 3/20 | batch 5/60 | global_step 6125 | loss_total 0.5171\n",
      "step 3/3 | epoch 3/20 | batch 6/60 | global_step 6126 | loss_total 0.5180\n",
      "step 3/3 | epoch 3/20 | batch 7/60 | global_step 6127 | loss_total 3.0694\n",
      "step 3/3 | epoch 3/20 | batch 8/60 | global_step 6128 | loss_total 0.4753\n",
      "step 3/3 | epoch 3/20 | batch 9/60 | global_step 6129 | loss_total 0.5230\n",
      "step 3/3 | epoch 3/20 | batch 10/60 | global_step 6130 | loss_total 1.7185\n",
      "step 3/3 | epoch 3/20 | batch 11/60 | global_step 6131 | loss_total 1.8174\n",
      "step 3/3 | epoch 3/20 | batch 12/60 | global_step 6132 | loss_total 0.5995\n",
      "step 3/3 | epoch 3/20 | batch 13/60 | global_step 6133 | loss_total 0.4739\n",
      "step 3/3 | epoch 3/20 | batch 14/60 | global_step 6134 | loss_total 5.5980\n",
      "step 3/3 | epoch 3/20 | batch 15/60 | global_step 6135 | loss_total 0.5308\n",
      "step 3/3 | epoch 3/20 | batch 16/60 | global_step 6136 | loss_total 1.6331\n",
      "step 3/3 | epoch 3/20 | batch 17/60 | global_step 6137 | loss_total 1.6031\n",
      "step 3/3 | epoch 3/20 | batch 18/60 | global_step 6138 | loss_total 0.5392\n",
      "step 3/3 | epoch 3/20 | batch 19/60 | global_step 6139 | loss_total 0.8205\n",
      "step 3/3 | epoch 3/20 | batch 20/60 | global_step 6140 | loss_total 2.3138\n",
      "step 3/3 | epoch 3/20 | batch 21/60 | global_step 6141 | loss_total 1.6552\n",
      "step 3/3 | epoch 3/20 | batch 22/60 | global_step 6142 | loss_total 1.3150\n",
      "step 3/3 | epoch 3/20 | batch 23/60 | global_step 6143 | loss_total 0.8140\n",
      "step 3/3 | epoch 3/20 | batch 24/60 | global_step 6144 | loss_total 1.5119\n",
      "step 3/3 | epoch 3/20 | batch 25/60 | global_step 6145 | loss_total 0.5255\n",
      "step 3/3 | epoch 3/20 | batch 26/60 | global_step 6146 | loss_total 2.7148\n",
      "step 3/3 | epoch 3/20 | batch 27/60 | global_step 6147 | loss_total 0.7620\n",
      "step 3/3 | epoch 3/20 | batch 28/60 | global_step 6148 | loss_total 1.4665\n",
      "step 3/3 | epoch 3/20 | batch 29/60 | global_step 6149 | loss_total 3.1487\n",
      "step 3/3 | epoch 3/20 | batch 30/60 | global_step 6150 | loss_total 0.5180\n",
      "step 3/3 | epoch 3/20 | batch 31/60 | global_step 6151 | loss_total 1.7901\n",
      "step 3/3 | epoch 3/20 | batch 32/60 | global_step 6152 | loss_total 0.7910\n",
      "step 3/3 | epoch 3/20 | batch 33/60 | global_step 6153 | loss_total 7.6335\n",
      "step 3/3 | epoch 3/20 | batch 34/60 | global_step 6154 | loss_total 0.4872\n",
      "step 3/3 | epoch 3/20 | batch 35/60 | global_step 6155 | loss_total 0.7635\n",
      "step 3/3 | epoch 3/20 | batch 36/60 | global_step 6156 | loss_total 0.5127\n",
      "step 3/3 | epoch 3/20 | batch 37/60 | global_step 6157 | loss_total 0.5112\n",
      "step 3/3 | epoch 3/20 | batch 38/60 | global_step 6158 | loss_total 0.5095\n",
      "step 3/3 | epoch 3/20 | batch 39/60 | global_step 6159 | loss_total 0.5077\n",
      "step 3/3 | epoch 3/20 | batch 40/60 | global_step 6160 | loss_total 2.3079\n",
      "step 3/3 | epoch 3/20 | batch 41/60 | global_step 6161 | loss_total 4.2231\n",
      "step 3/3 | epoch 3/20 | batch 42/60 | global_step 6162 | loss_total 1.4269\n",
      "step 3/3 | epoch 3/20 | batch 43/60 | global_step 6163 | loss_total 0.5031\n",
      "step 3/3 | epoch 3/20 | batch 44/60 | global_step 6164 | loss_total 0.8436\n",
      "step 3/3 | epoch 3/20 | batch 45/60 | global_step 6165 | loss_total 1.0430\n",
      "step 3/3 | epoch 3/20 | batch 46/60 | global_step 6166 | loss_total 4.3419\n",
      "step 3/3 | epoch 3/20 | batch 47/60 | global_step 6167 | loss_total 0.7286\n",
      "step 3/3 | epoch 3/20 | batch 48/60 | global_step 6168 | loss_total 0.8231\n",
      "step 3/3 | epoch 3/20 | batch 49/60 | global_step 6169 | loss_total 1.4961\n",
      "step 3/3 | epoch 3/20 | batch 50/60 | global_step 6170 | loss_total 0.5154\n",
      "step 3/3 | epoch 3/20 | batch 51/60 | global_step 6171 | loss_total 0.5671\n",
      "step 3/3 | epoch 3/20 | batch 52/60 | global_step 6172 | loss_total 4.5455\n",
      "step 3/3 | epoch 3/20 | batch 53/60 | global_step 6173 | loss_total 4.0826\n",
      "step 3/3 | epoch 3/20 | batch 54/60 | global_step 6174 | loss_total 0.7525\n",
      "step 3/3 | epoch 3/20 | batch 55/60 | global_step 6175 | loss_total 0.7495\n",
      "step 3/3 | epoch 3/20 | batch 56/60 | global_step 6176 | loss_total 1.4544\n",
      "step 3/3 | epoch 3/20 | batch 57/60 | global_step 6177 | loss_total 0.5348\n",
      "step 3/3 | epoch 3/20 | batch 58/60 | global_step 6178 | loss_total 0.8689\n",
      "step 3/3 | epoch 3/20 | batch 59/60 | global_step 6179 | loss_total 0.9693\n",
      "step 3/3 | epoch 3/20 | batch 60/60 | global_step 6180 | loss_total 0.5456\n",
      "[epoch done] step 3/3 epoch 3/20 | train_total=1.5152 val_total=0.5339\n",
      "step 3/3 | epoch 4/20 | batch 1/60 | global_step 6181 | loss_total 0.5420\n",
      "step 3/3 | epoch 4/20 | batch 2/60 | global_step 6182 | loss_total 0.5730\n",
      "step 3/3 | epoch 4/20 | batch 3/60 | global_step 6183 | loss_total 1.8454\n",
      "step 3/3 | epoch 4/20 | batch 4/60 | global_step 6184 | loss_total 0.6831\n",
      "step 3/3 | epoch 4/20 | batch 5/60 | global_step 6185 | loss_total 1.6742\n",
      "step 3/3 | epoch 4/20 | batch 6/60 | global_step 6186 | loss_total 2.3696\n",
      "step 3/3 | epoch 4/20 | batch 7/60 | global_step 6187 | loss_total 0.6238\n",
      "step 3/3 | epoch 4/20 | batch 8/60 | global_step 6188 | loss_total 1.0643\n",
      "step 3/3 | epoch 4/20 | batch 9/60 | global_step 6189 | loss_total 2.2776\n",
      "step 3/3 | epoch 4/20 | batch 10/60 | global_step 6190 | loss_total 0.5354\n",
      "step 3/3 | epoch 4/20 | batch 11/60 | global_step 6191 | loss_total 0.5463\n",
      "step 3/3 | epoch 4/20 | batch 12/60 | global_step 6192 | loss_total 6.3453\n",
      "step 3/3 | epoch 4/20 | batch 13/60 | global_step 6193 | loss_total 0.5429\n",
      "step 3/3 | epoch 4/20 | batch 14/60 | global_step 6194 | loss_total 1.4975\n",
      "step 3/3 | epoch 4/20 | batch 15/60 | global_step 6195 | loss_total 0.7394\n",
      "step 3/3 | epoch 4/20 | batch 16/60 | global_step 6196 | loss_total 0.5246\n",
      "step 3/3 | epoch 4/20 | batch 17/60 | global_step 6197 | loss_total 0.5170\n",
      "step 3/3 | epoch 4/20 | batch 18/60 | global_step 6198 | loss_total 4.8884\n",
      "step 3/3 | epoch 4/20 | batch 19/60 | global_step 6199 | loss_total 5.9660\n",
      "step 3/3 | epoch 4/20 | batch 20/60 | global_step 6200 | loss_total 0.7542\n",
      "step 3/3 | epoch 4/20 | batch 21/60 | global_step 6201 | loss_total 0.9791\n",
      "step 3/3 | epoch 4/20 | batch 22/60 | global_step 6202 | loss_total 3.5011\n",
      "step 3/3 | epoch 4/20 | batch 23/60 | global_step 6203 | loss_total 1.7737\n",
      "step 3/3 | epoch 4/20 | batch 24/60 | global_step 6204 | loss_total 0.5292\n",
      "step 3/3 | epoch 4/20 | batch 25/60 | global_step 6205 | loss_total 0.9655\n",
      "step 3/3 | epoch 4/20 | batch 26/60 | global_step 6206 | loss_total 1.7222\n",
      "step 3/3 | epoch 4/20 | batch 27/60 | global_step 6207 | loss_total 0.5132\n",
      "step 3/3 | epoch 4/20 | batch 28/60 | global_step 6208 | loss_total 0.7806\n",
      "step 3/3 | epoch 4/20 | batch 29/60 | global_step 6209 | loss_total 0.7368\n",
      "step 3/3 | epoch 4/20 | batch 30/60 | global_step 6210 | loss_total 4.0740\n",
      "step 3/3 | epoch 4/20 | batch 31/60 | global_step 6211 | loss_total 2.2697\n",
      "step 3/3 | epoch 4/20 | batch 32/60 | global_step 6212 | loss_total 1.5739\n",
      "step 3/3 | epoch 4/20 | batch 33/60 | global_step 6213 | loss_total 0.5836\n",
      "step 3/3 | epoch 4/20 | batch 34/60 | global_step 6214 | loss_total 0.5153\n",
      "step 3/3 | epoch 4/20 | batch 35/60 | global_step 6215 | loss_total 0.5993\n",
      "step 3/3 | epoch 4/20 | batch 36/60 | global_step 6216 | loss_total 0.5812\n",
      "step 3/3 | epoch 4/20 | batch 37/60 | global_step 6217 | loss_total 0.5309\n",
      "step 3/3 | epoch 4/20 | batch 38/60 | global_step 6218 | loss_total 0.9399\n",
      "step 3/3 | epoch 4/20 | batch 39/60 | global_step 6219 | loss_total 2.3634\n",
      "step 3/3 | epoch 4/20 | batch 40/60 | global_step 6220 | loss_total 0.5194\n",
      "step 3/3 | epoch 4/20 | batch 41/60 | global_step 6221 | loss_total 3.4332\n",
      "step 3/3 | epoch 4/20 | batch 42/60 | global_step 6222 | loss_total 1.1185\n",
      "step 3/3 | epoch 4/20 | batch 43/60 | global_step 6223 | loss_total 6.8966\n",
      "step 3/3 | epoch 4/20 | batch 44/60 | global_step 6224 | loss_total 0.5102\n",
      "step 3/3 | epoch 4/20 | batch 45/60 | global_step 6225 | loss_total 2.0703\n",
      "step 3/3 | epoch 4/20 | batch 46/60 | global_step 6226 | loss_total 2.7149\n",
      "step 3/3 | epoch 4/20 | batch 47/60 | global_step 6227 | loss_total 5.4107\n",
      "step 3/3 | epoch 4/20 | batch 48/60 | global_step 6228 | loss_total 0.5241\n",
      "step 3/3 | epoch 4/20 | batch 49/60 | global_step 6229 | loss_total 0.7976\n",
      "step 3/3 | epoch 4/20 | batch 50/60 | global_step 6230 | loss_total 0.5075\n",
      "step 3/3 | epoch 4/20 | batch 51/60 | global_step 6231 | loss_total 1.7561\n",
      "step 3/3 | epoch 4/20 | batch 52/60 | global_step 6232 | loss_total 3.6530\n",
      "step 3/3 | epoch 4/20 | batch 53/60 | global_step 6233 | loss_total 3.3740\n",
      "step 3/3 | epoch 4/20 | batch 54/60 | global_step 6234 | loss_total 6.0863\n",
      "step 3/3 | epoch 4/20 | batch 55/60 | global_step 6235 | loss_total 0.7347\n",
      "step 3/3 | epoch 4/20 | batch 56/60 | global_step 6236 | loss_total 1.2351\n",
      "step 3/3 | epoch 4/20 | batch 57/60 | global_step 6237 | loss_total 0.4920\n",
      "step 3/3 | epoch 4/20 | batch 58/60 | global_step 6238 | loss_total 0.5154\n",
      "step 3/3 | epoch 4/20 | batch 59/60 | global_step 6239 | loss_total 1.3451\n",
      "step 3/3 | epoch 4/20 | batch 60/60 | global_step 6240 | loss_total 2.5720\n",
      "[epoch done] step 3/3 epoch 4/20 | train_total=1.7718 val_total=0.5760\n",
      "step 3/3 | epoch 5/20 | batch 1/60 | global_step 6241 | loss_total 0.5072\n",
      "step 3/3 | epoch 5/20 | batch 2/60 | global_step 6242 | loss_total 1.8503\n",
      "step 3/3 | epoch 5/20 | batch 3/60 | global_step 6243 | loss_total 0.5079\n",
      "step 3/3 | epoch 5/20 | batch 4/60 | global_step 6244 | loss_total 0.9034\n",
      "step 3/3 | epoch 5/20 | batch 5/60 | global_step 6245 | loss_total 1.3898\n",
      "step 3/3 | epoch 5/20 | batch 6/60 | global_step 6246 | loss_total 1.1223\n",
      "step 3/3 | epoch 5/20 | batch 7/60 | global_step 6247 | loss_total 0.5093\n",
      "step 3/3 | epoch 5/20 | batch 8/60 | global_step 6248 | loss_total 0.7770\n",
      "step 3/3 | epoch 5/20 | batch 9/60 | global_step 6249 | loss_total 0.4842\n",
      "step 3/3 | epoch 5/20 | batch 10/60 | global_step 6250 | loss_total 0.5100\n",
      "step 3/3 | epoch 5/20 | batch 11/60 | global_step 6251 | loss_total 0.5405\n",
      "step 3/3 | epoch 5/20 | batch 12/60 | global_step 6252 | loss_total 1.1765\n",
      "step 3/3 | epoch 5/20 | batch 13/60 | global_step 6253 | loss_total 1.9763\n",
      "step 3/3 | epoch 5/20 | batch 14/60 | global_step 6254 | loss_total 0.5373\n",
      "step 3/3 | epoch 5/20 | batch 15/60 | global_step 6255 | loss_total 0.5408\n",
      "step 3/3 | epoch 5/20 | batch 16/60 | global_step 6256 | loss_total 0.4797\n",
      "step 3/3 | epoch 5/20 | batch 17/60 | global_step 6257 | loss_total 1.3478\n",
      "step 3/3 | epoch 5/20 | batch 18/60 | global_step 6258 | loss_total 1.8134\n",
      "step 3/3 | epoch 5/20 | batch 19/60 | global_step 6259 | loss_total 0.6425\n",
      "step 3/3 | epoch 5/20 | batch 20/60 | global_step 6260 | loss_total 2.8411\n",
      "step 3/3 | epoch 5/20 | batch 21/60 | global_step 6261 | loss_total 0.6190\n",
      "step 3/3 | epoch 5/20 | batch 22/60 | global_step 6262 | loss_total 1.0282\n",
      "step 3/3 | epoch 5/20 | batch 23/60 | global_step 6263 | loss_total 1.4591\n",
      "step 3/3 | epoch 5/20 | batch 24/60 | global_step 6264 | loss_total 0.5181\n",
      "step 3/3 | epoch 5/20 | batch 25/60 | global_step 6265 | loss_total 0.7510\n",
      "step 3/3 | epoch 5/20 | batch 26/60 | global_step 6266 | loss_total 0.8349\n",
      "step 3/3 | epoch 5/20 | batch 27/60 | global_step 6267 | loss_total 1.4258\n",
      "step 3/3 | epoch 5/20 | batch 28/60 | global_step 6268 | loss_total 1.4174\n",
      "step 3/3 | epoch 5/20 | batch 29/60 | global_step 6269 | loss_total 1.5501\n",
      "step 3/3 | epoch 5/20 | batch 30/60 | global_step 6270 | loss_total 0.4999\n",
      "step 3/3 | epoch 5/20 | batch 31/60 | global_step 6271 | loss_total 0.5697\n",
      "step 3/3 | epoch 5/20 | batch 32/60 | global_step 6272 | loss_total 1.6506\n",
      "step 3/3 | epoch 5/20 | batch 33/60 | global_step 6273 | loss_total 2.5337\n",
      "step 3/3 | epoch 5/20 | batch 34/60 | global_step 6274 | loss_total 1.6856\n",
      "step 3/3 | epoch 5/20 | batch 35/60 | global_step 6275 | loss_total 9.2378\n",
      "step 3/3 | epoch 5/20 | batch 36/60 | global_step 6276 | loss_total 1.3088\n",
      "step 3/3 | epoch 5/20 | batch 37/60 | global_step 6277 | loss_total 0.5243\n",
      "step 3/3 | epoch 5/20 | batch 38/60 | global_step 6278 | loss_total 1.2960\n",
      "step 3/3 | epoch 5/20 | batch 39/60 | global_step 6279 | loss_total 0.5078\n",
      "step 3/3 | epoch 5/20 | batch 40/60 | global_step 6280 | loss_total 0.5143\n",
      "step 3/3 | epoch 5/20 | batch 41/60 | global_step 6281 | loss_total 1.2049\n",
      "step 3/3 | epoch 5/20 | batch 42/60 | global_step 6282 | loss_total 0.5038\n",
      "step 3/3 | epoch 5/20 | batch 43/60 | global_step 6283 | loss_total 1.5379\n",
      "step 3/3 | epoch 5/20 | batch 44/60 | global_step 6284 | loss_total 0.5122\n",
      "step 3/3 | epoch 5/20 | batch 45/60 | global_step 6285 | loss_total 0.8398\n",
      "step 3/3 | epoch 5/20 | batch 46/60 | global_step 6286 | loss_total 0.7040\n",
      "step 3/3 | epoch 5/20 | batch 47/60 | global_step 6287 | loss_total 0.5367\n",
      "step 3/3 | epoch 5/20 | batch 48/60 | global_step 6288 | loss_total 0.5103\n",
      "step 3/3 | epoch 5/20 | batch 49/60 | global_step 6289 | loss_total 2.6922\n",
      "step 3/3 | epoch 5/20 | batch 50/60 | global_step 6290 | loss_total 4.3328\n",
      "step 3/3 | epoch 5/20 | batch 51/60 | global_step 6291 | loss_total 0.5030\n",
      "step 3/3 | epoch 5/20 | batch 52/60 | global_step 6292 | loss_total 0.9137\n",
      "step 3/3 | epoch 5/20 | batch 53/60 | global_step 6293 | loss_total 0.5862\n",
      "step 3/3 | epoch 5/20 | batch 54/60 | global_step 6294 | loss_total 0.7537\n",
      "step 3/3 | epoch 5/20 | batch 55/60 | global_step 6295 | loss_total 2.0373\n",
      "step 3/3 | epoch 5/20 | batch 56/60 | global_step 6296 | loss_total 0.5008\n",
      "step 3/3 | epoch 5/20 | batch 57/60 | global_step 6297 | loss_total 1.0360\n",
      "step 3/3 | epoch 5/20 | batch 58/60 | global_step 6298 | loss_total 1.2819\n",
      "step 3/3 | epoch 5/20 | batch 59/60 | global_step 6299 | loss_total 1.2534\n",
      "step 3/3 | epoch 5/20 | batch 60/60 | global_step 6300 | loss_total 1.3435\n",
      "[epoch done] step 3/3 epoch 5/20 | train_total=1.2329 val_total=0.5973\n",
      "step 3/3 | epoch 6/20 | batch 1/60 | global_step 6301 | loss_total 0.7499\n",
      "step 3/3 | epoch 6/20 | batch 2/60 | global_step 6302 | loss_total 13.3569\n",
      "step 3/3 | epoch 6/20 | batch 3/60 | global_step 6303 | loss_total 0.7465\n",
      "step 3/3 | epoch 6/20 | batch 4/60 | global_step 6304 | loss_total 0.9396\n",
      "step 3/3 | epoch 6/20 | batch 5/60 | global_step 6305 | loss_total 1.3563\n",
      "step 3/3 | epoch 6/20 | batch 6/60 | global_step 6306 | loss_total 6.0518\n",
      "step 3/3 | epoch 6/20 | batch 7/60 | global_step 6307 | loss_total 0.5193\n",
      "step 3/3 | epoch 6/20 | batch 8/60 | global_step 6308 | loss_total 1.1008\n",
      "step 3/3 | epoch 6/20 | batch 9/60 | global_step 6309 | loss_total 3.6838\n",
      "step 3/3 | epoch 6/20 | batch 10/60 | global_step 6310 | loss_total 1.0816\n",
      "step 3/3 | epoch 6/20 | batch 11/60 | global_step 6311 | loss_total 2.0259\n",
      "step 3/3 | epoch 6/20 | batch 12/60 | global_step 6312 | loss_total 1.6118\n",
      "step 3/3 | epoch 6/20 | batch 13/60 | global_step 6313 | loss_total 5.1852\n",
      "step 3/3 | epoch 6/20 | batch 14/60 | global_step 6314 | loss_total 1.1007\n",
      "step 3/3 | epoch 6/20 | batch 15/60 | global_step 6315 | loss_total 0.6657\n",
      "step 3/3 | epoch 6/20 | batch 16/60 | global_step 6316 | loss_total 0.5680\n",
      "step 3/3 | epoch 6/20 | batch 17/60 | global_step 6317 | loss_total 2.1456\n",
      "step 3/3 | epoch 6/20 | batch 18/60 | global_step 6318 | loss_total 0.5916\n",
      "step 3/3 | epoch 6/20 | batch 19/60 | global_step 6319 | loss_total 0.8439\n",
      "step 3/3 | epoch 6/20 | batch 20/60 | global_step 6320 | loss_total 1.0250\n",
      "step 3/3 | epoch 6/20 | batch 21/60 | global_step 6321 | loss_total 0.5845\n",
      "step 3/3 | epoch 6/20 | batch 22/60 | global_step 6322 | loss_total 2.0732\n",
      "step 3/3 | epoch 6/20 | batch 23/60 | global_step 6323 | loss_total 0.7984\n",
      "step 3/3 | epoch 6/20 | batch 24/60 | global_step 6324 | loss_total 1.5627\n",
      "step 3/3 | epoch 6/20 | batch 25/60 | global_step 6325 | loss_total 0.8546\n",
      "step 3/3 | epoch 6/20 | batch 26/60 | global_step 6326 | loss_total 1.6799\n",
      "step 3/3 | epoch 6/20 | batch 27/60 | global_step 6327 | loss_total 0.7577\n",
      "step 3/3 | epoch 6/20 | batch 28/60 | global_step 6328 | loss_total 0.8896\n",
      "step 3/3 | epoch 6/20 | batch 29/60 | global_step 6329 | loss_total 0.7792\n",
      "step 3/3 | epoch 6/20 | batch 30/60 | global_step 6330 | loss_total 1.1236\n",
      "step 3/3 | epoch 6/20 | batch 31/60 | global_step 6331 | loss_total 1.9212\n",
      "step 3/3 | epoch 6/20 | batch 32/60 | global_step 6332 | loss_total 0.9774\n",
      "step 3/3 | epoch 6/20 | batch 33/60 | global_step 6333 | loss_total 0.5357\n",
      "step 3/3 | epoch 6/20 | batch 34/60 | global_step 6334 | loss_total 1.0004\n",
      "step 3/3 | epoch 6/20 | batch 35/60 | global_step 6335 | loss_total 1.0560\n",
      "step 3/3 | epoch 6/20 | batch 36/60 | global_step 6336 | loss_total 0.6215\n",
      "step 3/3 | epoch 6/20 | batch 37/60 | global_step 6337 | loss_total 12.0751\n",
      "step 3/3 | epoch 6/20 | batch 38/60 | global_step 6338 | loss_total 1.0528\n",
      "step 3/3 | epoch 6/20 | batch 39/60 | global_step 6339 | loss_total 0.7925\n",
      "step 3/3 | epoch 6/20 | batch 40/60 | global_step 6340 | loss_total 1.0693\n",
      "step 3/3 | epoch 6/20 | batch 41/60 | global_step 6341 | loss_total 1.4076\n",
      "step 3/3 | epoch 6/20 | batch 42/60 | global_step 6342 | loss_total 0.5605\n",
      "step 3/3 | epoch 6/20 | batch 43/60 | global_step 6343 | loss_total 0.7909\n",
      "step 3/3 | epoch 6/20 | batch 44/60 | global_step 6344 | loss_total 1.1588\n",
      "step 3/3 | epoch 6/20 | batch 45/60 | global_step 6345 | loss_total 2.0116\n",
      "step 3/3 | epoch 6/20 | batch 46/60 | global_step 6346 | loss_total 1.0818\n",
      "step 3/3 | epoch 6/20 | batch 47/60 | global_step 6347 | loss_total 0.9918\n",
      "step 3/3 | epoch 6/20 | batch 48/60 | global_step 6348 | loss_total 0.6887\n",
      "step 3/3 | epoch 6/20 | batch 49/60 | global_step 6349 | loss_total 2.2124\n",
      "step 3/3 | epoch 6/20 | batch 50/60 | global_step 6350 | loss_total 0.5644\n",
      "step 3/3 | epoch 6/20 | batch 51/60 | global_step 6351 | loss_total 0.5805\n",
      "step 3/3 | epoch 6/20 | batch 52/60 | global_step 6352 | loss_total 0.5551\n",
      "step 3/3 | epoch 6/20 | batch 53/60 | global_step 6353 | loss_total 0.5670\n",
      "step 3/3 | epoch 6/20 | batch 54/60 | global_step 6354 | loss_total 1.0016\n",
      "step 3/3 | epoch 6/20 | batch 55/60 | global_step 6355 | loss_total 1.0454\n",
      "step 3/3 | epoch 6/20 | batch 56/60 | global_step 6356 | loss_total 0.5587\n",
      "step 3/3 | epoch 6/20 | batch 57/60 | global_step 6357 | loss_total 0.7904\n",
      "step 3/3 | epoch 6/20 | batch 58/60 | global_step 6358 | loss_total 0.5524\n",
      "step 3/3 | epoch 6/20 | batch 59/60 | global_step 6359 | loss_total 0.5514\n",
      "step 3/3 | epoch 6/20 | batch 60/60 | global_step 6360 | loss_total 0.8209\n",
      "[epoch done] step 3/3 epoch 6/20 | train_total=1.6008 val_total=0.7112\n",
      "step 3/3 | epoch 7/20 | batch 1/60 | global_step 6361 | loss_total 1.0592\n",
      "step 3/3 | epoch 7/20 | batch 2/60 | global_step 6362 | loss_total 1.1957\n",
      "step 3/3 | epoch 7/20 | batch 3/60 | global_step 6363 | loss_total 1.7730\n",
      "step 3/3 | epoch 7/20 | batch 4/60 | global_step 6364 | loss_total 0.5447\n",
      "step 3/3 | epoch 7/20 | batch 5/60 | global_step 6365 | loss_total 1.5666\n",
      "step 3/3 | epoch 7/20 | batch 6/60 | global_step 6366 | loss_total 1.3314\n",
      "step 3/3 | epoch 7/20 | batch 7/60 | global_step 6367 | loss_total 0.5337\n",
      "step 3/3 | epoch 7/20 | batch 8/60 | global_step 6368 | loss_total 1.7499\n",
      "step 3/3 | epoch 7/20 | batch 9/60 | global_step 6369 | loss_total 0.9889\n",
      "step 3/3 | epoch 7/20 | batch 10/60 | global_step 6370 | loss_total 1.9529\n",
      "step 3/3 | epoch 7/20 | batch 11/60 | global_step 6371 | loss_total 8.3937\n",
      "step 3/3 | epoch 7/20 | batch 12/60 | global_step 6372 | loss_total 2.9332\n",
      "step 3/3 | epoch 7/20 | batch 13/60 | global_step 6373 | loss_total 0.5070\n",
      "step 3/3 | epoch 7/20 | batch 14/60 | global_step 6374 | loss_total 0.5479\n",
      "step 3/3 | epoch 7/20 | batch 15/60 | global_step 6375 | loss_total 0.6109\n",
      "step 3/3 | epoch 7/20 | batch 16/60 | global_step 6376 | loss_total 1.4850\n",
      "step 3/3 | epoch 7/20 | batch 17/60 | global_step 6377 | loss_total 1.5488\n",
      "step 3/3 | epoch 7/20 | batch 18/60 | global_step 6378 | loss_total 0.5092\n",
      "step 3/3 | epoch 7/20 | batch 19/60 | global_step 6379 | loss_total 0.5505\n",
      "step 3/3 | epoch 7/20 | batch 20/60 | global_step 6380 | loss_total 0.9483\n",
      "step 3/3 | epoch 7/20 | batch 21/60 | global_step 6381 | loss_total 1.1926\n",
      "step 3/3 | epoch 7/20 | batch 22/60 | global_step 6382 | loss_total 0.8452\n",
      "step 3/3 | epoch 7/20 | batch 23/60 | global_step 6383 | loss_total 1.6042\n",
      "step 3/3 | epoch 7/20 | batch 24/60 | global_step 6384 | loss_total 1.0401\n",
      "step 3/3 | epoch 7/20 | batch 25/60 | global_step 6385 | loss_total 0.5329\n",
      "step 3/3 | epoch 7/20 | batch 26/60 | global_step 6386 | loss_total 0.5100\n",
      "step 3/3 | epoch 7/20 | batch 27/60 | global_step 6387 | loss_total 0.9952\n",
      "step 3/3 | epoch 7/20 | batch 28/60 | global_step 6388 | loss_total 1.0868\n",
      "step 3/3 | epoch 7/20 | batch 29/60 | global_step 6389 | loss_total 0.9675\n",
      "step 3/3 | epoch 7/20 | batch 30/60 | global_step 6390 | loss_total 2.3770\n",
      "step 3/3 | epoch 7/20 | batch 31/60 | global_step 6391 | loss_total 0.9144\n",
      "step 3/3 | epoch 7/20 | batch 32/60 | global_step 6392 | loss_total 0.5586\n",
      "step 3/3 | epoch 7/20 | batch 33/60 | global_step 6393 | loss_total 0.5464\n",
      "step 3/3 | epoch 7/20 | batch 34/60 | global_step 6394 | loss_total 0.5182\n",
      "step 3/3 | epoch 7/20 | batch 35/60 | global_step 6395 | loss_total 0.5068\n",
      "step 3/3 | epoch 7/20 | batch 36/60 | global_step 6396 | loss_total 0.5288\n",
      "step 3/3 | epoch 7/20 | batch 37/60 | global_step 6397 | loss_total 0.9841\n",
      "step 3/3 | epoch 7/20 | batch 38/60 | global_step 6398 | loss_total 0.9837\n",
      "step 3/3 | epoch 7/20 | batch 39/60 | global_step 6399 | loss_total 1.4705\n",
      "step 3/3 | epoch 7/20 | batch 40/60 | global_step 6400 | loss_total 0.5276\n",
      "step 3/3 | epoch 7/20 | batch 41/60 | global_step 6401 | loss_total 0.9178\n",
      "step 3/3 | epoch 7/20 | batch 42/60 | global_step 6402 | loss_total 1.5175\n",
      "step 3/3 | epoch 7/20 | batch 43/60 | global_step 6403 | loss_total 0.5222\n",
      "step 3/3 | epoch 7/20 | batch 44/60 | global_step 6404 | loss_total 1.3372\n",
      "step 3/3 | epoch 7/20 | batch 45/60 | global_step 6405 | loss_total 0.9040\n",
      "step 3/3 | epoch 7/20 | batch 46/60 | global_step 6406 | loss_total 1.8754\n",
      "step 3/3 | epoch 7/20 | batch 47/60 | global_step 6407 | loss_total 0.5834\n",
      "step 3/3 | epoch 7/20 | batch 48/60 | global_step 6408 | loss_total 1.0299\n",
      "step 3/3 | epoch 7/20 | batch 49/60 | global_step 6409 | loss_total 0.4726\n",
      "step 3/3 | epoch 7/20 | batch 50/60 | global_step 6410 | loss_total 1.2910\n",
      "step 3/3 | epoch 7/20 | batch 51/60 | global_step 6411 | loss_total 1.2956\n",
      "step 3/3 | epoch 7/20 | batch 52/60 | global_step 6412 | loss_total 0.8101\n",
      "step 3/3 | epoch 7/20 | batch 53/60 | global_step 6413 | loss_total 0.6311\n",
      "step 3/3 | epoch 7/20 | batch 54/60 | global_step 6414 | loss_total 1.0383\n",
      "step 3/3 | epoch 7/20 | batch 55/60 | global_step 6415 | loss_total 0.8830\n",
      "step 3/3 | epoch 7/20 | batch 56/60 | global_step 6416 | loss_total 0.6031\n",
      "step 3/3 | epoch 7/20 | batch 57/60 | global_step 6417 | loss_total 26.3673\n",
      "step 3/3 | epoch 7/20 | batch 58/60 | global_step 6418 | loss_total 0.4955\n",
      "step 3/3 | epoch 7/20 | batch 59/60 | global_step 6419 | loss_total 4.2745\n",
      "step 3/3 | epoch 7/20 | batch 60/60 | global_step 6420 | loss_total 0.4950\n",
      "[epoch done] step 3/3 epoch 7/20 | train_total=1.6128 val_total=0.7511\n",
      "step 3/3 | epoch 8/20 | batch 1/60 | global_step 6421 | loss_total 0.5058\n",
      "step 3/3 | epoch 8/20 | batch 2/60 | global_step 6422 | loss_total 0.8593\n",
      "step 3/3 | epoch 8/20 | batch 3/60 | global_step 6423 | loss_total 0.4937\n",
      "step 3/3 | epoch 8/20 | batch 4/60 | global_step 6424 | loss_total 1.2828\n",
      "step 3/3 | epoch 8/20 | batch 5/60 | global_step 6425 | loss_total 0.5393\n",
      "step 3/3 | epoch 8/20 | batch 6/60 | global_step 6426 | loss_total 1.0974\n",
      "step 3/3 | epoch 8/20 | batch 7/60 | global_step 6427 | loss_total 0.4792\n",
      "step 3/3 | epoch 8/20 | batch 8/60 | global_step 6428 | loss_total 14.3519\n",
      "step 3/3 | epoch 8/20 | batch 9/60 | global_step 6429 | loss_total 0.4758\n",
      "step 3/3 | epoch 8/20 | batch 10/60 | global_step 6430 | loss_total 0.5997\n",
      "step 3/3 | epoch 8/20 | batch 11/60 | global_step 6431 | loss_total 16.9761\n",
      "step 3/3 | epoch 8/20 | batch 12/60 | global_step 6432 | loss_total 1.3848\n",
      "step 3/3 | epoch 8/20 | batch 13/60 | global_step 6433 | loss_total 0.5170\n",
      "step 3/3 | epoch 8/20 | batch 14/60 | global_step 6434 | loss_total 1.2458\n",
      "step 3/3 | epoch 8/20 | batch 15/60 | global_step 6435 | loss_total 1.0479\n",
      "step 3/3 | epoch 8/20 | batch 16/60 | global_step 6436 | loss_total 0.8802\n",
      "step 3/3 | epoch 8/20 | batch 17/60 | global_step 6437 | loss_total 2.4397\n",
      "step 3/3 | epoch 8/20 | batch 18/60 | global_step 6438 | loss_total 2.6269\n",
      "step 3/3 | epoch 8/20 | batch 19/60 | global_step 6439 | loss_total 1.0973\n",
      "step 3/3 | epoch 8/20 | batch 20/60 | global_step 6440 | loss_total 1.0054\n",
      "step 3/3 | epoch 8/20 | batch 21/60 | global_step 6441 | loss_total 0.8896\n",
      "step 3/3 | epoch 8/20 | batch 22/60 | global_step 6442 | loss_total 0.5992\n",
      "step 3/3 | epoch 8/20 | batch 23/60 | global_step 6443 | loss_total 0.5277\n",
      "step 3/3 | epoch 8/20 | batch 24/60 | global_step 6444 | loss_total 0.6432\n",
      "step 3/3 | epoch 8/20 | batch 25/60 | global_step 6445 | loss_total 0.5588\n",
      "step 3/3 | epoch 8/20 | batch 26/60 | global_step 6446 | loss_total 0.5305\n",
      "step 3/3 | epoch 8/20 | batch 27/60 | global_step 6447 | loss_total 0.9237\n",
      "step 3/3 | epoch 8/20 | batch 28/60 | global_step 6448 | loss_total 0.5268\n",
      "step 3/3 | epoch 8/20 | batch 29/60 | global_step 6449 | loss_total 0.5250\n",
      "step 3/3 | epoch 8/20 | batch 30/60 | global_step 6450 | loss_total 1.0684\n",
      "step 3/3 | epoch 8/20 | batch 31/60 | global_step 6451 | loss_total 3.1067\n",
      "step 3/3 | epoch 8/20 | batch 32/60 | global_step 6452 | loss_total 0.5176\n",
      "step 3/3 | epoch 8/20 | batch 33/60 | global_step 6453 | loss_total 1.6296\n",
      "step 3/3 | epoch 8/20 | batch 34/60 | global_step 6454 | loss_total 0.7971\n",
      "step 3/3 | epoch 8/20 | batch 35/60 | global_step 6455 | loss_total 0.8360\n",
      "step 3/3 | epoch 8/20 | batch 36/60 | global_step 6456 | loss_total 0.6807\n",
      "step 3/3 | epoch 8/20 | batch 37/60 | global_step 6457 | loss_total 1.0812\n",
      "step 3/3 | epoch 8/20 | batch 38/60 | global_step 6458 | loss_total 0.5073\n",
      "step 3/3 | epoch 8/20 | batch 39/60 | global_step 6459 | loss_total 0.4935\n",
      "step 3/3 | epoch 8/20 | batch 40/60 | global_step 6460 | loss_total 1.6702\n",
      "step 3/3 | epoch 8/20 | batch 41/60 | global_step 6461 | loss_total 1.2542\n",
      "step 3/3 | epoch 8/20 | batch 42/60 | global_step 6462 | loss_total 12.8128\n",
      "step 3/3 | epoch 8/20 | batch 43/60 | global_step 6463 | loss_total 1.2906\n",
      "step 3/3 | epoch 8/20 | batch 44/60 | global_step 6464 | loss_total 0.5097\n",
      "step 3/3 | epoch 8/20 | batch 45/60 | global_step 6465 | loss_total 3.5569\n",
      "step 3/3 | epoch 8/20 | batch 46/60 | global_step 6466 | loss_total 1.2522\n",
      "step 3/3 | epoch 8/20 | batch 47/60 | global_step 6467 | loss_total 0.5327\n",
      "step 3/3 | epoch 8/20 | batch 48/60 | global_step 6468 | loss_total 0.5091\n",
      "step 3/3 | epoch 8/20 | batch 49/60 | global_step 6469 | loss_total 0.4839\n",
      "step 3/3 | epoch 8/20 | batch 50/60 | global_step 6470 | loss_total 6.8523\n",
      "step 3/3 | epoch 8/20 | batch 51/60 | global_step 6471 | loss_total 0.8191\n",
      "step 3/3 | epoch 8/20 | batch 52/60 | global_step 6472 | loss_total 0.5810\n",
      "step 3/3 | epoch 8/20 | batch 53/60 | global_step 6473 | loss_total 0.4804\n",
      "step 3/3 | epoch 8/20 | batch 54/60 | global_step 6474 | loss_total 0.7785\n",
      "step 3/3 | epoch 8/20 | batch 55/60 | global_step 6475 | loss_total 0.5058\n",
      "step 3/3 | epoch 8/20 | batch 56/60 | global_step 6476 | loss_total 0.5029\n",
      "step 3/3 | epoch 8/20 | batch 57/60 | global_step 6477 | loss_total 0.5284\n",
      "step 3/3 | epoch 8/20 | batch 58/60 | global_step 6478 | loss_total 6.3727\n",
      "step 3/3 | epoch 8/20 | batch 59/60 | global_step 6479 | loss_total 0.8052\n",
      "step 3/3 | epoch 8/20 | batch 60/60 | global_step 6480 | loss_total 0.5012\n",
      "[epoch done] step 3/3 epoch 8/20 | train_total=1.8158 val_total=0.6086\n",
      "step 3/3 | epoch 9/20 | batch 1/60 | global_step 6481 | loss_total 0.4986\n",
      "step 3/3 | epoch 9/20 | batch 2/60 | global_step 6482 | loss_total 1.2299\n",
      "step 3/3 | epoch 9/20 | batch 3/60 | global_step 6483 | loss_total 0.5717\n",
      "step 3/3 | epoch 9/20 | batch 4/60 | global_step 6484 | loss_total 1.1154\n",
      "step 3/3 | epoch 9/20 | batch 5/60 | global_step 6485 | loss_total 0.7765\n",
      "step 3/3 | epoch 9/20 | batch 6/60 | global_step 6486 | loss_total 0.8615\n",
      "step 3/3 | epoch 9/20 | batch 7/60 | global_step 6487 | loss_total 0.4800\n",
      "step 3/3 | epoch 9/20 | batch 8/60 | global_step 6488 | loss_total 2.1105\n",
      "step 3/3 | epoch 9/20 | batch 9/60 | global_step 6489 | loss_total 1.2314\n",
      "step 3/3 | epoch 9/20 | batch 10/60 | global_step 6490 | loss_total 2.9932\n",
      "step 3/3 | epoch 9/20 | batch 11/60 | global_step 6491 | loss_total 0.5261\n",
      "step 3/3 | epoch 9/20 | batch 12/60 | global_step 6492 | loss_total 0.4931\n",
      "step 3/3 | epoch 9/20 | batch 13/60 | global_step 6493 | loss_total 0.4867\n",
      "step 3/3 | epoch 9/20 | batch 14/60 | global_step 6494 | loss_total 1.1220\n",
      "step 3/3 | epoch 9/20 | batch 15/60 | global_step 6495 | loss_total 1.7236\n",
      "step 3/3 | epoch 9/20 | batch 16/60 | global_step 6496 | loss_total 0.4937\n",
      "step 3/3 | epoch 9/20 | batch 17/60 | global_step 6497 | loss_total 1.0593\n",
      "step 3/3 | epoch 9/20 | batch 18/60 | global_step 6498 | loss_total 0.4951\n",
      "step 3/3 | epoch 9/20 | batch 19/60 | global_step 6499 | loss_total 0.4960\n",
      "step 3/3 | epoch 9/20 | batch 20/60 | global_step 6500 | loss_total 0.4949\n",
      "step 3/3 | epoch 9/20 | batch 21/60 | global_step 6501 | loss_total 0.4947\n",
      "step 3/3 | epoch 9/20 | batch 22/60 | global_step 6502 | loss_total 1.0193\n",
      "step 3/3 | epoch 9/20 | batch 23/60 | global_step 6503 | loss_total 1.3335\n",
      "step 3/3 | epoch 9/20 | batch 24/60 | global_step 6504 | loss_total 1.0819\n",
      "step 3/3 | epoch 9/20 | batch 25/60 | global_step 6505 | loss_total 0.8928\n",
      "step 3/3 | epoch 9/20 | batch 26/60 | global_step 6506 | loss_total 0.8380\n",
      "step 3/3 | epoch 9/20 | batch 27/60 | global_step 6507 | loss_total 0.9633\n",
      "step 3/3 | epoch 9/20 | batch 28/60 | global_step 6508 | loss_total 0.4997\n",
      "step 3/3 | epoch 9/20 | batch 29/60 | global_step 6509 | loss_total 0.5170\n",
      "step 3/3 | epoch 9/20 | batch 30/60 | global_step 6510 | loss_total 0.9693\n",
      "step 3/3 | epoch 9/20 | batch 31/60 | global_step 6511 | loss_total 0.5517\n",
      "step 3/3 | epoch 9/20 | batch 32/60 | global_step 6512 | loss_total 0.9284\n",
      "step 3/3 | epoch 9/20 | batch 33/60 | global_step 6513 | loss_total 0.9527\n",
      "step 3/3 | epoch 9/20 | batch 34/60 | global_step 6514 | loss_total 0.9113\n",
      "step 3/3 | epoch 9/20 | batch 35/60 | global_step 6515 | loss_total 0.5434\n",
      "step 3/3 | epoch 9/20 | batch 36/60 | global_step 6516 | loss_total 1.0808\n",
      "step 3/3 | epoch 9/20 | batch 37/60 | global_step 6517 | loss_total 1.6037\n",
      "step 3/3 | epoch 9/20 | batch 38/60 | global_step 6518 | loss_total 0.8085\n",
      "step 3/3 | epoch 9/20 | batch 39/60 | global_step 6519 | loss_total 0.7375\n",
      "step 3/3 | epoch 9/20 | batch 40/60 | global_step 6520 | loss_total 0.9334\n",
      "step 3/3 | epoch 9/20 | batch 41/60 | global_step 6521 | loss_total 0.7528\n",
      "step 3/3 | epoch 9/20 | batch 42/60 | global_step 6522 | loss_total 1.2833\n",
      "step 3/3 | epoch 9/20 | batch 43/60 | global_step 6523 | loss_total 1.0551\n",
      "step 3/3 | epoch 9/20 | batch 44/60 | global_step 6524 | loss_total 0.5590\n",
      "step 3/3 | epoch 9/20 | batch 45/60 | global_step 6525 | loss_total 0.9520\n",
      "step 3/3 | epoch 9/20 | batch 46/60 | global_step 6526 | loss_total 0.7661\n",
      "step 3/3 | epoch 9/20 | batch 47/60 | global_step 6527 | loss_total 0.5232\n",
      "step 3/3 | epoch 9/20 | batch 48/60 | global_step 6528 | loss_total 1.0417\n",
      "step 3/3 | epoch 9/20 | batch 49/60 | global_step 6529 | loss_total 0.9247\n",
      "step 3/3 | epoch 9/20 | batch 50/60 | global_step 6530 | loss_total 0.6321\n",
      "step 3/3 | epoch 9/20 | batch 51/60 | global_step 6531 | loss_total 0.9422\n",
      "step 3/3 | epoch 9/20 | batch 52/60 | global_step 6532 | loss_total 0.5215\n",
      "step 3/3 | epoch 9/20 | batch 53/60 | global_step 6533 | loss_total 0.4935\n",
      "step 3/3 | epoch 9/20 | batch 54/60 | global_step 6534 | loss_total 0.8983\n",
      "step 3/3 | epoch 9/20 | batch 55/60 | global_step 6535 | loss_total 2.6838\n",
      "step 3/3 | epoch 9/20 | batch 56/60 | global_step 6536 | loss_total 0.8403\n",
      "step 3/3 | epoch 9/20 | batch 57/60 | global_step 6537 | loss_total 0.5346\n",
      "step 3/3 | epoch 9/20 | batch 58/60 | global_step 6538 | loss_total 2.7301\n",
      "step 3/3 | epoch 9/20 | batch 59/60 | global_step 6539 | loss_total 0.5058\n",
      "step 3/3 | epoch 9/20 | batch 60/60 | global_step 6540 | loss_total 0.8300\n",
      "[epoch done] step 3/3 epoch 9/20 | train_total=0.9398 val_total=0.6819\n",
      "step 3/3 | epoch 10/20 | batch 1/60 | global_step 6541 | loss_total 1.5052\n",
      "step 3/3 | epoch 10/20 | batch 2/60 | global_step 6542 | loss_total 1.6687\n",
      "step 3/3 | epoch 10/20 | batch 3/60 | global_step 6543 | loss_total 0.9254\n",
      "step 3/3 | epoch 10/20 | batch 4/60 | global_step 6544 | loss_total 0.8260\n",
      "step 3/3 | epoch 10/20 | batch 5/60 | global_step 6545 | loss_total 1.1482\n",
      "step 3/3 | epoch 10/20 | batch 6/60 | global_step 6546 | loss_total 0.5234\n",
      "step 3/3 | epoch 10/20 | batch 7/60 | global_step 6547 | loss_total 0.9187\n",
      "step 3/3 | epoch 10/20 | batch 8/60 | global_step 6548 | loss_total 0.7701\n",
      "step 3/3 | epoch 10/20 | batch 9/60 | global_step 6549 | loss_total 0.8842\n",
      "step 3/3 | epoch 10/20 | batch 10/60 | global_step 6550 | loss_total 0.6987\n",
      "step 3/3 | epoch 10/20 | batch 11/60 | global_step 6551 | loss_total 2.6883\n",
      "step 3/3 | epoch 10/20 | batch 12/60 | global_step 6552 | loss_total 0.8552\n",
      "step 3/3 | epoch 10/20 | batch 13/60 | global_step 6553 | loss_total 0.7896\n",
      "step 3/3 | epoch 10/20 | batch 14/60 | global_step 6554 | loss_total 1.9151\n",
      "step 3/3 | epoch 10/20 | batch 15/60 | global_step 6555 | loss_total 0.5015\n",
      "step 3/3 | epoch 10/20 | batch 16/60 | global_step 6556 | loss_total 0.5884\n",
      "step 3/3 | epoch 10/20 | batch 17/60 | global_step 6557 | loss_total 0.5083\n",
      "step 3/3 | epoch 10/20 | batch 18/60 | global_step 6558 | loss_total 0.9492\n",
      "step 3/3 | epoch 10/20 | batch 19/60 | global_step 6559 | loss_total 1.2271\n",
      "step 3/3 | epoch 10/20 | batch 20/60 | global_step 6560 | loss_total 0.4791\n",
      "step 3/3 | epoch 10/20 | batch 21/60 | global_step 6561 | loss_total 0.5762\n",
      "step 3/3 | epoch 10/20 | batch 22/60 | global_step 6562 | loss_total 1.1787\n",
      "step 3/3 | epoch 10/20 | batch 23/60 | global_step 6563 | loss_total 1.4130\n",
      "step 3/3 | epoch 10/20 | batch 24/60 | global_step 6564 | loss_total 0.9248\n",
      "step 3/3 | epoch 10/20 | batch 25/60 | global_step 6565 | loss_total 1.2895\n",
      "step 3/3 | epoch 10/20 | batch 26/60 | global_step 6566 | loss_total 0.7598\n",
      "step 3/3 | epoch 10/20 | batch 27/60 | global_step 6567 | loss_total 0.8181\n",
      "step 3/3 | epoch 10/20 | batch 28/60 | global_step 6568 | loss_total 7.5221\n",
      "step 3/3 | epoch 10/20 | batch 29/60 | global_step 6569 | loss_total 0.7672\n",
      "step 3/3 | epoch 10/20 | batch 30/60 | global_step 6570 | loss_total 0.4799\n",
      "step 3/3 | epoch 10/20 | batch 31/60 | global_step 6571 | loss_total 1.2462\n",
      "step 3/3 | epoch 10/20 | batch 32/60 | global_step 6572 | loss_total 0.5360\n",
      "step 3/3 | epoch 10/20 | batch 33/60 | global_step 6573 | loss_total 1.1774\n",
      "step 3/3 | epoch 10/20 | batch 34/60 | global_step 6574 | loss_total 0.8943\n",
      "step 3/3 | epoch 10/20 | batch 35/60 | global_step 6575 | loss_total 0.9731\n",
      "step 3/3 | epoch 10/20 | batch 36/60 | global_step 6576 | loss_total 0.5021\n",
      "step 3/3 | epoch 10/20 | batch 37/60 | global_step 6577 | loss_total 0.5050\n",
      "step 3/3 | epoch 10/20 | batch 38/60 | global_step 6578 | loss_total 0.5427\n",
      "step 3/3 | epoch 10/20 | batch 39/60 | global_step 6579 | loss_total 1.1883\n",
      "step 3/3 | epoch 10/20 | batch 40/60 | global_step 6580 | loss_total 2.5324\n",
      "step 3/3 | epoch 10/20 | batch 41/60 | global_step 6581 | loss_total 0.4645\n",
      "step 3/3 | epoch 10/20 | batch 42/60 | global_step 6582 | loss_total 0.5446\n",
      "step 3/3 | epoch 10/20 | batch 43/60 | global_step 6583 | loss_total 0.4919\n",
      "step 3/3 | epoch 10/20 | batch 44/60 | global_step 6584 | loss_total 0.7893\n",
      "step 3/3 | epoch 10/20 | batch 45/60 | global_step 6585 | loss_total 0.4935\n",
      "step 3/3 | epoch 10/20 | batch 46/60 | global_step 6586 | loss_total 1.4218\n",
      "step 3/3 | epoch 10/20 | batch 47/60 | global_step 6587 | loss_total 0.9078\n",
      "step 3/3 | epoch 10/20 | batch 48/60 | global_step 6588 | loss_total 1.7426\n",
      "step 3/3 | epoch 10/20 | batch 49/60 | global_step 6589 | loss_total 0.4295\n",
      "step 3/3 | epoch 10/20 | batch 50/60 | global_step 6590 | loss_total 0.9677\n",
      "step 3/3 | epoch 10/20 | batch 51/60 | global_step 6591 | loss_total 0.6391\n",
      "step 3/3 | epoch 10/20 | batch 52/60 | global_step 6592 | loss_total 0.4761\n",
      "step 3/3 | epoch 10/20 | batch 53/60 | global_step 6593 | loss_total 0.5426\n",
      "step 3/3 | epoch 10/20 | batch 54/60 | global_step 6594 | loss_total 1.0371\n",
      "step 3/3 | epoch 10/20 | batch 55/60 | global_step 6595 | loss_total 1.7412\n",
      "step 3/3 | epoch 10/20 | batch 56/60 | global_step 6596 | loss_total 0.7921\n",
      "step 3/3 | epoch 10/20 | batch 57/60 | global_step 6597 | loss_total 1.3202\n",
      "step 3/3 | epoch 10/20 | batch 58/60 | global_step 6598 | loss_total 0.9180\n",
      "step 3/3 | epoch 10/20 | batch 59/60 | global_step 6599 | loss_total 3.7795\n",
      "step 3/3 | epoch 10/20 | batch 60/60 | global_step 6600 | loss_total 5.2862\n",
      "[epoch done] step 3/3 epoch 10/20 | train_total=1.1830 val_total=0.6836\n",
      "step 3/3 | epoch 11/20 | batch 1/60 | global_step 6601 | loss_total 1.3742\n",
      "step 3/3 | epoch 11/20 | batch 2/60 | global_step 6602 | loss_total 0.4686\n",
      "step 3/3 | epoch 11/20 | batch 3/60 | global_step 6603 | loss_total 1.3743\n",
      "step 3/3 | epoch 11/20 | batch 4/60 | global_step 6604 | loss_total 1.1348\n",
      "step 3/3 | epoch 11/20 | batch 5/60 | global_step 6605 | loss_total 0.5261\n",
      "step 3/3 | epoch 11/20 | batch 6/60 | global_step 6606 | loss_total 2.4214\n",
      "step 3/3 | epoch 11/20 | batch 7/60 | global_step 6607 | loss_total 0.4739\n",
      "step 3/3 | epoch 11/20 | batch 8/60 | global_step 6608 | loss_total 3.4060\n",
      "step 3/3 | epoch 11/20 | batch 9/60 | global_step 6609 | loss_total 0.4489\n",
      "step 3/3 | epoch 11/20 | batch 10/60 | global_step 6610 | loss_total 0.4692\n",
      "step 3/3 | epoch 11/20 | batch 11/60 | global_step 6611 | loss_total 1.1075\n",
      "step 3/3 | epoch 11/20 | batch 12/60 | global_step 6612 | loss_total 0.9137\n",
      "step 3/3 | epoch 11/20 | batch 13/60 | global_step 6613 | loss_total 0.5292\n",
      "step 3/3 | epoch 11/20 | batch 14/60 | global_step 6614 | loss_total 0.8322\n",
      "step 3/3 | epoch 11/20 | batch 15/60 | global_step 6615 | loss_total 0.4723\n",
      "step 3/3 | epoch 11/20 | batch 16/60 | global_step 6616 | loss_total 0.4630\n",
      "step 3/3 | epoch 11/20 | batch 17/60 | global_step 6617 | loss_total 1.6410\n",
      "step 3/3 | epoch 11/20 | batch 18/60 | global_step 6618 | loss_total 0.9488\n",
      "step 3/3 | epoch 11/20 | batch 19/60 | global_step 6619 | loss_total 0.8287\n",
      "step 3/3 | epoch 11/20 | batch 20/60 | global_step 6620 | loss_total 0.5482\n",
      "step 3/3 | epoch 11/20 | batch 21/60 | global_step 6621 | loss_total 0.7848\n",
      "step 3/3 | epoch 11/20 | batch 22/60 | global_step 6622 | loss_total 1.5722\n",
      "step 3/3 | epoch 11/20 | batch 23/60 | global_step 6623 | loss_total 0.8198\n",
      "step 3/3 | epoch 11/20 | batch 24/60 | global_step 6624 | loss_total 0.8325\n",
      "step 3/3 | epoch 11/20 | batch 25/60 | global_step 6625 | loss_total 0.8917\n",
      "step 3/3 | epoch 11/20 | batch 26/60 | global_step 6626 | loss_total 1.1831\n",
      "step 3/3 | epoch 11/20 | batch 27/60 | global_step 6627 | loss_total 2.6803\n",
      "step 3/3 | epoch 11/20 | batch 28/60 | global_step 6628 | loss_total 0.7454\n",
      "step 3/3 | epoch 11/20 | batch 29/60 | global_step 6629 | loss_total 3.9002\n",
      "step 3/3 | epoch 11/20 | batch 30/60 | global_step 6630 | loss_total 1.0449\n",
      "step 3/3 | epoch 11/20 | batch 31/60 | global_step 6631 | loss_total 0.9843\n",
      "step 3/3 | epoch 11/20 | batch 32/60 | global_step 6632 | loss_total 0.7932\n",
      "step 3/3 | epoch 11/20 | batch 33/60 | global_step 6633 | loss_total 1.5763\n",
      "step 3/3 | epoch 11/20 | batch 34/60 | global_step 6634 | loss_total 0.6090\n",
      "step 3/3 | epoch 11/20 | batch 35/60 | global_step 6635 | loss_total 1.6819\n",
      "step 3/3 | epoch 11/20 | batch 36/60 | global_step 6636 | loss_total 0.6535\n",
      "step 3/3 | epoch 11/20 | batch 37/60 | global_step 6637 | loss_total 0.6493\n",
      "step 3/3 | epoch 11/20 | batch 38/60 | global_step 6638 | loss_total 0.5660\n",
      "step 3/3 | epoch 11/20 | batch 39/60 | global_step 6639 | loss_total 0.8220\n",
      "step 3/3 | epoch 11/20 | batch 40/60 | global_step 6640 | loss_total 0.7430\n",
      "step 3/3 | epoch 11/20 | batch 41/60 | global_step 6641 | loss_total 0.8767\n",
      "step 3/3 | epoch 11/20 | batch 42/60 | global_step 6642 | loss_total 0.8422\n",
      "step 3/3 | epoch 11/20 | batch 43/60 | global_step 6643 | loss_total 0.8344\n",
      "step 3/3 | epoch 11/20 | batch 44/60 | global_step 6644 | loss_total 0.5449\n",
      "step 3/3 | epoch 11/20 | batch 45/60 | global_step 6645 | loss_total 0.4878\n",
      "step 3/3 | epoch 11/20 | batch 46/60 | global_step 6646 | loss_total 0.8305\n",
      "step 3/3 | epoch 11/20 | batch 47/60 | global_step 6647 | loss_total 1.1591\n",
      "step 3/3 | epoch 11/20 | batch 48/60 | global_step 6648 | loss_total 1.1275\n",
      "step 3/3 | epoch 11/20 | batch 49/60 | global_step 6649 | loss_total 1.1722\n",
      "step 3/3 | epoch 11/20 | batch 50/60 | global_step 6650 | loss_total 0.9262\n",
      "step 3/3 | epoch 11/20 | batch 51/60 | global_step 6651 | loss_total 1.6677\n",
      "step 3/3 | epoch 11/20 | batch 52/60 | global_step 6652 | loss_total 0.8883\n",
      "step 3/3 | epoch 11/20 | batch 53/60 | global_step 6653 | loss_total 1.2389\n",
      "step 3/3 | epoch 11/20 | batch 54/60 | global_step 6654 | loss_total 0.5467\n",
      "step 3/3 | epoch 11/20 | batch 55/60 | global_step 6655 | loss_total 0.6159\n",
      "step 3/3 | epoch 11/20 | batch 56/60 | global_step 6656 | loss_total 0.5519\n",
      "step 3/3 | epoch 11/20 | batch 57/60 | global_step 6657 | loss_total 0.6874\n",
      "step 3/3 | epoch 11/20 | batch 58/60 | global_step 6658 | loss_total 1.6336\n",
      "step 3/3 | epoch 11/20 | batch 59/60 | global_step 6659 | loss_total 0.7824\n",
      "step 3/3 | epoch 11/20 | batch 60/60 | global_step 6660 | loss_total 1.1109\n",
      "[epoch done] step 3/3 epoch 11/20 | train_total=1.0407 val_total=0.7132\n",
      "step 3/3 | epoch 12/20 | batch 1/60 | global_step 6661 | loss_total 3.0950\n",
      "step 3/3 | epoch 12/20 | batch 2/60 | global_step 6662 | loss_total 0.5610\n",
      "step 3/3 | epoch 12/20 | batch 3/60 | global_step 6663 | loss_total 1.3287\n",
      "step 3/3 | epoch 12/20 | batch 4/60 | global_step 6664 | loss_total 10.1432\n",
      "step 3/3 | epoch 12/20 | batch 5/60 | global_step 6665 | loss_total 0.8750\n",
      "step 3/3 | epoch 12/20 | batch 6/60 | global_step 6666 | loss_total 0.5808\n",
      "step 3/3 | epoch 12/20 | batch 7/60 | global_step 6667 | loss_total 1.0719\n",
      "step 3/3 | epoch 12/20 | batch 8/60 | global_step 6668 | loss_total 1.3807\n",
      "step 3/3 | epoch 12/20 | batch 9/60 | global_step 6669 | loss_total 0.8234\n",
      "step 3/3 | epoch 12/20 | batch 10/60 | global_step 6670 | loss_total 0.9006\n",
      "step 3/3 | epoch 12/20 | batch 11/60 | global_step 6671 | loss_total 0.5443\n",
      "step 3/3 | epoch 12/20 | batch 12/60 | global_step 6672 | loss_total 0.5340\n",
      "step 3/3 | epoch 12/20 | batch 13/60 | global_step 6673 | loss_total 0.5407\n",
      "step 3/3 | epoch 12/20 | batch 14/60 | global_step 6674 | loss_total 0.5611\n",
      "step 3/3 | epoch 12/20 | batch 15/60 | global_step 6675 | loss_total 1.1096\n",
      "step 3/3 | epoch 12/20 | batch 16/60 | global_step 6676 | loss_total 2.8616\n",
      "step 3/3 | epoch 12/20 | batch 17/60 | global_step 6677 | loss_total 0.9526\n",
      "step 3/3 | epoch 12/20 | batch 18/60 | global_step 6678 | loss_total 1.1311\n",
      "step 3/3 | epoch 12/20 | batch 19/60 | global_step 6679 | loss_total 1.2042\n",
      "step 3/3 | epoch 12/20 | batch 20/60 | global_step 6680 | loss_total 0.7695\n",
      "step 3/3 | epoch 12/20 | batch 21/60 | global_step 6681 | loss_total 1.2875\n",
      "step 3/3 | epoch 12/20 | batch 22/60 | global_step 6682 | loss_total 1.2009\n",
      "step 3/3 | epoch 12/20 | batch 23/60 | global_step 6683 | loss_total 0.9893\n",
      "step 3/3 | epoch 12/20 | batch 24/60 | global_step 6684 | loss_total 1.1195\n",
      "step 3/3 | epoch 12/20 | batch 25/60 | global_step 6685 | loss_total 5.2736\n",
      "step 3/3 | epoch 12/20 | batch 26/60 | global_step 6686 | loss_total 1.1396\n",
      "step 3/3 | epoch 12/20 | batch 27/60 | global_step 6687 | loss_total 0.6553\n",
      "step 3/3 | epoch 12/20 | batch 28/60 | global_step 6688 | loss_total 1.5494\n",
      "step 3/3 | epoch 12/20 | batch 29/60 | global_step 6689 | loss_total 1.1885\n",
      "step 3/3 | epoch 12/20 | batch 30/60 | global_step 6690 | loss_total 0.7876\n",
      "step 3/3 | epoch 12/20 | batch 31/60 | global_step 6691 | loss_total 0.8372\n",
      "step 3/3 | epoch 12/20 | batch 32/60 | global_step 6692 | loss_total 1.0820\n",
      "step 3/3 | epoch 12/20 | batch 33/60 | global_step 6693 | loss_total 0.5671\n",
      "step 3/3 | epoch 12/20 | batch 34/60 | global_step 6694 | loss_total 1.7109\n",
      "step 3/3 | epoch 12/20 | batch 35/60 | global_step 6695 | loss_total 0.5463\n",
      "step 3/3 | epoch 12/20 | batch 36/60 | global_step 6696 | loss_total 0.5736\n",
      "step 3/3 | epoch 12/20 | batch 37/60 | global_step 6697 | loss_total 7.0213\n",
      "step 3/3 | epoch 12/20 | batch 38/60 | global_step 6698 | loss_total 0.5996\n",
      "step 3/3 | epoch 12/20 | batch 39/60 | global_step 6699 | loss_total 0.5671\n",
      "step 3/3 | epoch 12/20 | batch 40/60 | global_step 6700 | loss_total 0.5854\n",
      "step 3/3 | epoch 12/20 | batch 41/60 | global_step 6701 | loss_total 1.0476\n",
      "step 3/3 | epoch 12/20 | batch 42/60 | global_step 6702 | loss_total 0.9599\n",
      "step 3/3 | epoch 12/20 | batch 43/60 | global_step 6703 | loss_total 4.0481\n",
      "step 3/3 | epoch 12/20 | batch 44/60 | global_step 6704 | loss_total 1.2307\n",
      "step 3/3 | epoch 12/20 | batch 45/60 | global_step 6705 | loss_total 0.8256\n",
      "step 3/3 | epoch 12/20 | batch 46/60 | global_step 6706 | loss_total 2.9368\n",
      "step 3/3 | epoch 12/20 | batch 47/60 | global_step 6707 | loss_total 1.0369\n",
      "step 3/3 | epoch 12/20 | batch 48/60 | global_step 6708 | loss_total 1.0944\n",
      "step 3/3 | epoch 12/20 | batch 49/60 | global_step 6709 | loss_total 1.0104\n",
      "step 3/3 | epoch 12/20 | batch 50/60 | global_step 6710 | loss_total 0.8296\n",
      "step 3/3 | epoch 12/20 | batch 51/60 | global_step 6711 | loss_total 0.8315\n",
      "step 3/3 | epoch 12/20 | batch 52/60 | global_step 6712 | loss_total 0.6324\n",
      "step 3/3 | epoch 12/20 | batch 53/60 | global_step 6713 | loss_total 0.5649\n",
      "step 3/3 | epoch 12/20 | batch 54/60 | global_step 6714 | loss_total 0.5905\n",
      "step 3/3 | epoch 12/20 | batch 55/60 | global_step 6715 | loss_total 3.1678\n",
      "step 3/3 | epoch 12/20 | batch 56/60 | global_step 6716 | loss_total 0.8260\n",
      "step 3/3 | epoch 12/20 | batch 57/60 | global_step 6717 | loss_total 0.9438\n",
      "step 3/3 | epoch 12/20 | batch 58/60 | global_step 6718 | loss_total 0.5586\n",
      "step 3/3 | epoch 12/20 | batch 59/60 | global_step 6719 | loss_total 0.8293\n",
      "step 3/3 | epoch 12/20 | batch 60/60 | global_step 6720 | loss_total 0.5597\n",
      "[epoch done] step 3/3 epoch 12/20 | train_total=1.4129 val_total=0.7144\n",
      "step 3/3 | epoch 13/20 | batch 1/60 | global_step 6721 | loss_total 0.5626\n",
      "step 3/3 | epoch 13/20 | batch 2/60 | global_step 6722 | loss_total 0.5588\n",
      "step 3/3 | epoch 13/20 | batch 3/60 | global_step 6723 | loss_total 0.8164\n",
      "step 3/3 | epoch 13/20 | batch 4/60 | global_step 6724 | loss_total 1.5483\n",
      "step 3/3 | epoch 13/20 | batch 5/60 | global_step 6725 | loss_total 0.8425\n",
      "step 3/3 | epoch 13/20 | batch 6/60 | global_step 6726 | loss_total 0.7090\n",
      "step 3/3 | epoch 13/20 | batch 7/60 | global_step 6727 | loss_total 0.5511\n",
      "step 3/3 | epoch 13/20 | batch 8/60 | global_step 6728 | loss_total 0.8175\n",
      "step 3/3 | epoch 13/20 | batch 9/60 | global_step 6729 | loss_total 0.5613\n",
      "step 3/3 | epoch 13/20 | batch 10/60 | global_step 6730 | loss_total 0.5423\n",
      "step 3/3 | epoch 13/20 | batch 11/60 | global_step 6731 | loss_total 0.8153\n",
      "step 3/3 | epoch 13/20 | batch 12/60 | global_step 6732 | loss_total 0.5173\n",
      "step 3/3 | epoch 13/20 | batch 13/60 | global_step 6733 | loss_total 0.7635\n",
      "step 3/3 | epoch 13/20 | batch 14/60 | global_step 6734 | loss_total 0.8110\n",
      "step 3/3 | epoch 13/20 | batch 15/60 | global_step 6735 | loss_total 0.5097\n",
      "step 3/3 | epoch 13/20 | batch 16/60 | global_step 6736 | loss_total 0.5910\n",
      "step 3/3 | epoch 13/20 | batch 17/60 | global_step 6737 | loss_total 0.5463\n",
      "step 3/3 | epoch 13/20 | batch 18/60 | global_step 6738 | loss_total 0.6593\n",
      "step 3/3 | epoch 13/20 | batch 19/60 | global_step 6739 | loss_total 0.5356\n",
      "step 3/3 | epoch 13/20 | batch 20/60 | global_step 6740 | loss_total 0.7966\n",
      "step 3/3 | epoch 13/20 | batch 21/60 | global_step 6741 | loss_total 0.7928\n",
      "step 3/3 | epoch 13/20 | batch 22/60 | global_step 6742 | loss_total 0.4937\n",
      "step 3/3 | epoch 13/20 | batch 23/60 | global_step 6743 | loss_total 1.2175\n",
      "step 3/3 | epoch 13/20 | batch 24/60 | global_step 6744 | loss_total 0.5717\n",
      "step 3/3 | epoch 13/20 | batch 25/60 | global_step 6745 | loss_total 0.5277\n",
      "step 3/3 | epoch 13/20 | batch 26/60 | global_step 6746 | loss_total 0.7973\n",
      "step 3/3 | epoch 13/20 | batch 27/60 | global_step 6747 | loss_total 0.7830\n",
      "step 3/3 | epoch 13/20 | batch 28/60 | global_step 6748 | loss_total 0.4825\n",
      "step 3/3 | epoch 13/20 | batch 29/60 | global_step 6749 | loss_total 2.5267\n",
      "step 3/3 | epoch 13/20 | batch 30/60 | global_step 6750 | loss_total 0.4780\n",
      "step 3/3 | epoch 13/20 | batch 31/60 | global_step 6751 | loss_total 1.0043\n",
      "step 3/3 | epoch 13/20 | batch 32/60 | global_step 6752 | loss_total 2.0083\n",
      "step 3/3 | epoch 13/20 | batch 33/60 | global_step 6753 | loss_total 1.3507\n",
      "step 3/3 | epoch 13/20 | batch 34/60 | global_step 6754 | loss_total 0.5400\n",
      "step 3/3 | epoch 13/20 | batch 35/60 | global_step 6755 | loss_total 0.5751\n",
      "step 3/3 | epoch 13/20 | batch 36/60 | global_step 6756 | loss_total 0.4137\n",
      "step 3/3 | epoch 13/20 | batch 37/60 | global_step 6757 | loss_total 2.0692\n",
      "step 3/3 | epoch 13/20 | batch 38/60 | global_step 6758 | loss_total 0.8461\n",
      "step 3/3 | epoch 13/20 | batch 39/60 | global_step 6759 | loss_total 1.1769\n",
      "step 3/3 | epoch 13/20 | batch 40/60 | global_step 6760 | loss_total 0.8309\n",
      "step 3/3 | epoch 13/20 | batch 41/60 | global_step 6761 | loss_total 0.8453\n",
      "step 3/3 | epoch 13/20 | batch 42/60 | global_step 6762 | loss_total 13.9596\n",
      "step 3/3 | epoch 13/20 | batch 43/60 | global_step 6763 | loss_total 0.4666\n",
      "step 3/3 | epoch 13/20 | batch 44/60 | global_step 6764 | loss_total 0.5081\n",
      "step 3/3 | epoch 13/20 | batch 45/60 | global_step 6765 | loss_total 0.5149\n",
      "step 3/3 | epoch 13/20 | batch 46/60 | global_step 6766 | loss_total 0.5626\n",
      "step 3/3 | epoch 13/20 | batch 47/60 | global_step 6767 | loss_total 0.4672\n",
      "step 3/3 | epoch 13/20 | batch 48/60 | global_step 6768 | loss_total 0.5131\n",
      "step 3/3 | epoch 13/20 | batch 49/60 | global_step 6769 | loss_total 0.8279\n",
      "step 3/3 | epoch 13/20 | batch 50/60 | global_step 6770 | loss_total 0.7858\n",
      "step 3/3 | epoch 13/20 | batch 51/60 | global_step 6771 | loss_total 0.5114\n",
      "step 3/3 | epoch 13/20 | batch 52/60 | global_step 6772 | loss_total 0.4629\n",
      "step 3/3 | epoch 13/20 | batch 53/60 | global_step 6773 | loss_total 1.0094\n",
      "step 3/3 | epoch 13/20 | batch 54/60 | global_step 6774 | loss_total 0.8023\n",
      "step 3/3 | epoch 13/20 | batch 55/60 | global_step 6775 | loss_total 10.2525\n",
      "step 3/3 | epoch 13/20 | batch 56/60 | global_step 6776 | loss_total 0.4586\n",
      "step 3/3 | epoch 13/20 | batch 57/60 | global_step 6777 | loss_total 0.6416\n",
      "step 3/3 | epoch 13/20 | batch 58/60 | global_step 6778 | loss_total 0.7969\n",
      "step 3/3 | epoch 13/20 | batch 59/60 | global_step 6779 | loss_total 0.8092\n",
      "step 3/3 | epoch 13/20 | batch 60/60 | global_step 6780 | loss_total 0.9752\n",
      "[epoch done] step 3/3 epoch 13/20 | train_total=1.1619 val_total=0.7370\n",
      "step 3/3 | epoch 14/20 | batch 1/60 | global_step 6781 | loss_total 0.8988\n",
      "step 3/3 | epoch 14/20 | batch 2/60 | global_step 6782 | loss_total 1.0959\n",
      "step 3/3 | epoch 14/20 | batch 3/60 | global_step 6783 | loss_total 0.7314\n",
      "step 3/3 | epoch 14/20 | batch 4/60 | global_step 6784 | loss_total 0.7817\n",
      "step 3/3 | epoch 14/20 | batch 5/60 | global_step 6785 | loss_total 0.6241\n",
      "step 3/3 | epoch 14/20 | batch 6/60 | global_step 6786 | loss_total 1.1643\n",
      "step 3/3 | epoch 14/20 | batch 7/60 | global_step 6787 | loss_total 0.6063\n",
      "step 3/3 | epoch 14/20 | batch 8/60 | global_step 6788 | loss_total 0.6740\n",
      "step 3/3 | epoch 14/20 | batch 9/60 | global_step 6789 | loss_total 0.7590\n",
      "step 3/3 | epoch 14/20 | batch 10/60 | global_step 6790 | loss_total 0.8102\n",
      "step 3/3 | epoch 14/20 | batch 11/60 | global_step 6791 | loss_total 1.7133\n",
      "step 3/3 | epoch 14/20 | batch 12/60 | global_step 6792 | loss_total 0.8934\n",
      "step 3/3 | epoch 14/20 | batch 13/60 | global_step 6793 | loss_total 0.7259\n",
      "step 3/3 | epoch 14/20 | batch 14/60 | global_step 6794 | loss_total 1.1711\n",
      "step 3/3 | epoch 14/20 | batch 15/60 | global_step 6795 | loss_total 1.1518\n",
      "step 3/3 | epoch 14/20 | batch 16/60 | global_step 6796 | loss_total 1.1896\n",
      "step 3/3 | epoch 14/20 | batch 17/60 | global_step 6797 | loss_total 1.6714\n",
      "step 3/3 | epoch 14/20 | batch 18/60 | global_step 6798 | loss_total 0.5203\n",
      "step 3/3 | epoch 14/20 | batch 19/60 | global_step 6799 | loss_total 0.5513\n",
      "step 3/3 | epoch 14/20 | batch 20/60 | global_step 6800 | loss_total 0.5364\n",
      "step 3/3 | epoch 14/20 | batch 21/60 | global_step 6801 | loss_total 0.5343\n",
      "step 3/3 | epoch 14/20 | batch 22/60 | global_step 6802 | loss_total 1.2880\n",
      "step 3/3 | epoch 14/20 | batch 23/60 | global_step 6803 | loss_total 0.6580\n",
      "step 3/3 | epoch 14/20 | batch 24/60 | global_step 6804 | loss_total 0.5079\n",
      "step 3/3 | epoch 14/20 | batch 25/60 | global_step 6805 | loss_total 0.7976\n",
      "step 3/3 | epoch 14/20 | batch 26/60 | global_step 6806 | loss_total 1.1761\n",
      "step 3/3 | epoch 14/20 | batch 27/60 | global_step 6807 | loss_total 1.3922\n",
      "step 3/3 | epoch 14/20 | batch 28/60 | global_step 6808 | loss_total 1.0929\n",
      "step 3/3 | epoch 14/20 | batch 29/60 | global_step 6809 | loss_total 0.6248\n",
      "step 3/3 | epoch 14/20 | batch 30/60 | global_step 6810 | loss_total 0.7413\n",
      "step 3/3 | epoch 14/20 | batch 31/60 | global_step 6811 | loss_total 0.5902\n",
      "step 3/3 | epoch 14/20 | batch 32/60 | global_step 6812 | loss_total 0.5373\n",
      "step 3/3 | epoch 14/20 | batch 33/60 | global_step 6813 | loss_total 0.8829\n",
      "step 3/3 | epoch 14/20 | batch 34/60 | global_step 6814 | loss_total 0.5356\n",
      "step 3/3 | epoch 14/20 | batch 35/60 | global_step 6815 | loss_total 0.4929\n",
      "step 3/3 | epoch 14/20 | batch 36/60 | global_step 6816 | loss_total 0.5978\n",
      "step 3/3 | epoch 14/20 | batch 37/60 | global_step 6817 | loss_total 0.5347\n",
      "step 3/3 | epoch 14/20 | batch 38/60 | global_step 6818 | loss_total 0.7206\n",
      "step 3/3 | epoch 14/20 | batch 39/60 | global_step 6819 | loss_total 0.5832\n",
      "step 3/3 | epoch 14/20 | batch 40/60 | global_step 6820 | loss_total 0.5288\n",
      "step 3/3 | epoch 14/20 | batch 41/60 | global_step 6821 | loss_total 0.5732\n",
      "step 3/3 | epoch 14/20 | batch 42/60 | global_step 6822 | loss_total 0.5656\n",
      "step 3/3 | epoch 14/20 | batch 43/60 | global_step 6823 | loss_total 0.5071\n",
      "step 3/3 | epoch 14/20 | batch 44/60 | global_step 6824 | loss_total 14.7476\n",
      "step 3/3 | epoch 14/20 | batch 45/60 | global_step 6825 | loss_total 1.2625\n",
      "step 3/3 | epoch 14/20 | batch 46/60 | global_step 6826 | loss_total 0.9205\n",
      "step 3/3 | epoch 14/20 | batch 47/60 | global_step 6827 | loss_total 1.3207\n",
      "step 3/3 | epoch 14/20 | batch 48/60 | global_step 6828 | loss_total 0.8444\n",
      "step 3/3 | epoch 14/20 | batch 49/60 | global_step 6829 | loss_total 1.3754\n",
      "step 3/3 | epoch 14/20 | batch 50/60 | global_step 6830 | loss_total 0.5083\n",
      "step 3/3 | epoch 14/20 | batch 51/60 | global_step 6831 | loss_total 0.5107\n",
      "step 3/3 | epoch 14/20 | batch 52/60 | global_step 6832 | loss_total 0.5111\n",
      "step 3/3 | epoch 14/20 | batch 53/60 | global_step 6833 | loss_total 0.5108\n",
      "step 3/3 | epoch 14/20 | batch 54/60 | global_step 6834 | loss_total 0.5017\n",
      "step 3/3 | epoch 14/20 | batch 55/60 | global_step 6835 | loss_total 0.8328\n",
      "step 3/3 | epoch 14/20 | batch 56/60 | global_step 6836 | loss_total 0.8245\n",
      "step 3/3 | epoch 14/20 | batch 57/60 | global_step 6837 | loss_total 0.8229\n",
      "step 3/3 | epoch 14/20 | batch 58/60 | global_step 6838 | loss_total 0.9887\n",
      "step 3/3 | epoch 14/20 | batch 59/60 | global_step 6839 | loss_total 0.4963\n",
      "step 3/3 | epoch 14/20 | batch 60/60 | global_step 6840 | loss_total 0.8348\n",
      "[epoch done] step 3/3 epoch 14/20 | train_total=1.0424 val_total=0.5955\n",
      "[phase3] hard negatives added: base=2703 hard_unique=565 repeat=2 total_neg=3833\n",
      "[phase3] epoch 15/20 hard_negatives=ON train_chunks=1708\n",
      "step 3/3 | epoch 15/20 | batch 1/60 | global_step 6841 | loss_total 7.3958\n",
      "step 3/3 | epoch 15/20 | batch 2/60 | global_step 6842 | loss_total 1.0867\n",
      "step 3/3 | epoch 15/20 | batch 3/60 | global_step 6843 | loss_total 2.3031\n",
      "step 3/3 | epoch 15/20 | batch 4/60 | global_step 6844 | loss_total 0.5053\n",
      "step 3/3 | epoch 15/20 | batch 5/60 | global_step 6845 | loss_total 1.0640\n",
      "step 3/3 | epoch 15/20 | batch 6/60 | global_step 6846 | loss_total 0.4924\n",
      "step 3/3 | epoch 15/20 | batch 7/60 | global_step 6847 | loss_total 1.0558\n",
      "step 3/3 | epoch 15/20 | batch 8/60 | global_step 6848 | loss_total 1.8835\n",
      "step 3/3 | epoch 15/20 | batch 9/60 | global_step 6849 | loss_total 0.6064\n",
      "step 3/3 | epoch 15/20 | batch 10/60 | global_step 6850 | loss_total 2.0336\n",
      "step 3/3 | epoch 15/20 | batch 11/60 | global_step 6851 | loss_total 0.4961\n",
      "step 3/3 | epoch 15/20 | batch 12/60 | global_step 6852 | loss_total 0.4913\n",
      "step 3/3 | epoch 15/20 | batch 13/60 | global_step 6853 | loss_total 1.0868\n",
      "step 3/3 | epoch 15/20 | batch 14/60 | global_step 6854 | loss_total 0.8026\n",
      "step 3/3 | epoch 15/20 | batch 15/60 | global_step 6855 | loss_total 1.3375\n",
      "step 3/3 | epoch 15/20 | batch 16/60 | global_step 6856 | loss_total 1.2798\n",
      "step 3/3 | epoch 15/20 | batch 17/60 | global_step 6857 | loss_total 2.1884\n",
      "step 3/3 | epoch 15/20 | batch 18/60 | global_step 6858 | loss_total 0.8756\n",
      "step 3/3 | epoch 15/20 | batch 19/60 | global_step 6859 | loss_total 1.3815\n",
      "step 3/3 | epoch 15/20 | batch 20/60 | global_step 6860 | loss_total 0.9425\n",
      "step 3/3 | epoch 15/20 | batch 21/60 | global_step 6861 | loss_total 0.5205\n",
      "step 3/3 | epoch 15/20 | batch 22/60 | global_step 6862 | loss_total 0.5329\n",
      "step 3/3 | epoch 15/20 | batch 23/60 | global_step 6863 | loss_total 1.4046\n",
      "step 3/3 | epoch 15/20 | batch 24/60 | global_step 6864 | loss_total 0.9154\n",
      "step 3/3 | epoch 15/20 | batch 25/60 | global_step 6865 | loss_total 0.8324\n",
      "step 3/3 | epoch 15/20 | batch 26/60 | global_step 6866 | loss_total 0.5438\n",
      "step 3/3 | epoch 15/20 | batch 27/60 | global_step 6867 | loss_total 0.4825\n",
      "step 3/3 | epoch 15/20 | batch 28/60 | global_step 6868 | loss_total 0.5400\n",
      "step 3/3 | epoch 15/20 | batch 29/60 | global_step 6869 | loss_total 0.7990\n",
      "step 3/3 | epoch 15/20 | batch 30/60 | global_step 6870 | loss_total 0.5154\n",
      "step 3/3 | epoch 15/20 | batch 31/60 | global_step 6871 | loss_total 0.8999\n",
      "step 3/3 | epoch 15/20 | batch 32/60 | global_step 6872 | loss_total 0.6267\n",
      "step 3/3 | epoch 15/20 | batch 33/60 | global_step 6873 | loss_total 1.1967\n",
      "step 3/3 | epoch 15/20 | batch 34/60 | global_step 6874 | loss_total 0.6260\n",
      "step 3/3 | epoch 15/20 | batch 35/60 | global_step 6875 | loss_total 1.0830\n",
      "step 3/3 | epoch 15/20 | batch 36/60 | global_step 6876 | loss_total 0.5208\n",
      "step 3/3 | epoch 15/20 | batch 37/60 | global_step 6877 | loss_total 2.4148\n",
      "step 3/3 | epoch 15/20 | batch 38/60 | global_step 6878 | loss_total 0.5146\n",
      "step 3/3 | epoch 15/20 | batch 39/60 | global_step 6879 | loss_total 0.7851\n",
      "step 3/3 | epoch 15/20 | batch 40/60 | global_step 6880 | loss_total 1.4046\n",
      "step 3/3 | epoch 15/20 | batch 41/60 | global_step 6881 | loss_total 0.8698\n",
      "step 3/3 | epoch 15/20 | batch 42/60 | global_step 6882 | loss_total 1.1763\n",
      "step 3/3 | epoch 15/20 | batch 43/60 | global_step 6883 | loss_total 0.5278\n",
      "step 3/3 | epoch 15/20 | batch 44/60 | global_step 6884 | loss_total 0.9837\n",
      "step 3/3 | epoch 15/20 | batch 45/60 | global_step 6885 | loss_total 1.2597\n",
      "step 3/3 | epoch 15/20 | batch 46/60 | global_step 6886 | loss_total 0.7779\n",
      "step 3/3 | epoch 15/20 | batch 47/60 | global_step 6887 | loss_total 0.5531\n",
      "step 3/3 | epoch 15/20 | batch 48/60 | global_step 6888 | loss_total 0.4903\n",
      "step 3/3 | epoch 15/20 | batch 49/60 | global_step 6889 | loss_total 0.5467\n",
      "step 3/3 | epoch 15/20 | batch 50/60 | global_step 6890 | loss_total 1.1458\n",
      "step 3/3 | epoch 15/20 | batch 51/60 | global_step 6891 | loss_total 0.4890\n",
      "step 3/3 | epoch 15/20 | batch 52/60 | global_step 6892 | loss_total 1.3335\n",
      "step 3/3 | epoch 15/20 | batch 53/60 | global_step 6893 | loss_total 0.5152\n",
      "step 3/3 | epoch 15/20 | batch 54/60 | global_step 6894 | loss_total 0.5147\n",
      "step 3/3 | epoch 15/20 | batch 55/60 | global_step 6895 | loss_total 0.8582\n",
      "step 3/3 | epoch 15/20 | batch 56/60 | global_step 6896 | loss_total 1.1244\n",
      "step 3/3 | epoch 15/20 | batch 57/60 | global_step 6897 | loss_total 0.8910\n",
      "step 3/3 | epoch 15/20 | batch 58/60 | global_step 6898 | loss_total 1.3051\n",
      "step 3/3 | epoch 15/20 | batch 59/60 | global_step 6899 | loss_total 0.8673\n",
      "step 3/3 | epoch 15/20 | batch 60/60 | global_step 6900 | loss_total 0.5769\n",
      "[epoch done] step 3/3 epoch 15/20 | train_total=1.0546 val_total=0.6892\n",
      "step 3/3 | epoch 16/20 | batch 1/60 | global_step 6901 | loss_total 0.6824\n",
      "step 3/3 | epoch 16/20 | batch 2/60 | global_step 6902 | loss_total 0.7015\n",
      "step 3/3 | epoch 16/20 | batch 3/60 | global_step 6903 | loss_total 0.8455\n",
      "step 3/3 | epoch 16/20 | batch 4/60 | global_step 6904 | loss_total 0.8340\n",
      "step 3/3 | epoch 16/20 | batch 5/60 | global_step 6905 | loss_total 0.5049\n",
      "step 3/3 | epoch 16/20 | batch 6/60 | global_step 6906 | loss_total 1.1207\n",
      "step 3/3 | epoch 16/20 | batch 7/60 | global_step 6907 | loss_total 1.3478\n",
      "step 3/3 | epoch 16/20 | batch 8/60 | global_step 6908 | loss_total 0.5325\n",
      "step 3/3 | epoch 16/20 | batch 9/60 | global_step 6909 | loss_total 0.7887\n",
      "step 3/3 | epoch 16/20 | batch 10/60 | global_step 6910 | loss_total 1.1158\n",
      "step 3/3 | epoch 16/20 | batch 11/60 | global_step 6911 | loss_total 0.8072\n",
      "step 3/3 | epoch 16/20 | batch 12/60 | global_step 6912 | loss_total 0.5156\n",
      "step 3/3 | epoch 16/20 | batch 13/60 | global_step 6913 | loss_total 0.7838\n",
      "step 3/3 | epoch 16/20 | batch 14/60 | global_step 6914 | loss_total 0.8214\n",
      "step 3/3 | epoch 16/20 | batch 15/60 | global_step 6915 | loss_total 0.5163\n",
      "step 3/3 | epoch 16/20 | batch 16/60 | global_step 6916 | loss_total 0.5110\n",
      "step 3/3 | epoch 16/20 | batch 17/60 | global_step 6917 | loss_total 1.2524\n",
      "step 3/3 | epoch 16/20 | batch 18/60 | global_step 6918 | loss_total 0.5665\n",
      "step 3/3 | epoch 16/20 | batch 19/60 | global_step 6919 | loss_total 0.8627\n",
      "step 3/3 | epoch 16/20 | batch 20/60 | global_step 6920 | loss_total 0.5281\n",
      "step 3/3 | epoch 16/20 | batch 21/60 | global_step 6921 | loss_total 0.9276\n",
      "step 3/3 | epoch 16/20 | batch 22/60 | global_step 6922 | loss_total 0.9931\n",
      "step 3/3 | epoch 16/20 | batch 23/60 | global_step 6923 | loss_total 1.6106\n",
      "step 3/3 | epoch 16/20 | batch 24/60 | global_step 6924 | loss_total 0.5827\n",
      "step 3/3 | epoch 16/20 | batch 25/60 | global_step 6925 | loss_total 0.5436\n",
      "step 3/3 | epoch 16/20 | batch 26/60 | global_step 6926 | loss_total 1.4673\n",
      "step 3/3 | epoch 16/20 | batch 27/60 | global_step 6927 | loss_total 0.5144\n",
      "step 3/3 | epoch 16/20 | batch 28/60 | global_step 6928 | loss_total 0.6646\n",
      "step 3/3 | epoch 16/20 | batch 29/60 | global_step 6929 | loss_total 16.5119\n",
      "step 3/3 | epoch 16/20 | batch 30/60 | global_step 6930 | loss_total 1.4877\n",
      "step 3/3 | epoch 16/20 | batch 31/60 | global_step 6931 | loss_total 1.0633\n",
      "step 3/3 | epoch 16/20 | batch 32/60 | global_step 6932 | loss_total 0.5419\n",
      "step 3/3 | epoch 16/20 | batch 33/60 | global_step 6933 | loss_total 0.5409\n",
      "step 3/3 | epoch 16/20 | batch 34/60 | global_step 6934 | loss_total 0.9630\n",
      "step 3/3 | epoch 16/20 | batch 35/60 | global_step 6935 | loss_total 0.6169\n",
      "step 3/3 | epoch 16/20 | batch 36/60 | global_step 6936 | loss_total 0.9473\n",
      "step 3/3 | epoch 16/20 | batch 37/60 | global_step 6937 | loss_total 1.3975\n",
      "step 3/3 | epoch 16/20 | batch 38/60 | global_step 6938 | loss_total 0.5183\n",
      "step 3/3 | epoch 16/20 | batch 39/60 | global_step 6939 | loss_total 0.5076\n",
      "step 3/3 | epoch 16/20 | batch 40/60 | global_step 6940 | loss_total 0.9655\n",
      "step 3/3 | epoch 16/20 | batch 41/60 | global_step 6941 | loss_total 1.4873\n",
      "step 3/3 | epoch 16/20 | batch 42/60 | global_step 6942 | loss_total 0.5138\n",
      "step 3/3 | epoch 16/20 | batch 43/60 | global_step 6943 | loss_total 1.1591\n",
      "step 3/3 | epoch 16/20 | batch 44/60 | global_step 6944 | loss_total 1.7084\n",
      "step 3/3 | epoch 16/20 | batch 45/60 | global_step 6945 | loss_total 1.3434\n",
      "step 3/3 | epoch 16/20 | batch 46/60 | global_step 6946 | loss_total 0.5289\n",
      "step 3/3 | epoch 16/20 | batch 47/60 | global_step 6947 | loss_total 0.5351\n",
      "step 3/3 | epoch 16/20 | batch 48/60 | global_step 6948 | loss_total 2.6310\n",
      "step 3/3 | epoch 16/20 | batch 49/60 | global_step 6949 | loss_total 0.5228\n",
      "step 3/3 | epoch 16/20 | batch 50/60 | global_step 6950 | loss_total 1.2543\n",
      "step 3/3 | epoch 16/20 | batch 51/60 | global_step 6951 | loss_total 1.0503\n",
      "step 3/3 | epoch 16/20 | batch 52/60 | global_step 6952 | loss_total 1.4310\n",
      "step 3/3 | epoch 16/20 | batch 53/60 | global_step 6953 | loss_total 0.9179\n",
      "step 3/3 | epoch 16/20 | batch 54/60 | global_step 6954 | loss_total 0.5432\n",
      "step 3/3 | epoch 16/20 | batch 55/60 | global_step 6955 | loss_total 1.5574\n",
      "step 3/3 | epoch 16/20 | batch 56/60 | global_step 6956 | loss_total 0.8065\n",
      "step 3/3 | epoch 16/20 | batch 57/60 | global_step 6957 | loss_total 0.8647\n",
      "step 3/3 | epoch 16/20 | batch 58/60 | global_step 6958 | loss_total 0.8123\n",
      "step 3/3 | epoch 16/20 | batch 59/60 | global_step 6959 | loss_total 0.5939\n",
      "step 3/3 | epoch 16/20 | batch 60/60 | global_step 6960 | loss_total 1.1427\n",
      "[epoch done] step 3/3 epoch 16/20 | train_total=1.1735 val_total=0.7264\n",
      "step 3/3 | epoch 17/20 | batch 1/60 | global_step 6961 | loss_total 0.5147\n",
      "step 3/3 | epoch 17/20 | batch 2/60 | global_step 6962 | loss_total 0.8973\n",
      "step 3/3 | epoch 17/20 | batch 3/60 | global_step 6963 | loss_total 0.5008\n",
      "step 3/3 | epoch 17/20 | batch 4/60 | global_step 6964 | loss_total 0.5478\n",
      "step 3/3 | epoch 17/20 | batch 5/60 | global_step 6965 | loss_total 0.5748\n",
      "step 3/3 | epoch 17/20 | batch 6/60 | global_step 6966 | loss_total 1.3480\n",
      "step 3/3 | epoch 17/20 | batch 7/60 | global_step 6967 | loss_total 1.3216\n",
      "step 3/3 | epoch 17/20 | batch 8/60 | global_step 6968 | loss_total 0.5193\n",
      "step 3/3 | epoch 17/20 | batch 9/60 | global_step 6969 | loss_total 0.4901\n",
      "step 3/3 | epoch 17/20 | batch 10/60 | global_step 6970 | loss_total 0.9549\n",
      "step 3/3 | epoch 17/20 | batch 11/60 | global_step 6971 | loss_total 0.8022\n",
      "step 3/3 | epoch 17/20 | batch 12/60 | global_step 6972 | loss_total 0.7863\n",
      "step 3/3 | epoch 17/20 | batch 13/60 | global_step 6973 | loss_total 1.0116\n",
      "step 3/3 | epoch 17/20 | batch 14/60 | global_step 6974 | loss_total 0.9278\n",
      "step 3/3 | epoch 17/20 | batch 15/60 | global_step 6975 | loss_total 0.9551\n",
      "step 3/3 | epoch 17/20 | batch 16/60 | global_step 6976 | loss_total 1.0354\n",
      "step 3/3 | epoch 17/20 | batch 17/60 | global_step 6977 | loss_total 1.3151\n",
      "step 3/3 | epoch 17/20 | batch 18/60 | global_step 6978 | loss_total 1.0098\n",
      "step 3/3 | epoch 17/20 | batch 19/60 | global_step 6979 | loss_total 0.9930\n",
      "step 3/3 | epoch 17/20 | batch 20/60 | global_step 6980 | loss_total 0.6647\n",
      "step 3/3 | epoch 17/20 | batch 21/60 | global_step 6981 | loss_total 1.1234\n",
      "step 3/3 | epoch 17/20 | batch 22/60 | global_step 6982 | loss_total 0.7875\n",
      "step 3/3 | epoch 17/20 | batch 23/60 | global_step 6983 | loss_total 1.2951\n",
      "step 3/3 | epoch 17/20 | batch 24/60 | global_step 6984 | loss_total 0.8591\n",
      "step 3/3 | epoch 17/20 | batch 25/60 | global_step 6985 | loss_total 0.7894\n",
      "step 3/3 | epoch 17/20 | batch 26/60 | global_step 6986 | loss_total 1.1001\n",
      "step 3/3 | epoch 17/20 | batch 27/60 | global_step 6987 | loss_total 0.8931\n",
      "step 3/3 | epoch 17/20 | batch 28/60 | global_step 6988 | loss_total 0.5472\n",
      "step 3/3 | epoch 17/20 | batch 29/60 | global_step 6989 | loss_total 0.5684\n",
      "step 3/3 | epoch 17/20 | batch 30/60 | global_step 6990 | loss_total 0.5612\n",
      "step 3/3 | epoch 17/20 | batch 31/60 | global_step 6991 | loss_total 1.4402\n",
      "step 3/3 | epoch 17/20 | batch 32/60 | global_step 6992 | loss_total 0.8579\n",
      "step 3/3 | epoch 17/20 | batch 33/60 | global_step 6993 | loss_total 0.7964\n",
      "step 3/3 | epoch 17/20 | batch 34/60 | global_step 6994 | loss_total 0.5290\n",
      "step 3/3 | epoch 17/20 | batch 35/60 | global_step 6995 | loss_total 0.8418\n",
      "step 3/3 | epoch 17/20 | batch 36/60 | global_step 6996 | loss_total 1.1330\n",
      "step 3/3 | epoch 17/20 | batch 37/60 | global_step 6997 | loss_total 0.8236\n",
      "step 3/3 | epoch 17/20 | batch 38/60 | global_step 6998 | loss_total 0.7759\n",
      "step 3/3 | epoch 17/20 | batch 39/60 | global_step 6999 | loss_total 0.8716\n",
      "step 3/3 | epoch 17/20 | batch 40/60 | global_step 7000 | loss_total 1.2413\n",
      "step 3/3 | epoch 17/20 | batch 41/60 | global_step 7001 | loss_total 1.1600\n",
      "step 3/3 | epoch 17/20 | batch 42/60 | global_step 7002 | loss_total 0.5613\n",
      "step 3/3 | epoch 17/20 | batch 43/60 | global_step 7003 | loss_total 0.9504\n",
      "step 3/3 | epoch 17/20 | batch 44/60 | global_step 7004 | loss_total 0.9413\n",
      "step 3/3 | epoch 17/20 | batch 45/60 | global_step 7005 | loss_total 0.5443\n",
      "step 3/3 | epoch 17/20 | batch 46/60 | global_step 7006 | loss_total 1.4553\n",
      "step 3/3 | epoch 17/20 | batch 47/60 | global_step 7007 | loss_total 1.2274\n",
      "step 3/3 | epoch 17/20 | batch 48/60 | global_step 7008 | loss_total 1.0320\n",
      "step 3/3 | epoch 17/20 | batch 49/60 | global_step 7009 | loss_total 0.8226\n",
      "step 3/3 | epoch 17/20 | batch 50/60 | global_step 7010 | loss_total 0.7053\n",
      "step 3/3 | epoch 17/20 | batch 51/60 | global_step 7011 | loss_total 0.8324\n",
      "step 3/3 | epoch 17/20 | batch 52/60 | global_step 7012 | loss_total 2.1058\n",
      "step 3/3 | epoch 17/20 | batch 53/60 | global_step 7013 | loss_total 0.8324\n",
      "step 3/3 | epoch 17/20 | batch 54/60 | global_step 7014 | loss_total 1.1345\n",
      "step 3/3 | epoch 17/20 | batch 55/60 | global_step 7015 | loss_total 0.8725\n",
      "step 3/3 | epoch 17/20 | batch 56/60 | global_step 7016 | loss_total 1.0267\n",
      "step 3/3 | epoch 17/20 | batch 57/60 | global_step 7017 | loss_total 1.2237\n",
      "step 3/3 | epoch 17/20 | batch 58/60 | global_step 7018 | loss_total 1.1208\n",
      "step 3/3 | epoch 17/20 | batch 59/60 | global_step 7019 | loss_total 0.5645\n",
      "step 3/3 | epoch 17/20 | batch 60/60 | global_step 7020 | loss_total 0.5598\n",
      "[epoch done] step 3/3 epoch 17/20 | train_total=0.9113 val_total=0.7095\n",
      "step 3/3 | epoch 18/20 | batch 1/60 | global_step 7021 | loss_total 0.6364\n",
      "step 3/3 | epoch 18/20 | batch 2/60 | global_step 7022 | loss_total 0.8314\n",
      "step 3/3 | epoch 18/20 | batch 3/60 | global_step 7023 | loss_total 0.8942\n",
      "step 3/3 | epoch 18/20 | batch 4/60 | global_step 7024 | loss_total 1.1992\n",
      "step 3/3 | epoch 18/20 | batch 5/60 | global_step 7025 | loss_total 0.7925\n",
      "step 3/3 | epoch 18/20 | batch 6/60 | global_step 7026 | loss_total 1.9660\n",
      "step 3/3 | epoch 18/20 | batch 7/60 | global_step 7027 | loss_total 2.5682\n",
      "step 3/3 | epoch 18/20 | batch 8/60 | global_step 7028 | loss_total 0.9889\n",
      "step 3/3 | epoch 18/20 | batch 9/60 | global_step 7029 | loss_total 0.5806\n",
      "step 3/3 | epoch 18/20 | batch 10/60 | global_step 7030 | loss_total 0.8481\n",
      "step 3/3 | epoch 18/20 | batch 11/60 | global_step 7031 | loss_total 0.7970\n",
      "step 3/3 | epoch 18/20 | batch 12/60 | global_step 7032 | loss_total 1.1625\n",
      "step 3/3 | epoch 18/20 | batch 13/60 | global_step 7033 | loss_total 0.7939\n",
      "step 3/3 | epoch 18/20 | batch 14/60 | global_step 7034 | loss_total 0.9989\n",
      "step 3/3 | epoch 18/20 | batch 15/60 | global_step 7035 | loss_total 0.9329\n",
      "step 3/3 | epoch 18/20 | batch 16/60 | global_step 7036 | loss_total 0.8152\n",
      "step 3/3 | epoch 18/20 | batch 17/60 | global_step 7037 | loss_total 0.5980\n",
      "step 3/3 | epoch 18/20 | batch 18/60 | global_step 7038 | loss_total 1.1861\n",
      "step 3/3 | epoch 18/20 | batch 19/60 | global_step 7039 | loss_total 0.8575\n",
      "step 3/3 | epoch 18/20 | batch 20/60 | global_step 7040 | loss_total 0.7237\n",
      "step 3/3 | epoch 18/20 | batch 21/60 | global_step 7041 | loss_total 1.2162\n",
      "step 3/3 | epoch 18/20 | batch 22/60 | global_step 7042 | loss_total 2.5155\n",
      "step 3/3 | epoch 18/20 | batch 23/60 | global_step 7043 | loss_total 1.1424\n",
      "step 3/3 | epoch 18/20 | batch 24/60 | global_step 7044 | loss_total 0.5820\n",
      "step 3/3 | epoch 18/20 | batch 25/60 | global_step 7045 | loss_total 0.7694\n",
      "step 3/3 | epoch 18/20 | batch 26/60 | global_step 7046 | loss_total 1.1779\n",
      "step 3/3 | epoch 18/20 | batch 27/60 | global_step 7047 | loss_total 0.9777\n",
      "step 3/3 | epoch 18/20 | batch 28/60 | global_step 7048 | loss_total 1.3618\n",
      "step 3/3 | epoch 18/20 | batch 29/60 | global_step 7049 | loss_total 0.8338\n",
      "step 3/3 | epoch 18/20 | batch 30/60 | global_step 7050 | loss_total 0.5617\n",
      "step 3/3 | epoch 18/20 | batch 31/60 | global_step 7051 | loss_total 0.5911\n",
      "step 3/3 | epoch 18/20 | batch 32/60 | global_step 7052 | loss_total 1.1898\n",
      "step 3/3 | epoch 18/20 | batch 33/60 | global_step 7053 | loss_total 0.5948\n",
      "step 3/3 | epoch 18/20 | batch 34/60 | global_step 7054 | loss_total 0.7339\n",
      "step 3/3 | epoch 18/20 | batch 35/60 | global_step 7055 | loss_total 1.0125\n",
      "step 3/3 | epoch 18/20 | batch 36/60 | global_step 7056 | loss_total 0.5188\n",
      "step 3/3 | epoch 18/20 | batch 37/60 | global_step 7057 | loss_total 0.6275\n",
      "step 3/3 | epoch 18/20 | batch 38/60 | global_step 7058 | loss_total 0.8210\n",
      "step 3/3 | epoch 18/20 | batch 39/60 | global_step 7059 | loss_total 1.0170\n",
      "step 3/3 | epoch 18/20 | batch 40/60 | global_step 7060 | loss_total 1.3152\n",
      "step 3/3 | epoch 18/20 | batch 41/60 | global_step 7061 | loss_total 0.8570\n",
      "step 3/3 | epoch 18/20 | batch 42/60 | global_step 7062 | loss_total 0.6426\n",
      "step 3/3 | epoch 18/20 | batch 43/60 | global_step 7063 | loss_total 0.5676\n",
      "step 3/3 | epoch 18/20 | batch 44/60 | global_step 7064 | loss_total 0.8394\n",
      "step 3/3 | epoch 18/20 | batch 45/60 | global_step 7065 | loss_total 1.5798\n",
      "step 3/3 | epoch 18/20 | batch 46/60 | global_step 7066 | loss_total 0.8210\n",
      "step 3/3 | epoch 18/20 | batch 47/60 | global_step 7067 | loss_total 0.5287\n",
      "step 3/3 | epoch 18/20 | batch 48/60 | global_step 7068 | loss_total 0.9805\n",
      "step 3/3 | epoch 18/20 | batch 49/60 | global_step 7069 | loss_total 0.6950\n",
      "step 3/3 | epoch 18/20 | batch 50/60 | global_step 7070 | loss_total 0.6196\n",
      "step 3/3 | epoch 18/20 | batch 51/60 | global_step 7071 | loss_total 0.8279\n",
      "step 3/3 | epoch 18/20 | batch 52/60 | global_step 7072 | loss_total 0.7698\n",
      "step 3/3 | epoch 18/20 | batch 53/60 | global_step 7073 | loss_total 0.5774\n",
      "step 3/3 | epoch 18/20 | batch 54/60 | global_step 7074 | loss_total 0.8016\n",
      "step 3/3 | epoch 18/20 | batch 55/60 | global_step 7075 | loss_total 0.5334\n",
      "step 3/3 | epoch 18/20 | batch 56/60 | global_step 7076 | loss_total 0.6015\n",
      "step 3/3 | epoch 18/20 | batch 57/60 | global_step 7077 | loss_total 1.2108\n",
      "step 3/3 | epoch 18/20 | batch 58/60 | global_step 7078 | loss_total 0.8215\n",
      "step 3/3 | epoch 18/20 | batch 59/60 | global_step 7079 | loss_total 2.9731\n",
      "step 3/3 | epoch 18/20 | batch 60/60 | global_step 7080 | loss_total 0.7788\n",
      "[epoch done] step 3/3 epoch 18/20 | train_total=0.9626 val_total=0.7823\n",
      "step 3/3 | epoch 19/20 | batch 1/60 | global_step 7081 | loss_total 0.7234\n",
      "step 3/3 | epoch 19/20 | batch 2/60 | global_step 7082 | loss_total 0.5610\n",
      "step 3/3 | epoch 19/20 | batch 3/60 | global_step 7083 | loss_total 0.5623\n",
      "step 3/3 | epoch 19/20 | batch 4/60 | global_step 7084 | loss_total 0.5851\n",
      "step 3/3 | epoch 19/20 | batch 5/60 | global_step 7085 | loss_total 0.7244\n",
      "step 3/3 | epoch 19/20 | batch 6/60 | global_step 7086 | loss_total 1.3816\n",
      "step 3/3 | epoch 19/20 | batch 7/60 | global_step 7087 | loss_total 0.5371\n",
      "step 3/3 | epoch 19/20 | batch 8/60 | global_step 7088 | loss_total 0.9116\n",
      "step 3/3 | epoch 19/20 | batch 9/60 | global_step 7089 | loss_total 0.8294\n",
      "step 3/3 | epoch 19/20 | batch 10/60 | global_step 7090 | loss_total 0.9802\n",
      "step 3/3 | epoch 19/20 | batch 11/60 | global_step 7091 | loss_total 1.0897\n",
      "step 3/3 | epoch 19/20 | batch 12/60 | global_step 7092 | loss_total 0.7268\n",
      "step 3/3 | epoch 19/20 | batch 13/60 | global_step 7093 | loss_total 0.7164\n",
      "step 3/3 | epoch 19/20 | batch 14/60 | global_step 7094 | loss_total 1.2903\n",
      "step 3/3 | epoch 19/20 | batch 15/60 | global_step 7095 | loss_total 1.3692\n",
      "step 3/3 | epoch 19/20 | batch 16/60 | global_step 7096 | loss_total 0.7163\n",
      "step 3/3 | epoch 19/20 | batch 17/60 | global_step 7097 | loss_total 0.5491\n",
      "step 3/3 | epoch 19/20 | batch 18/60 | global_step 7098 | loss_total 1.1135\n",
      "step 3/3 | epoch 19/20 | batch 19/60 | global_step 7099 | loss_total 0.5747\n",
      "step 3/3 | epoch 19/20 | batch 20/60 | global_step 7100 | loss_total 0.9275\n",
      "step 3/3 | epoch 19/20 | batch 21/60 | global_step 7101 | loss_total 0.7028\n",
      "step 3/3 | epoch 19/20 | batch 22/60 | global_step 7102 | loss_total 1.3477\n",
      "step 3/3 | epoch 19/20 | batch 23/60 | global_step 7103 | loss_total 1.1721\n",
      "step 3/3 | epoch 19/20 | batch 24/60 | global_step 7104 | loss_total 0.5462\n",
      "step 3/3 | epoch 19/20 | batch 25/60 | global_step 7105 | loss_total 0.5572\n",
      "step 3/3 | epoch 19/20 | batch 26/60 | global_step 7106 | loss_total 1.1192\n",
      "step 3/3 | epoch 19/20 | batch 27/60 | global_step 7107 | loss_total 1.1720\n",
      "step 3/3 | epoch 19/20 | batch 28/60 | global_step 7108 | loss_total 0.7448\n",
      "step 3/3 | epoch 19/20 | batch 29/60 | global_step 7109 | loss_total 2.8038\n",
      "step 3/3 | epoch 19/20 | batch 30/60 | global_step 7110 | loss_total 0.6332\n",
      "step 3/3 | epoch 19/20 | batch 31/60 | global_step 7111 | loss_total 1.0626\n",
      "step 3/3 | epoch 19/20 | batch 32/60 | global_step 7112 | loss_total 0.5618\n",
      "step 3/3 | epoch 19/20 | batch 33/60 | global_step 7113 | loss_total 0.5703\n",
      "step 3/3 | epoch 19/20 | batch 34/60 | global_step 7114 | loss_total 1.7712\n",
      "step 3/3 | epoch 19/20 | batch 35/60 | global_step 7115 | loss_total 1.6132\n",
      "step 3/3 | epoch 19/20 | batch 36/60 | global_step 7116 | loss_total 0.5936\n",
      "step 3/3 | epoch 19/20 | batch 37/60 | global_step 7117 | loss_total 1.3486\n",
      "step 3/3 | epoch 19/20 | batch 38/60 | global_step 7118 | loss_total 0.7517\n",
      "step 3/3 | epoch 19/20 | batch 39/60 | global_step 7119 | loss_total 0.7246\n",
      "step 3/3 | epoch 19/20 | batch 40/60 | global_step 7120 | loss_total 0.7342\n",
      "step 3/3 | epoch 19/20 | batch 41/60 | global_step 7121 | loss_total 0.9370\n",
      "step 3/3 | epoch 19/20 | batch 42/60 | global_step 7122 | loss_total 0.7306\n",
      "step 3/3 | epoch 19/20 | batch 43/60 | global_step 7123 | loss_total 1.7320\n",
      "step 3/3 | epoch 19/20 | batch 44/60 | global_step 7124 | loss_total 0.5645\n",
      "step 3/3 | epoch 19/20 | batch 45/60 | global_step 7125 | loss_total 0.7724\n",
      "step 3/3 | epoch 19/20 | batch 46/60 | global_step 7126 | loss_total 0.8740\n",
      "step 3/3 | epoch 19/20 | batch 47/60 | global_step 7127 | loss_total 0.7285\n",
      "step 3/3 | epoch 19/20 | batch 48/60 | global_step 7128 | loss_total 1.7723\n",
      "step 3/3 | epoch 19/20 | batch 49/60 | global_step 7129 | loss_total 1.1069\n",
      "step 3/3 | epoch 19/20 | batch 50/60 | global_step 7130 | loss_total 0.5779\n",
      "step 3/3 | epoch 19/20 | batch 51/60 | global_step 7131 | loss_total 1.0915\n",
      "step 3/3 | epoch 19/20 | batch 52/60 | global_step 7132 | loss_total 5.7524\n",
      "step 3/3 | epoch 19/20 | batch 53/60 | global_step 7133 | loss_total 1.0048\n",
      "step 3/3 | epoch 19/20 | batch 54/60 | global_step 7134 | loss_total 1.0215\n",
      "step 3/3 | epoch 19/20 | batch 55/60 | global_step 7135 | loss_total 1.6774\n",
      "step 3/3 | epoch 19/20 | batch 56/60 | global_step 7136 | loss_total 0.7550\n",
      "step 3/3 | epoch 19/20 | batch 57/60 | global_step 7137 | loss_total 0.7374\n",
      "step 3/3 | epoch 19/20 | batch 58/60 | global_step 7138 | loss_total 0.7312\n",
      "step 3/3 | epoch 19/20 | batch 59/60 | global_step 7139 | loss_total 0.7293\n",
      "step 3/3 | epoch 19/20 | batch 60/60 | global_step 7140 | loss_total 0.5755\n",
      "[epoch done] step 3/3 epoch 19/20 | train_total=1.0212 val_total=0.7105\n",
      "step 3/3 | epoch 20/20 | batch 1/60 | global_step 7141 | loss_total 0.5784\n",
      "step 3/3 | epoch 20/20 | batch 2/60 | global_step 7142 | loss_total 1.0914\n",
      "step 3/3 | epoch 20/20 | batch 3/60 | global_step 7143 | loss_total 1.5770\n",
      "step 3/3 | epoch 20/20 | batch 4/60 | global_step 7144 | loss_total 1.0062\n",
      "step 3/3 | epoch 20/20 | batch 5/60 | global_step 7145 | loss_total 0.6299\n",
      "step 3/3 | epoch 20/20 | batch 6/60 | global_step 7146 | loss_total 0.8525\n",
      "step 3/3 | epoch 20/20 | batch 7/60 | global_step 7147 | loss_total 0.8518\n",
      "step 3/3 | epoch 20/20 | batch 8/60 | global_step 7148 | loss_total 0.8468\n",
      "step 3/3 | epoch 20/20 | batch 9/60 | global_step 7149 | loss_total 0.8345\n",
      "step 3/3 | epoch 20/20 | batch 10/60 | global_step 7150 | loss_total 0.8915\n",
      "step 3/3 | epoch 20/20 | batch 11/60 | global_step 7151 | loss_total 0.7911\n",
      "step 3/3 | epoch 20/20 | batch 12/60 | global_step 7152 | loss_total 0.6570\n",
      "step 3/3 | epoch 20/20 | batch 13/60 | global_step 7153 | loss_total 0.5790\n",
      "step 3/3 | epoch 20/20 | batch 14/60 | global_step 7154 | loss_total 0.9966\n",
      "step 3/3 | epoch 20/20 | batch 15/60 | global_step 7155 | loss_total 1.1780\n",
      "step 3/3 | epoch 20/20 | batch 16/60 | global_step 7156 | loss_total 0.8207\n",
      "step 3/3 | epoch 20/20 | batch 17/60 | global_step 7157 | loss_total 0.5624\n",
      "step 3/3 | epoch 20/20 | batch 18/60 | global_step 7158 | loss_total 0.5793\n",
      "step 3/3 | epoch 20/20 | batch 19/60 | global_step 7159 | loss_total 0.5627\n",
      "step 3/3 | epoch 20/20 | batch 20/60 | global_step 7160 | loss_total 0.7098\n",
      "step 3/3 | epoch 20/20 | batch 21/60 | global_step 7161 | loss_total 1.0444\n",
      "step 3/3 | epoch 20/20 | batch 22/60 | global_step 7162 | loss_total 1.0932\n",
      "step 3/3 | epoch 20/20 | batch 23/60 | global_step 7163 | loss_total 1.2470\n",
      "step 3/3 | epoch 20/20 | batch 24/60 | global_step 7164 | loss_total 0.9634\n",
      "step 3/3 | epoch 20/20 | batch 25/60 | global_step 7165 | loss_total 0.8910\n",
      "step 3/3 | epoch 20/20 | batch 26/60 | global_step 7166 | loss_total 0.5380\n",
      "step 3/3 | epoch 20/20 | batch 27/60 | global_step 7167 | loss_total 0.6619\n",
      "step 3/3 | epoch 20/20 | batch 28/60 | global_step 7168 | loss_total 0.7133\n",
      "step 3/3 | epoch 20/20 | batch 29/60 | global_step 7169 | loss_total 0.7389\n",
      "step 3/3 | epoch 20/20 | batch 30/60 | global_step 7170 | loss_total 1.1138\n",
      "step 3/3 | epoch 20/20 | batch 31/60 | global_step 7171 | loss_total 1.1144\n",
      "step 3/3 | epoch 20/20 | batch 32/60 | global_step 7172 | loss_total 0.7750\n",
      "step 3/3 | epoch 20/20 | batch 33/60 | global_step 7173 | loss_total 0.5475\n",
      "step 3/3 | epoch 20/20 | batch 34/60 | global_step 7174 | loss_total 0.5823\n",
      "step 3/3 | epoch 20/20 | batch 35/60 | global_step 7175 | loss_total 1.3512\n",
      "step 3/3 | epoch 20/20 | batch 36/60 | global_step 7176 | loss_total 0.5588\n",
      "step 3/3 | epoch 20/20 | batch 37/60 | global_step 7177 | loss_total 0.5582\n",
      "step 3/3 | epoch 20/20 | batch 38/60 | global_step 7178 | loss_total 0.5389\n",
      "step 3/3 | epoch 20/20 | batch 39/60 | global_step 7179 | loss_total 0.7563\n",
      "step 3/3 | epoch 20/20 | batch 40/60 | global_step 7180 | loss_total 0.7602\n",
      "step 3/3 | epoch 20/20 | batch 41/60 | global_step 7181 | loss_total 1.6207\n",
      "step 3/3 | epoch 20/20 | batch 42/60 | global_step 7182 | loss_total 0.5317\n",
      "step 3/3 | epoch 20/20 | batch 43/60 | global_step 7183 | loss_total 2.3806\n",
      "step 3/3 | epoch 20/20 | batch 44/60 | global_step 7184 | loss_total 1.9665\n",
      "step 3/3 | epoch 20/20 | batch 45/60 | global_step 7185 | loss_total 1.1043\n",
      "step 3/3 | epoch 20/20 | batch 46/60 | global_step 7186 | loss_total 0.5276\n",
      "step 3/3 | epoch 20/20 | batch 47/60 | global_step 7187 | loss_total 0.7876\n",
      "step 3/3 | epoch 20/20 | batch 48/60 | global_step 7188 | loss_total 0.8067\n",
      "step 3/3 | epoch 20/20 | batch 49/60 | global_step 7189 | loss_total 0.8365\n",
      "step 3/3 | epoch 20/20 | batch 50/60 | global_step 7190 | loss_total 0.5320\n",
      "step 3/3 | epoch 20/20 | batch 51/60 | global_step 7191 | loss_total 1.0968\n",
      "step 3/3 | epoch 20/20 | batch 52/60 | global_step 7192 | loss_total 0.9290\n",
      "step 3/3 | epoch 20/20 | batch 53/60 | global_step 7193 | loss_total 1.0659\n",
      "step 3/3 | epoch 20/20 | batch 54/60 | global_step 7194 | loss_total 0.7067\n",
      "step 3/3 | epoch 20/20 | batch 55/60 | global_step 7195 | loss_total 1.3895\n",
      "step 3/3 | epoch 20/20 | batch 56/60 | global_step 7196 | loss_total 0.5064\n",
      "step 3/3 | epoch 20/20 | batch 57/60 | global_step 7197 | loss_total 0.7722\n",
      "step 3/3 | epoch 20/20 | batch 58/60 | global_step 7198 | loss_total 0.7565\n",
      "step 3/3 | epoch 20/20 | batch 59/60 | global_step 7199 | loss_total 0.7926\n",
      "step 3/3 | epoch 20/20 | batch 60/60 | global_step 7200 | loss_total 1.0862\n",
      "[epoch done] step 3/3 epoch 20/20 | train_total=0.8890 val_total=0.8023\n",
      "Saved after-training examples: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f\\examples_after.csv (4 rows)\n",
      "Saved before-vs-after compare: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f\\examples_compare.csv (4 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>music_prob_before</th>\n",
       "      <th>style_pred_before</th>\n",
       "      <th>style_prob_before</th>\n",
       "      <th>music_prob_after</th>\n",
       "      <th>style_pred_after</th>\n",
       "      <th>style_prob_after</th>\n",
       "      <th>pred_changed</th>\n",
       "      <th>music_prob_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc0_music</td>\n",
       "      <td>Lloyd Rodgers - One Questions of Discipline an...</td>\n",
       "      <td>0.380886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186144</td>\n",
       "      <td>9.999740e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958771</td>\n",
       "      <td>True</td>\n",
       "      <td>0.619088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hh_lfbb</td>\n",
       "      <td>72bpm_hh_lfbb_mid_001_04.wav</td>\n",
       "      <td>0.324834</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185225</td>\n",
       "      <td>9.999373e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646332</td>\n",
       "      <td>False</td>\n",
       "      <td>0.675103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>libirspeech</td>\n",
       "      <td>6319-275224-0019.flac</td>\n",
       "      <td>0.361139</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185548</td>\n",
       "      <td>6.512378e-08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.973384</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.361139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xtc_hiphop</td>\n",
       "      <td>FD1404_lop_098bpm.wav</td>\n",
       "      <td>0.360715</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185436</td>\n",
       "      <td>9.604760e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706460</td>\n",
       "      <td>False</td>\n",
       "      <td>0.599761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                               file  \\\n",
       "0    cc0_music  Lloyd Rodgers - One Questions of Discipline an...   \n",
       "1      hh_lfbb                       72bpm_hh_lfbb_mid_001_04.wav   \n",
       "2  libirspeech                              6319-275224-0019.flac   \n",
       "3   xtc_hiphop                              FD1404_lop_098bpm.wav   \n",
       "\n",
       "   music_prob_before  style_pred_before  style_prob_before  music_prob_after  \\\n",
       "0           0.380886                  1           0.186144      9.999740e-01   \n",
       "1           0.324834                  1           0.185225      9.999373e-01   \n",
       "2           0.361139                  1           0.185548      6.512378e-08   \n",
       "3           0.360715                  1           0.185436      9.604760e-01   \n",
       "\n",
       "   style_pred_after  style_prob_after  pred_changed  music_prob_delta  \n",
       "0                 0          0.958771          True          0.619088  \n",
       "1                 1          0.646332         False          0.675103  \n",
       "2                 2          0.973384          True         -0.361139  \n",
       "3                 1          0.706460         False          0.599761  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training status: COMPLETE\n",
      "Next resume pointer: phase_idx=3 epoch=1 step=1\n",
      "save_dir: z:\\328\\CMPUT328-A2\\codexworks\\301\\414-pl1\\saves\\lab1_run_grl_hn_f\n",
      "history_rows: 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>epoch</th>\n",
       "      <th>pipeline_step</th>\n",
       "      <th>train_content</th>\n",
       "      <th>train_style</th>\n",
       "      <th>train_music</th>\n",
       "      <th>train_content_adv</th>\n",
       "      <th>train_content_l1</th>\n",
       "      <th>train_music_bias</th>\n",
       "      <th>train_anchor</th>\n",
       "      <th>...</th>\n",
       "      <th>val_style</th>\n",
       "      <th>val_music</th>\n",
       "      <th>val_content_adv</th>\n",
       "      <th>val_content_l1</th>\n",
       "      <th>val_music_bias</th>\n",
       "      <th>val_anchor</th>\n",
       "      <th>val_total</th>\n",
       "      <th>val_music_pos_weight</th>\n",
       "      <th>val_music_skipped</th>\n",
       "      <th>global_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.394863</td>\n",
       "      <td>0.032554</td>\n",
       "      <td>1.281727</td>\n",
       "      <td>0.078809</td>\n",
       "      <td>6.293959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897593</td>\n",
       "      <td>0.078223</td>\n",
       "      <td>11.890263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713174</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.447343</td>\n",
       "      <td>0.124624</td>\n",
       "      <td>1.257361</td>\n",
       "      <td>0.078377</td>\n",
       "      <td>6.720754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902195</td>\n",
       "      <td>0.077827</td>\n",
       "      <td>7.002124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.282022</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>1.135970</td>\n",
       "      <td>0.078785</td>\n",
       "      <td>5.410568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.011869</td>\n",
       "      <td>0.078492</td>\n",
       "      <td>8.983283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>0.052275</td>\n",
       "      <td>1.174711</td>\n",
       "      <td>0.079380</td>\n",
       "      <td>4.167575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.805550</td>\n",
       "      <td>0.079330</td>\n",
       "      <td>5.610002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.595524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.448559</td>\n",
       "      <td>0.039553</td>\n",
       "      <td>1.228143</td>\n",
       "      <td>0.079490</td>\n",
       "      <td>4.791576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.720507</td>\n",
       "      <td>0.079541</td>\n",
       "      <td>6.951894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.689229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.544174</td>\n",
       "      <td>0.067438</td>\n",
       "      <td>1.184772</td>\n",
       "      <td>0.079416</td>\n",
       "      <td>4.630778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877660</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>8.095507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.424783</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>1.225639</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>6.938027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.865544</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>6.764722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709506</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.360392</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>1.230055</td>\n",
       "      <td>0.079062</td>\n",
       "      <td>5.769918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978240</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>14.114408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.782259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.459982</td>\n",
       "      <td>0.030544</td>\n",
       "      <td>1.225173</td>\n",
       "      <td>0.079219</td>\n",
       "      <td>5.711941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.896505</td>\n",
       "      <td>0.079014</td>\n",
       "      <td>12.965034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.710514</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.332895</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>1.219217</td>\n",
       "      <td>0.079240</td>\n",
       "      <td>7.739525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.008494</td>\n",
       "      <td>0.079352</td>\n",
       "      <td>6.938179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.802275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     phase  epoch  pipeline_step  train_content  train_style  train_music  \\\n",
       "110      3     11              3       0.000011     0.394863     0.032554   \n",
       "111      3     12              3       0.000021     0.447343     0.124624   \n",
       "112      3     13              3       0.000018     0.282022     0.094700   \n",
       "113      3     14              3       0.000018     0.361030     0.052275   \n",
       "114      3     15              3       0.000019     0.448559     0.039553   \n",
       "115      3     16              3       0.000023     0.544174     0.067438   \n",
       "116      3     17              3       0.000018     0.424783     0.005926   \n",
       "117      3     18              3       0.000022     0.360392     0.023875   \n",
       "118      3     19              3       0.000016     0.459982     0.030544   \n",
       "119      3     20              3       0.000025     0.332895     0.009254   \n",
       "\n",
       "     train_content_adv  train_content_l1  train_music_bias  train_anchor  ...  \\\n",
       "110           1.281727          0.078809          6.293959           0.0  ...   \n",
       "111           1.257361          0.078377          6.720754           0.0  ...   \n",
       "112           1.135970          0.078785          5.410568           0.0  ...   \n",
       "113           1.174711          0.079380          4.167575           0.0  ...   \n",
       "114           1.228143          0.079490          4.791576           0.0  ...   \n",
       "115           1.184772          0.079416          4.630778           0.0  ...   \n",
       "116           1.225639          0.079151          6.938027           0.0  ...   \n",
       "117           1.230055          0.079062          5.769918           0.0  ...   \n",
       "118           1.225173          0.079219          5.711941           0.0  ...   \n",
       "119           1.219217          0.079240          7.739525           0.0  ...   \n",
       "\n",
       "     val_style  val_music  val_content_adv  val_content_l1  val_music_bias  \\\n",
       "110   0.481685        0.0         0.897593        0.078223       11.890263   \n",
       "111   0.484213        0.0         0.902195        0.077827        7.002124   \n",
       "112   0.357627        0.0         1.011869        0.078492        8.983283   \n",
       "113   0.312119        0.0         0.805550        0.079330        5.610002   \n",
       "114   0.723791        0.0         0.720507        0.079541        6.951894   \n",
       "115   0.558993        0.0         0.877660        0.079114        8.095507   \n",
       "116   0.533409        0.0         0.865544        0.079084        6.764722   \n",
       "117   0.537550        0.0         0.978240        0.078600       14.114408   \n",
       "118   0.474347        0.0         0.896505        0.079014       12.965034   \n",
       "119   0.553195        0.0         1.008494        0.079352        6.938179   \n",
       "\n",
       "     val_anchor  val_total  val_music_pos_weight  val_music_skipped  \\\n",
       "110         0.0   0.713174                   1.0                1.0   \n",
       "111         0.0   0.714375                   1.0                1.0   \n",
       "112         0.0   0.736957                   1.0                1.0   \n",
       "113         0.0   0.595524                   1.0                1.0   \n",
       "114         0.0   0.689229                   1.0                1.0   \n",
       "115         0.0   0.726414                   1.0                1.0   \n",
       "116         0.0   0.709506                   1.0                1.0   \n",
       "117         0.0   0.782259                   1.0                1.0   \n",
       "118         0.0   0.710514                   1.0                1.0   \n",
       "119         0.0   0.802275                   1.0                1.0   \n",
       "\n",
       "     global_step  \n",
       "110         6660  \n",
       "111         6720  \n",
       "112         6780  \n",
       "113         6840  \n",
       "114         6900  \n",
       "115         6960  \n",
       "116         7020  \n",
       "117         7080  \n",
       "118         7140  \n",
       "119         7200  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Implementation: resumable curriculum trainer with step/epoch checkpoints\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# --------------------\n",
    "# Run control variables (set these before running this cell)\n",
    "# --------------------\n",
    "SAVENAME = \"lab1_run_grl_hn_f\"  # required: run folder name under ./saves/\n",
    "MODE = \"fresh\"               # \"fresh\" starts new run, \"resume\" loads ./saves/<SAVENAME>/latest.pt\n",
    "RUN_TRAINING = True         # set True to start/continue training\n",
    "RUN_UNTIL_PHASE = None        # None => run all remaining phases, or set 1/2/3 to stop after that phase\n",
    "REQUIRE_CUDA = True         # True => fail fast if CUDA is not available\n",
    "DEVICE_PREFERENCE = \"cuda\"   # \"cuda\" or \"cpu\"\n",
    "EXAMPLE_MAX_PER_SOURCE = 1   # examples per source for before/after snapshot\n",
    "PHASE3_NEGATIVE_CAP = 4000    # cap negatives used in phase 3\n",
    "PHASE3_POS_TO_NEG_RATIO = 1.0 # positives sampled per negative in phase 3\n",
    "PHASE3_INCLUDE_FSD50K_NEG = False  # keep False unless you explicitly want FSD negatives\n",
    "PHASE3_HARD_NEGATIVE_ENABLE = True\n",
    "PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB = 0.90\n",
    "PHASE3_HARD_NEGATIVE_REPEAT = 2\n",
    "PHASE3_HARD_NEGATIVE_MAX = 1000\n",
    "PHASE3_HARD_NEGATIVE_CSV = None  # Optional explicit path to gate_predictions.csv\n",
    "PHASE3_HARD_NEGATIVE_AUDIT_ROOT = Path.cwd() / \"saves\" / \"lab1_run_a\" / \"audits\"\n",
    "PHASE3_KEEP_NEG_DUPLICATES = True  # Keep duplicates so hard negatives are oversampled\n",
    "PHASE3_HARD_NEGATIVE_LAST_N_EPOCHS = 6  # Apply hard negatives only in the final N epochs of phase 3\n",
    "\n",
    "\n",
    "def resolve_hard_negative_csv(explicit_path: str | Path | None = PHASE3_HARD_NEGATIVE_CSV) -> Path | None:\n",
    "    if explicit_path is not None:\n",
    "        p = Path(str(explicit_path))\n",
    "        return p if p.exists() else None\n",
    "    root = Path(PHASE3_HARD_NEGATIVE_AUDIT_ROOT)\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    candidates = sorted(\n",
    "        root.glob(\"**/gate_predictions.csv\"),\n",
    "        key=lambda x: x.stat().st_mtime,\n",
    "        reverse=True,\n",
    "    )\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "\n",
    "def load_hard_negative_paths(\n",
    "    min_music_prob: float = PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB,\n",
    "    csv_path: str | Path | None = PHASE3_HARD_NEGATIVE_CSV,\n",
    "    max_items: int | None = PHASE3_HARD_NEGATIVE_MAX,\n",
    ") -> set[str]:\n",
    "    p = resolve_hard_negative_csv(csv_path)\n",
    "    if p is None:\n",
    "        return set()\n",
    "    df = pd.read_csv(p)\n",
    "    required = {\"path\", \"source\", \"music_prob\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        return set()\n",
    "    hard = df[(df[\"source\"] == \"libirspeech\") & (df[\"music_prob\"] >= float(min_music_prob))].copy()\n",
    "    hard = hard[hard[\"path\"].map(lambda x: Path(str(x)).exists())]\n",
    "    if len(hard) == 0:\n",
    "        return set()\n",
    "    hard = hard.sort_values(\"music_prob\", ascending=False).reset_index(drop=True)\n",
    "    if max_items is not None:\n",
    "        hard = hard.head(int(max_items)).reset_index(drop=True)\n",
    "    return set(hard[\"path\"].astype(str).tolist())\n",
    "\n",
    "\n",
    "def build_phase3_music_guard_manifest(\n",
    "    negative_cap: int = PHASE3_NEGATIVE_CAP,\n",
    "    pos_to_neg_ratio: float = PHASE3_POS_TO_NEG_RATIO,\n",
    "    include_fsd50k_neg: bool = PHASE3_INCLUDE_FSD50K_NEG,\n",
    "    enable_hard_negatives: bool = True,\n",
    "    seed: int = SEED,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Phase 3 should be balanced; avoid all-negative collapse in music head.\"\"\"\n",
    "    neg_pool = phase3_manifest.copy()\n",
    "\n",
    "    keep_sources = [\"libirspeech\"]\n",
    "    if include_fsd50k_neg:\n",
    "        keep_sources.append(\"fsd50k\")\n",
    "\n",
    "    neg_base = neg_pool[neg_pool[\"source\"].isin(keep_sources)].copy()\n",
    "    if len(neg_base) == 0:\n",
    "        raise FileNotFoundError(\"No negative sources available for phase 3\")\n",
    "\n",
    "    if negative_cap is not None and len(neg_base) > int(negative_cap):\n",
    "        neg_base = neg_base.sample(int(negative_cap), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    neg = neg_base.copy()\n",
    "    if PHASE3_HARD_NEGATIVE_ENABLE and enable_hard_negatives:\n",
    "        hard_paths = load_hard_negative_paths(\n",
    "            min_music_prob=PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB,\n",
    "            csv_path=PHASE3_HARD_NEGATIVE_CSV,\n",
    "            max_items=PHASE3_HARD_NEGATIVE_MAX,\n",
    "        )\n",
    "        if len(hard_paths) > 0:\n",
    "            hard = neg_pool[(neg_pool[\"source\"] == \"libirspeech\") & (neg_pool[\"path\"].astype(str).isin(hard_paths))].copy()\n",
    "            hard = hard[hard[\"path\"].map(lambda x: Path(str(x)).exists())].reset_index(drop=True)\n",
    "            if len(hard) > 0:\n",
    "                rep = max(1, int(PHASE3_HARD_NEGATIVE_REPEAT))\n",
    "                hard_rep = pd.concat([hard] * rep, ignore_index=True)\n",
    "                neg = pd.concat([neg_base, hard_rep], ignore_index=True)\n",
    "                print(\n",
    "                    f\"[phase3] hard negatives added: base={len(neg_base)} hard_unique={len(hard)} \"\n",
    "                    f\"repeat={rep} total_neg={len(neg)}\"\n",
    "                )\n",
    "\n",
    "    neg[\"is_music\"] = 0\n",
    "\n",
    "    pos_pool = phase2_manifest.copy()\n",
    "    if len(pos_pool) == 0:\n",
    "        raise FileNotFoundError(\"No positive music pool available (phase2_manifest empty)\")\n",
    "\n",
    "    n_pos = max(1, int(len(neg) * float(pos_to_neg_ratio)))\n",
    "    pos = pos_pool.sample(min(n_pos, len(pos_pool)), random_state=seed).reset_index(drop=True)\n",
    "    pos[\"is_music\"] = 1\n",
    "\n",
    "    out = pd.concat([pos, neg], ignore_index=True)\n",
    "    out = out[[c for c in [\"source\", \"path\", \"ext\", \"size_bytes\", \"is_music\"] if c in out.columns]]\n",
    "    if not PHASE3_KEEP_NEG_DUPLICATES:\n",
    "        out = out.drop_duplicates(subset=[\"path\", \"is_music\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        out = out.reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_phase_manifest(\n",
    "    phase: int,\n",
    "    cfg: dict | None = None,\n",
    "    phase3_enable_hard_negatives: bool | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    if phase == 1:\n",
    "        phase1_path = MANIFEST_FILES[\"phase1_audio\"]\n",
    "        if not phase1_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing {phase1_path}. Render PDMX/TheSession audio first and create this manifest.\"\n",
    "            )\n",
    "        df = pd.read_csv(phase1_path)\n",
    "        if \"source\" not in df.columns:\n",
    "            df[\"source\"] = \"phase1_symbolic\"\n",
    "        df[\"is_music\"] = 1\n",
    "        return df[[c for c in [\"source\", \"path\", \"ext\", \"size_bytes\", \"is_music\"] if c in df.columns]].reset_index(drop=True)\n",
    "\n",
    "    if phase == 2:\n",
    "        return phase2_manifest[[\"source\", \"path\", \"ext\", \"size_bytes\", \"is_music\"]].reset_index(drop=True)\n",
    "\n",
    "    if phase == 3:\n",
    "        if phase3_enable_hard_negatives is None:\n",
    "            phase3_enable_hard_negatives = bool(cfg.get(\"phase3_enable_hard_negatives\", False)) if cfg else False\n",
    "        return build_phase3_music_guard_manifest(enable_hard_negatives=bool(phase3_enable_hard_negatives))\n",
    "\n",
    "    raise ValueError(\"phase must be one of {1, 2, 3}\")\n",
    "\n",
    "\n",
    "def split_manifest_by_path(df: pd.DataFrame, val_ratio: float = 0.1, seed: int = SEED):\n",
    "    unique_paths = df[[\"path\"]].drop_duplicates().sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_val = max(1, int(len(unique_paths) * val_ratio)) if len(unique_paths) > 1 else 0\n",
    "\n",
    "    val_paths = set(unique_paths.iloc[:n_val][\"path\"].tolist())\n",
    "    train_df = df[~df[\"path\"].isin(val_paths)].reset_index(drop=True)\n",
    "    val_df = df[df[\"path\"].isin(val_paths)].reset_index(drop=True)\n",
    "\n",
    "    if len(train_df) == 0 and len(val_df) > 0:\n",
    "        train_df = val_df.copy()\n",
    "    if len(val_df) == 0 and len(train_df) > 1:\n",
    "        val_df = train_df.sample(min(len(train_df), 32), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def build_global_source_map(phases):\n",
    "    all_sources = []\n",
    "    for phase in phases:\n",
    "        try:\n",
    "            d = get_phase_manifest(phase, cfg=None, phase3_enable_hard_negatives=False)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        all_sources.extend(d[\"source\"].dropna().unique().tolist())\n",
    "    all_sources = sorted(set(all_sources))\n",
    "    return {s: i for i, s in enumerate(all_sources)}\n",
    "\n",
    "\n",
    "class ChunkEncoder(nn.Module):\n",
    "    def __init__(self, n_sources: int, z_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.shared = nn.Linear(128, 256)\n",
    "        self.content_head = nn.Linear(256, z_dim)\n",
    "        self.style_head = nn.Linear(256, z_dim)\n",
    "        self.style_cls = nn.Linear(z_dim, n_sources)\n",
    "        self.content_style_adv = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(z_dim, n_sources),\n",
    "        )\n",
    "        self.music_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, log_mel: torch.Tensor, grl_lambda: float = 1.0):\n",
    "        x = log_mel.unsqueeze(1)\n",
    "        h = self.backbone(x).flatten(1)\n",
    "        h = F.relu(self.shared(h))\n",
    "        z_content = F.normalize(self.content_head(h), dim=-1)\n",
    "        z_style = F.normalize(self.style_head(h), dim=-1)\n",
    "        z_content_rev = grad_reverse(z_content, lambda_=grl_lambda)\n",
    "        return {\n",
    "            \"z_content\": z_content,\n",
    "            \"z_style\": z_style,\n",
    "            \"style_logits\": self.style_cls(z_style),\n",
    "            \"content_style_logits\": self.content_style_adv(z_content_rev),\n",
    "            \"music_logit\": self.music_head(h).squeeze(-1),\n",
    "        }\n",
    "\n",
    "\n",
    "class _GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = float(lambda_)\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "def grad_reverse(x: torch.Tensor, lambda_: float = 1.0) -> torch.Tensor:\n",
    "    return _GradientReversal.apply(x, lambda_)\n",
    "\n",
    "\n",
    "def compute_losses(\n",
    "    out_a,\n",
    "    out_b,\n",
    "    source_idx,\n",
    "    is_music,\n",
    "    weights,\n",
    "    teacher_out_a=None,\n",
    "    teacher_out_b=None,\n",
    "):\n",
    "    loss_content = F.mse_loss(out_a[\"z_content\"], out_b[\"z_content\"])\n",
    "    loss_style = F.cross_entropy(out_a[\"style_logits\"], source_idx)\n",
    "    loss_content_adv = F.cross_entropy(out_a[\"content_style_logits\"], source_idx)\n",
    "    loss_content_l1 = 0.5 * (\n",
    "        out_a[\"z_content\"].abs().mean() + out_b[\"z_content\"].abs().mean()\n",
    "    )\n",
    "\n",
    "    # In all-positive/all-negative batches, music BCE can bias the gate.\n",
    "    # Optionally skip these updates and train the gate only on mixed batches.\n",
    "    n_pos_raw = is_music.sum()\n",
    "    n_neg_raw = (1.0 - is_music).sum()\n",
    "    has_both_classes = bool((n_pos_raw > 0).item() and (n_neg_raw > 0).item())\n",
    "    music_only_when_mixed = bool(weights.get(\"music_only_when_mixed\", False))\n",
    "    skip_music = music_only_when_mixed and (not has_both_classes)\n",
    "\n",
    "    if skip_music:\n",
    "        loss_music = out_a[\"music_logit\"].sum() * 0.0\n",
    "        pos_weight = torch.tensor(1.0, device=is_music.device, dtype=is_music.dtype)\n",
    "    else:\n",
    "        n_pos = torch.clamp(n_pos_raw, min=1.0)\n",
    "        n_neg = torch.clamp(n_neg_raw, min=1.0)\n",
    "        if \"music_pos_weight\" in weights:\n",
    "            pw = float(weights[\"music_pos_weight\"])\n",
    "            pos_weight = torch.tensor(pw, device=is_music.device, dtype=is_music.dtype)\n",
    "        else:\n",
    "            pos_weight = torch.clamp(n_neg / n_pos, min=0.25, max=8.0).detach()\n",
    "\n",
    "        loss_music = F.binary_cross_entropy_with_logits(\n",
    "            out_a[\"music_logit\"],\n",
    "            is_music,\n",
    "            pos_weight=pos_weight,\n",
    "        )\n",
    "\n",
    "    loss_music_bias = out_a[\"music_logit\"].mean().abs()\n",
    "    loss_anchor = out_a[\"z_content\"].sum() * 0.0\n",
    "    if teacher_out_a is not None:\n",
    "        loss_anchor = loss_anchor + F.mse_loss(out_a[\"z_content\"], teacher_out_a[\"z_content\"])\n",
    "    if teacher_out_b is not None:\n",
    "        loss_anchor = loss_anchor + F.mse_loss(out_b[\"z_content\"], teacher_out_b[\"z_content\"])\n",
    "        loss_anchor = 0.5 * loss_anchor\n",
    "\n",
    "    total = (\n",
    "        weights[\"content\"] * loss_content\n",
    "        + weights[\"style\"] * loss_style\n",
    "        + weights[\"music\"] * loss_music\n",
    "        + weights.get(\"content_adv\", 0.0) * loss_content_adv\n",
    "        + weights.get(\"content_l1\", 0.0) * loss_content_l1\n",
    "        + weights.get(\"music_bias\", 0.0) * loss_music_bias\n",
    "        + weights.get(\"anchor\", 0.0) * loss_anchor\n",
    "    )\n",
    "    return total, {\n",
    "        \"content\": float(loss_content.detach().cpu().item()),\n",
    "        \"style\": float(loss_style.detach().cpu().item()),\n",
    "        \"music\": float(loss_music.detach().cpu().item()),\n",
    "        \"content_adv\": float(loss_content_adv.detach().cpu().item()),\n",
    "        \"content_l1\": float(loss_content_l1.detach().cpu().item()),\n",
    "        \"music_bias\": float(loss_music_bias.detach().cpu().item()),\n",
    "        \"anchor\": float(loss_anchor.detach().cpu().item()),\n",
    "        \"total\": float(total.detach().cpu().item()),\n",
    "        \"music_pos_weight\": float(pos_weight.detach().cpu().item()),\n",
    "        \"music_skipped\": float(skip_music),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_validation(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    max_steps,\n",
    "    weights,\n",
    "    grl_lambda: float = 1.0,\n",
    "    teacher_model=None,\n",
    "):\n",
    "    model.eval()\n",
    "    stats = {\n",
    "        \"content\": 0.0,\n",
    "        \"style\": 0.0,\n",
    "        \"music\": 0.0,\n",
    "        \"content_adv\": 0.0,\n",
    "        \"content_l1\": 0.0,\n",
    "        \"music_bias\": 0.0,\n",
    "        \"anchor\": 0.0,\n",
    "        \"total\": 0.0,\n",
    "        \"music_pos_weight\": 0.0,\n",
    "        \"music_skipped\": 0.0,\n",
    "    }\n",
    "    steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(loader, start=1):\n",
    "            if max_steps is not None and batch_idx > max_steps:\n",
    "                break\n",
    "            log_mel = batch[\"log_mel\"].to(device, non_blocking=True)\n",
    "            log_mel_aug = batch[\"log_mel_aug\"].to(device, non_blocking=True)\n",
    "            source_idx = batch[\"source_idx\"].to(device, non_blocking=True)\n",
    "            is_music = batch[\"is_music\"].to(device, non_blocking=True)\n",
    "\n",
    "            out_a = model(log_mel, grl_lambda=grl_lambda)\n",
    "            out_b = model(log_mel_aug, grl_lambda=grl_lambda)\n",
    "            teacher_out_a = None\n",
    "            teacher_out_b = None\n",
    "            if teacher_model is not None and float(weights.get(\"anchor\", 0.0)) > 0.0:\n",
    "                teacher_out_a = teacher_model(log_mel, grl_lambda=0.0)\n",
    "                teacher_out_b = teacher_model(log_mel_aug, grl_lambda=0.0)\n",
    "            _, parts = compute_losses(\n",
    "                out_a,\n",
    "                out_b,\n",
    "                source_idx,\n",
    "                is_music,\n",
    "                weights,\n",
    "                teacher_out_a=teacher_out_a,\n",
    "                teacher_out_b=teacher_out_b,\n",
    "            )\n",
    "            for k in stats:\n",
    "                stats[k] += parts[k]\n",
    "            steps += 1\n",
    "\n",
    "    if steps == 0:\n",
    "        return {k: float(\"nan\") for k in stats}\n",
    "    return {k: stats[k] / steps for k in stats}\n",
    "\n",
    "\n",
    "def make_epoch_loaders(train_ds, val_ds, cfg, phase: int, epoch: int):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(int(cfg[\"seed\"] + phase * 100000 + epoch))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        pin_memory=(DEVICE_PREFERENCE == \"cuda\"),\n",
    "        persistent_workers=(cfg[\"num_workers\"] > 0),\n",
    "        drop_last=True,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        pin_memory=(DEVICE_PREFERENCE == \"cuda\"),\n",
    "        persistent_workers=(cfg[\"num_workers\"] > 0),\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def set_phase_trainable(model, phase: int, cfg: dict):\n",
    "    # Default: train all branches.\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Optional phase-3 gate hardening pass: train only music head.\n",
    "    phase3_train_mode = str(cfg.get(\"phase3_train_mode\", \"full\")).lower()\n",
    "    if int(phase) == 3 and bool(cfg.get(\"phase3_music_head_only\", False)):\n",
    "        phase3_train_mode = \"music_head_only\"\n",
    "\n",
    "    if int(phase) == 3 and phase3_train_mode == \"music_head_only\":\n",
    "        freeze_modules = [\n",
    "            model.backbone,\n",
    "            model.shared,\n",
    "            model.content_head,\n",
    "            model.style_head,\n",
    "            model.style_cls,\n",
    "            model.content_style_adv,\n",
    "        ]\n",
    "        for m in freeze_modules:\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "        for p in model.music_head.parameters():\n",
    "            p.requires_grad = True\n",
    "    elif int(phase) == 3 and phase3_train_mode == \"auc_sharpener\":\n",
    "        # Freeze everything first, then unfreeze only the boundary-sharpening parts.\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Final conv block + pool and shared projection adapt the decision boundary\n",
    "        # with minimal drift in the content branch.\n",
    "        for p in model.backbone[6:].parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.shared.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Keep content/style embedding heads frozen to preserve disentanglement.\n",
    "        # Only classifier heads are trainable in this mode.\n",
    "        for p in model.style_cls.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.content_style_adv.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in model.music_head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "def build_phase_cache(\n",
    "    phase: int,\n",
    "    source_to_idx: dict,\n",
    "    cfg: dict,\n",
    "    phase3_enable_hard_negatives: bool | None = None,\n",
    "):\n",
    "    phase_df = get_phase_manifest(\n",
    "        phase,\n",
    "        cfg=cfg,\n",
    "        phase3_enable_hard_negatives=phase3_enable_hard_negatives,\n",
    "    ).copy()\n",
    "    phase_df = phase_df[phase_df[\"path\"].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "    phase_df[\"source_idx\"] = phase_df[\"source\"].map(source_to_idx).astype(int)\n",
    "\n",
    "    train_files, val_files = split_manifest_by_path(phase_df, val_ratio=cfg[\"val_ratio\"], seed=cfg[\"seed\"])\n",
    "\n",
    "    train_chunk_df = build_chunk_index(\n",
    "        train_files,\n",
    "        chunk_seconds=cfg[\"chunk_seconds\"],\n",
    "        stride_seconds=cfg[\"stride_seconds\"],\n",
    "        max_files_per_source=cfg[\"max_files_per_source\"],\n",
    "        max_chunks_per_file=cfg[\"max_chunks_per_file\"],\n",
    "    )\n",
    "    val_chunk_df = build_chunk_index(\n",
    "        val_files,\n",
    "        chunk_seconds=cfg[\"chunk_seconds\"],\n",
    "        stride_seconds=cfg[\"stride_seconds\"],\n",
    "        max_files_per_source=max(16, cfg[\"max_files_per_source\"] // 3),\n",
    "        max_chunks_per_file=max(2, cfg[\"max_chunks_per_file\"] // 2),\n",
    "    )\n",
    "\n",
    "    train_ds = Lab1ChunkDataset(train_chunk_df, sample_rate=cfg[\"sample_rate\"])\n",
    "    val_ds = Lab1ChunkDataset(val_chunk_df, sample_rate=cfg[\"sample_rate\"])\n",
    "\n",
    "    return {\n",
    "        \"phase_df\": phase_df,\n",
    "        \"train_chunk_df\": train_chunk_df,\n",
    "        \"val_chunk_df\": val_chunk_df,\n",
    "        \"train_ds\": train_ds,\n",
    "        \"val_ds\": val_ds,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_optimizer_lrs_for_phase(optimizer, cfg: dict, phase: int, epoch: int, total_epochs: int):\n",
    "    base_lr = float(cfg[\"lr\"])\n",
    "    phase3_last_n = int(cfg.get(\"phase3_hard_negative_last_n_epochs\", 0))\n",
    "    phase3_hardening_start = max(1, total_epochs - phase3_last_n + 1) if phase3_last_n > 0 else total_epochs + 1\n",
    "\n",
    "    # Default multipliers.\n",
    "    lr_mults = {\n",
    "        \"backbone\": float(cfg.get(\"backbone_lr_mult\", 1.0)),\n",
    "        \"content\": 1.0,\n",
    "        \"music\": float(cfg.get(\"music_lr_mult\", 1.0)),\n",
    "        \"style\": float(cfg.get(\"style_lr_mult\", 1.0)),\n",
    "        \"adv\": float(cfg.get(\"adv_lr_mult\", 1.0)),\n",
    "        \"other\": 1.0,\n",
    "    }\n",
    "\n",
    "    # Phase-3 hardening: keep backbone stable while sharpening the decision boundary.\n",
    "    if int(phase) == 3 and int(epoch) >= int(phase3_hardening_start):\n",
    "        lr_mults[\"backbone\"] *= float(cfg.get(\"phase3_backbone_lr_scale\", 0.1))\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        name = g.get(\"name\", \"other\")\n",
    "        g[\"lr\"] = base_lr * lr_mults.get(name, 1.0)\n",
    "\n",
    "\n",
    "def save_checkpoint(run_dir: Path, filename: str, model, optimizer, train_state: dict, cfg: dict, source_to_idx: dict):\n",
    "    ckpt_dir = run_dir / \"checkpoints\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"train_state\": train_state,\n",
    "        \"cfg\": cfg,\n",
    "        \"source_to_idx\": source_to_idx,\n",
    "        \"saved_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "    target = ckpt_dir / filename\n",
    "    torch.save(payload, str(target))\n",
    "\n",
    "    latest = run_dir / \"latest.pt\"\n",
    "    torch.save(payload, str(latest))\n",
    "\n",
    "    state_json = run_dir / \"run_state.json\"\n",
    "    state_json.write_text(json.dumps(train_state, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def resolve_device(require_cuda: bool = True, preference: str = \"cuda\"):\n",
    "    if preference == \"cuda\":\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            try:\n",
    "                torch.set_float32_matmul_precision(\"high\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            return \"cuda\"\n",
    "        if require_cuda:\n",
    "            raise RuntimeError(\n",
    "                \"CUDA is not available in the active kernel. \"\n",
    "                \"Select kernel 'Python (lab1-venv)' and verify torch is a CUDA build.\"\n",
    "            )\n",
    "        return \"cpu\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def load_latest(run_dir: Path, model, optimizer):\n",
    "    latest = run_dir / \"latest.pt\"\n",
    "    if not latest.exists():\n",
    "        raise FileNotFoundError(f\"No checkpoint found: {latest}\")\n",
    "    payload = torch.load(str(latest), map_location=\"cpu\")\n",
    "    try:\n",
    "        model.load_state_dict(payload[\"model\"])\n",
    "    except RuntimeError as exc:\n",
    "        raise RuntimeError(\n",
    "            \"Checkpoint architecture mismatch. If you enabled GRL/de-style remediation, \"\n",
    "            \"start with MODE='fresh' and a new SAVENAME.\"\n",
    "        ) from exc\n",
    "    optimizer.load_state_dict(payload[\"optimizer\"])\n",
    "    return payload\n",
    "\n",
    "\n",
    "def build_example_manifest(source_to_idx: dict, max_per_source: int = 1, seed: int = SEED) -> pd.DataFrame:\n",
    "    \"\"\"Fixed comparison set: one/few clips per source from phase2+phase3 manifests.\"\"\"\n",
    "    pools = [phase2_manifest.copy(), phase3_manifest.copy()]\n",
    "    df = pd.concat(pools, ignore_index=True)\n",
    "    df = df[df[\"path\"].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame(columns=[\"source\", \"path\", \"source_idx\"])\n",
    "\n",
    "    rows = []\n",
    "    for src, g in df.groupby(\"source\"):\n",
    "        if src not in source_to_idx:\n",
    "            continue\n",
    "        take = g.sample(min(max_per_source, len(g)), random_state=seed)\n",
    "        for _, r in take.iterrows():\n",
    "            rows.append({\n",
    "                \"source\": src,\n",
    "                \"path\": str(r[\"path\"]),\n",
    "                \"source_idx\": int(source_to_idx[src]),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_model_examples(model, device, examples_df: pd.DataFrame, sample_rate: int, sample_seconds: float) -> pd.DataFrame:\n",
    "    if len(examples_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    idx_to_source = {}\n",
    "    if hasattr(model, \"style_cls\"):\n",
    "        n = int(model.style_cls.out_features)\n",
    "        idx_to_source = {i: f\"source_{i}\" for i in range(n)}\n",
    "\n",
    "    rows = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, r in examples_df.iterrows():\n",
    "            path = str(r[\"path\"])\n",
    "            src = str(r[\"source\"])\n",
    "            y = load_audio_chunk_48k(path=path, start_sec=0.0, duration_sec=sample_seconds, sample_rate=sample_rate)\n",
    "            mel = extract_log_mel_fast(y, sr=sample_rate)\n",
    "            x = torch.from_numpy(mel).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "            out = model(x)\n",
    "            probs = torch.softmax(out[\"style_logits\"], dim=-1)[0]\n",
    "            pred_idx = int(torch.argmax(probs).item())\n",
    "            topk = torch.topk(probs, k=min(3, probs.numel()))\n",
    "            music_prob = float(torch.sigmoid(out[\"music_logit\"])[0].item())\n",
    "\n",
    "            rows.append({\n",
    "                \"source\": src,\n",
    "                \"file\": Path(path).name,\n",
    "                \"path\": path,\n",
    "                \"music_prob\": music_prob,\n",
    "                \"style_pred_idx\": pred_idx,\n",
    "                \"style_pred_prob\": float(probs[pred_idx].item()),\n",
    "                \"top1_idx\": int(topk.indices[0].item()) if topk.indices.numel() > 0 else None,\n",
    "                \"top1_prob\": float(topk.values[0].item()) if topk.values.numel() > 0 else None,\n",
    "                \"top2_idx\": int(topk.indices[1].item()) if topk.indices.numel() > 1 else None,\n",
    "                \"top2_prob\": float(topk.values[1].item()) if topk.values.numel() > 1 else None,\n",
    "                \"top3_idx\": int(topk.indices[2].item()) if topk.indices.numel() > 2 else None,\n",
    "                \"top3_prob\": float(topk.values[2].item()) if topk.values.numel() > 2 else None,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def render_example_comparison(pre_df: pd.DataFrame, post_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(pre_df) == 0 or len(post_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    left = pre_df[[\"source\", \"file\", \"music_prob\", \"style_pred_idx\", \"style_pred_prob\"]].rename(\n",
    "        columns={\n",
    "            \"music_prob\": \"music_prob_before\",\n",
    "            \"style_pred_idx\": \"style_pred_before\",\n",
    "            \"style_pred_prob\": \"style_prob_before\",\n",
    "        }\n",
    "    )\n",
    "    right = post_df[[\"source\", \"file\", \"music_prob\", \"style_pred_idx\", \"style_pred_prob\"]].rename(\n",
    "        columns={\n",
    "            \"music_prob\": \"music_prob_after\",\n",
    "            \"style_pred_idx\": \"style_pred_after\",\n",
    "            \"style_pred_prob\": \"style_prob_after\",\n",
    "        }\n",
    "    )\n",
    "    out = left.merge(right, on=[\"source\", \"file\"], how=\"inner\")\n",
    "    out[\"pred_changed\"] = out[\"style_pred_before\"] != out[\"style_pred_after\"]\n",
    "    out[\"music_prob_delta\"] = out[\"music_prob_after\"] - out[\"music_prob_before\"]\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_curriculum_resumable(cfg: dict, savename: str, mode: str, run_until_phase=None):\n",
    "    assert torch is not None, \"Torch is required\"\n",
    "    assert mode in {\"fresh\", \"resume\"}, \"MODE must be 'fresh' or 'resume'\"\n",
    "\n",
    "    device = resolve_device(require_cuda=REQUIRE_CUDA, preference=DEVICE_PREFERENCE)\n",
    "    phase_order = cfg[\"phase_order\"]\n",
    "    source_map_phases = cfg.get(\"source_map_phases\", phase_order)\n",
    "    source_to_idx = build_global_source_map(source_map_phases)\n",
    "    if len(source_to_idx) == 0:\n",
    "        raise RuntimeError(\"No data sources found for selected phases\")\n",
    "\n",
    "    model = ChunkEncoder(n_sources=len(source_to_idx), z_dim=cfg[\"z_dim\"]).to(device)\n",
    "    base_lr = float(cfg[\"lr\"])\n",
    "    style_lr_mult = float(cfg.get(\"style_lr_mult\", 1.0))\n",
    "    adv_lr_mult = float(cfg.get(\"adv_lr_mult\", 1.0))\n",
    "    backbone_lr_mult = float(cfg.get(\"backbone_lr_mult\", 1.0))\n",
    "    music_lr_mult = float(cfg.get(\"music_lr_mult\", 1.0))\n",
    "\n",
    "    backbone_params = list(model.backbone.parameters()) + list(model.shared.parameters())\n",
    "    content_params = list(model.content_head.parameters())\n",
    "    music_params = list(model.music_head.parameters())\n",
    "    style_params = list(model.style_head.parameters()) + list(model.style_cls.parameters())\n",
    "    adv_params = list(model.content_style_adv.parameters())\n",
    "\n",
    "    known_ids = {id(p) for p in (backbone_params + content_params + music_params + style_params + adv_params)}\n",
    "    other_params = [p for p in model.parameters() if id(p) not in known_ids]\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": base_lr * backbone_lr_mult, \"name\": \"backbone\"},\n",
    "            {\"params\": content_params, \"lr\": base_lr, \"name\": \"content\"},\n",
    "            {\"params\": music_params, \"lr\": base_lr * music_lr_mult, \"name\": \"music\"},\n",
    "            {\"params\": style_params, \"lr\": base_lr * style_lr_mult, \"name\": \"style\"},\n",
    "            {\"params\": adv_params, \"lr\": base_lr * adv_lr_mult, \"name\": \"adv\"},\n",
    "            {\"params\": other_params, \"lr\": base_lr, \"name\": \"other\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optional warm-start in fresh mode (e.g., branch-merge or phase-specific fine-tune).\n",
    "    init_ckpt = cfg.get(\"init_checkpoint\", None)\n",
    "    if mode == \"fresh\" and init_ckpt:\n",
    "        init_path = Path(str(init_ckpt))\n",
    "        if not init_path.exists():\n",
    "            raise FileNotFoundError(f\"init_checkpoint not found: {init_path}\")\n",
    "        init_payload = torch.load(str(init_path), map_location=\"cpu\")\n",
    "        init_state = init_payload.get(\"model\", init_payload)\n",
    "        missing, unexpected = model.load_state_dict(init_state, strict=False)\n",
    "        print(\n",
    "            f\"[init] loaded model warm-start from {init_path} | \"\n",
    "            f\"missing={len(missing)} unexpected={len(unexpected)}\"\n",
    "        )\n",
    "\n",
    "    # Optional teacher anchor to prevent content drift during phase-3 sharpening.\n",
    "    teacher_model = None\n",
    "    if bool(cfg.get(\"use_teacher_anchor\", False)):\n",
    "        teacher_ckpt = cfg.get(\"teacher_anchor_checkpoint\", init_ckpt)\n",
    "        if teacher_ckpt is None:\n",
    "            raise ValueError(\"use_teacher_anchor=True requires teacher_anchor_checkpoint or init_checkpoint.\")\n",
    "        teacher_path = Path(str(teacher_ckpt))\n",
    "        if not teacher_path.exists():\n",
    "            raise FileNotFoundError(f\"teacher anchor checkpoint not found: {teacher_path}\")\n",
    "        teacher_payload = torch.load(str(teacher_path), map_location=\"cpu\")\n",
    "        teacher_state = teacher_payload.get(\"model\", teacher_payload)\n",
    "        teacher_model = ChunkEncoder(n_sources=len(source_to_idx), z_dim=cfg[\"z_dim\"]).to(device)\n",
    "        m2, u2 = teacher_model.load_state_dict(teacher_state, strict=False)\n",
    "        teacher_model.eval()\n",
    "        for p in teacher_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(\n",
    "            f\"[anchor] teacher loaded from {teacher_path} | \"\n",
    "            f\"missing={len(m2)} unexpected={len(u2)}\"\n",
    "        )\n",
    "\n",
    "    run_dir = Path.cwd() / \"saves\" / savename\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    history_csv = run_dir / \"history.csv\"\n",
    "\n",
    "    if mode == \"fresh\":\n",
    "        # archive previous run with same savename to keep each fresh run separate\n",
    "        if (run_dir / \"latest.pt\").exists() or (run_dir / \"history.csv\").exists():\n",
    "            stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            archived = run_dir.parent / f\"{savename}_archived_{stamp}\"\n",
    "            run_dir.rename(archived)\n",
    "            run_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Archived old run to: {archived}\")\n",
    "\n",
    "        train_state = {\n",
    "            \"savename\": savename,\n",
    "            \"mode\": \"fresh\",\n",
    "            \"next_phase_idx\": 0,\n",
    "            \"next_epoch\": 1,\n",
    "            \"next_step\": 1,\n",
    "            \"global_step\": 0,\n",
    "            \"history_rows\": 0,\n",
    "        }\n",
    "        history_df = pd.DataFrame()\n",
    "\n",
    "        save_checkpoint(\n",
    "            run_dir,\n",
    "            \"init.pt\",\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_state=train_state,\n",
    "            cfg=cfg,\n",
    "            source_to_idx=source_to_idx,\n",
    "        )\n",
    "    else:\n",
    "        payload = load_latest(run_dir, model, optimizer)\n",
    "        train_state = payload[\"train_state\"]\n",
    "        if history_csv.exists():\n",
    "            history_df = pd.read_csv(history_csv)\n",
    "        else:\n",
    "            history_df = pd.DataFrame()\n",
    "\n",
    "    print(f\"Run dir: {run_dir}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Resume state: phase_idx={train_state['next_phase_idx']} epoch={train_state['next_epoch']} step={train_state['next_step']} global_step={train_state['global_step']}\")\n",
    "\n",
    "    # fixed examples for before/after comparison\n",
    "    examples_path = run_dir / \"examples_manifest.csv\"\n",
    "    pre_examples_path = run_dir / \"examples_before.csv\"\n",
    "    post_examples_path = run_dir / \"examples_after.csv\"\n",
    "    compare_examples_path = run_dir / \"examples_compare.csv\"\n",
    "\n",
    "    if mode == \"resume\" and examples_path.exists():\n",
    "        examples_df = pd.read_csv(examples_path)\n",
    "    else:\n",
    "        examples_df = build_example_manifest(source_to_idx, max_per_source=EXAMPLE_MAX_PER_SOURCE, seed=cfg[\"seed\"])\n",
    "        examples_df.to_csv(examples_path, index=False)\n",
    "\n",
    "    pre_examples_df = evaluate_model_examples(\n",
    "        model,\n",
    "        device,\n",
    "        examples_df=examples_df,\n",
    "        sample_rate=cfg[\"sample_rate\"],\n",
    "        sample_seconds=cfg[\"chunk_seconds\"],\n",
    "    )\n",
    "    pre_examples_df.to_csv(pre_examples_path, index=False)\n",
    "    print(f\"Saved before-training examples: {pre_examples_path} ({len(pre_examples_df)} rows)\")\n",
    "    if len(pre_examples_df):\n",
    "        display(pre_examples_df)\n",
    "\n",
    "    phase_cache = {}\n",
    "\n",
    "    start_phase_idx = int(train_state[\"next_phase_idx\"])\n",
    "    start_epoch = int(train_state[\"next_epoch\"])\n",
    "    start_step = int(train_state[\"next_step\"])\n",
    "\n",
    "    for p_idx in range(start_phase_idx, len(phase_order)):\n",
    "        phase = int(phase_order[p_idx])\n",
    "\n",
    "        if run_until_phase is not None and phase > int(run_until_phase):\n",
    "            print(f\"Stopping before phase {phase} due to RUN_UNTIL_PHASE={run_until_phase}\")\n",
    "            break\n",
    "\n",
    "        if phase not in phase_cache:\n",
    "            try:\n",
    "                phase_cache[phase] = build_phase_cache(\n",
    "                    phase,\n",
    "                    source_to_idx,\n",
    "                    cfg,\n",
    "                    phase3_enable_hard_negatives=False,\n",
    "                )\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"[SKIP] Phase {phase}: {e}\")\n",
    "                continue\n",
    "\n",
    "        cache = phase_cache[phase]\n",
    "        if len(cache[\"train_chunk_df\"]) == 0:\n",
    "            print(f\"[SKIP] Phase {phase}: no training chunks\")\n",
    "            continue\n",
    "\n",
    "        total_epochs = int(cfg[\"epochs_per_phase\"].get(phase, 1))\n",
    "        epoch_from = start_epoch if p_idx == start_phase_idx else 1\n",
    "\n",
    "        print(f\"\\n[Phase {phase}] files={len(cache['phase_df']):,} train_chunks={len(cache['train_chunk_df']):,} val_chunks={len(cache['val_chunk_df']):,} epochs={total_epochs}\")\n",
    "\n",
    "        if phase == 3 and len(cache[\"phase_df\"]) > 0:\n",
    "            p3 = cache[\"phase_df\"][\"is_music\"].value_counts().to_dict()\n",
    "            print(f\"phase3 balance (is_music): {p3}\")\n",
    "\n",
    "        set_phase_trainable(model, phase=phase, cfg=cfg)\n",
    "        trainable_n = int(sum(p.requires_grad for p in model.parameters()))\n",
    "        total_n = int(sum(1 for _ in model.parameters()))\n",
    "        print(f\"trainable params tensors: {trainable_n}/{total_n}\")\n",
    "\n",
    "        phase3_last_n = int(cfg.get(\"phase3_hard_negative_last_n_epochs\", 0))\n",
    "        phase3_hard_start = max(1, total_epochs - phase3_last_n + 1) if phase3_last_n > 0 else total_epochs + 1\n",
    "        current_hn_state = None\n",
    "\n",
    "        for epoch in range(epoch_from, total_epochs + 1):\n",
    "            if phase == 3 and phase3_last_n > 0:\n",
    "                use_hn = bool(epoch >= phase3_hard_start)\n",
    "                if current_hn_state is None or use_hn != current_hn_state:\n",
    "                    phase_cache[phase] = build_phase_cache(\n",
    "                        phase,\n",
    "                        source_to_idx,\n",
    "                        cfg,\n",
    "                        phase3_enable_hard_negatives=use_hn,\n",
    "                    )\n",
    "                    cache = phase_cache[phase]\n",
    "                    current_hn_state = use_hn\n",
    "                    print(\n",
    "                        f\"[phase3] epoch {epoch}/{total_epochs} hard_negatives={'ON' if use_hn else 'OFF'} \"\n",
    "                        f\"train_chunks={len(cache['train_chunk_df'])}\"\n",
    "                    )\n",
    "\n",
    "            set_optimizer_lrs_for_phase(\n",
    "                optimizer=optimizer,\n",
    "                cfg=cfg,\n",
    "                phase=phase,\n",
    "                epoch=epoch,\n",
    "                total_epochs=total_epochs,\n",
    "            )\n",
    "            train_loader, val_loader = make_epoch_loaders(cache[\"train_ds\"], cache[\"val_ds\"], cfg, phase, epoch)\n",
    "\n",
    "            max_train_steps = cfg[\"max_train_steps_per_epoch\"]\n",
    "            if max_train_steps is None:\n",
    "                target_steps = len(train_loader)\n",
    "            else:\n",
    "                target_steps = min(int(max_train_steps), len(train_loader))\n",
    "\n",
    "            epoch_start_step = start_step if (p_idx == start_phase_idx and epoch == epoch_from) else 1\n",
    "            if epoch_start_step > target_steps:\n",
    "                epoch_start_step = 1\n",
    "\n",
    "            model.train(True)\n",
    "            train_acc = {\n",
    "                \"content\": 0.0,\n",
    "                \"style\": 0.0,\n",
    "                \"music\": 0.0,\n",
    "                \"content_adv\": 0.0,\n",
    "                \"content_l1\": 0.0,\n",
    "                \"music_bias\": 0.0,\n",
    "                \"anchor\": 0.0,\n",
    "                \"total\": 0.0,\n",
    "                \"music_pos_weight\": 0.0,\n",
    "                \"music_skipped\": 0.0,\n",
    "            }\n",
    "            seen = 0\n",
    "            grl_lambda = cfg.get(\"phase_grl_lambda\", {}).get(phase, cfg.get(\"grl_lambda\", 1.0))\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_loader, start=1):\n",
    "                if batch_idx < epoch_start_step:\n",
    "                    continue\n",
    "                if batch_idx > target_steps:\n",
    "                    break\n",
    "\n",
    "                log_mel = batch[\"log_mel\"].to(device, non_blocking=True)\n",
    "                log_mel_aug = batch[\"log_mel_aug\"].to(device, non_blocking=True)\n",
    "                source_idx = batch[\"source_idx\"].to(device, non_blocking=True)\n",
    "                is_music = batch[\"is_music\"].to(device, non_blocking=True)\n",
    "\n",
    "                out_a = model(log_mel, grl_lambda=grl_lambda)\n",
    "                out_b = model(log_mel_aug, grl_lambda=grl_lambda)\n",
    "                phase_weights = cfg.get(\"phase_loss_weights\", {}).get(phase, cfg[\"loss_weights\"])\n",
    "                teacher_out_a = None\n",
    "                teacher_out_b = None\n",
    "                if (\n",
    "                    teacher_model is not None\n",
    "                    and int(phase) == 3\n",
    "                    and float(phase_weights.get(\"anchor\", 0.0)) > 0.0\n",
    "                ):\n",
    "                    with torch.no_grad():\n",
    "                        teacher_out_a = teacher_model(log_mel, grl_lambda=0.0)\n",
    "                        teacher_out_b = teacher_model(log_mel_aug, grl_lambda=0.0)\n",
    "                loss, parts = compute_losses(\n",
    "                    out_a,\n",
    "                    out_b,\n",
    "                    source_idx,\n",
    "                    is_music,\n",
    "                    phase_weights,\n",
    "                    teacher_out_a=teacher_out_a,\n",
    "                    teacher_out_b=teacher_out_b,\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                for k in train_acc:\n",
    "                    train_acc[k] += parts[k]\n",
    "                seen += 1\n",
    "\n",
    "                train_state[\"global_step\"] += 1\n",
    "                train_state[\"next_phase_idx\"] = p_idx\n",
    "                train_state[\"next_epoch\"] = epoch\n",
    "                train_state[\"next_step\"] = batch_idx + 1\n",
    "\n",
    "                step_file = f\"step_p{phase}_e{epoch:04d}_b{batch_idx:05d}_g{train_state['global_step']:08d}.pt\"\n",
    "                save_checkpoint(run_dir, step_file, model, optimizer, train_state, cfg, source_to_idx)\n",
    "\n",
    "                if (batch_idx % cfg[\"print_every_steps\"]) == 0:\n",
    "                    print(\n",
    "                        f\"step {p_idx+1}/{len(phase_order)} | epoch {epoch}/{total_epochs} | \"\n",
    "                        f\"batch {batch_idx}/{target_steps} | global_step {train_state['global_step']} | \"\n",
    "                        f\"loss_total {parts['total']:.4f}\"\n",
    "                    )\n",
    "\n",
    "            train_stats = {k: (train_acc[k] / seen if seen > 0 else float('nan')) for k in train_acc}\n",
    "            phase_weights = cfg.get(\"phase_loss_weights\", {}).get(phase, cfg[\"loss_weights\"])\n",
    "            val_stats = run_validation(\n",
    "                model,\n",
    "                val_loader,\n",
    "                device,\n",
    "                max_steps=cfg[\"max_val_steps_per_epoch\"],\n",
    "                weights=phase_weights,\n",
    "                grl_lambda=grl_lambda,\n",
    "                teacher_model=(teacher_model if int(phase) == 3 else None),\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"phase\": phase,\n",
    "                \"epoch\": epoch,\n",
    "                \"pipeline_step\": p_idx + 1,\n",
    "                **{f\"train_{k}\": v for k, v in train_stats.items()},\n",
    "                **{f\"val_{k}\": v for k, v in val_stats.items()},\n",
    "                \"global_step\": train_state[\"global_step\"],\n",
    "            }\n",
    "            history_df = pd.concat([history_df, pd.DataFrame([row])], ignore_index=True)\n",
    "            history_df.to_csv(history_csv, index=False)\n",
    "\n",
    "            # advance pointer to next epoch/phase\n",
    "            if epoch < total_epochs:\n",
    "                train_state[\"next_phase_idx\"] = p_idx\n",
    "                train_state[\"next_epoch\"] = epoch + 1\n",
    "                train_state[\"next_step\"] = 1\n",
    "            else:\n",
    "                train_state[\"next_phase_idx\"] = p_idx + 1\n",
    "                train_state[\"next_epoch\"] = 1\n",
    "                train_state[\"next_step\"] = 1\n",
    "\n",
    "            train_state[\"history_rows\"] = int(len(history_df))\n",
    "\n",
    "            epoch_file = f\"epoch_p{phase}_e{epoch:04d}_g{train_state['global_step']:08d}.pt\"\n",
    "            save_checkpoint(run_dir, epoch_file, model, optimizer, train_state, cfg, source_to_idx)\n",
    "\n",
    "            print(\n",
    "                f\"[epoch done] step {p_idx+1}/{len(phase_order)} epoch {epoch}/{total_epochs} | \"\n",
    "                f\"train_total={train_stats['total']:.4f} val_total={val_stats['total']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # reset resume offsets after first resumed phase is consumed\n",
    "        start_epoch = 1\n",
    "        start_step = 1\n",
    "\n",
    "        if run_until_phase is not None and phase == int(run_until_phase):\n",
    "            print(f\"Reached RUN_UNTIL_PHASE={run_until_phase}. Stopping cleanly.\")\n",
    "            break\n",
    "\n",
    "    post_examples_df = evaluate_model_examples(\n",
    "        model,\n",
    "        device,\n",
    "        examples_df=examples_df,\n",
    "        sample_rate=cfg[\"sample_rate\"],\n",
    "        sample_seconds=cfg[\"chunk_seconds\"],\n",
    "    )\n",
    "    post_examples_df.to_csv(post_examples_path, index=False)\n",
    "\n",
    "    compare_df = render_example_comparison(pre_examples_df, post_examples_df)\n",
    "    compare_df.to_csv(compare_examples_path, index=False)\n",
    "\n",
    "    print(f\"Saved after-training examples: {post_examples_path} ({len(post_examples_df)} rows)\")\n",
    "    print(f\"Saved before-vs-after compare: {compare_examples_path} ({len(compare_df)} rows)\")\n",
    "    if len(compare_df):\n",
    "        display(compare_df)\n",
    "\n",
    "    complete = int(train_state[\"next_phase_idx\"]) >= len(phase_order)\n",
    "    print(\"\\nTraining status:\", \"COMPLETE\" if complete else \"PARTIAL\")\n",
    "    print(f\"Next resume pointer: phase_idx={train_state['next_phase_idx']} epoch={train_state['next_epoch']} step={train_state['next_step']}\")\n",
    "\n",
    "    return model, history_df, run_dir\n",
    "\n",
    "\n",
    "# ---- training config ----\n",
    "CFG = {\n",
    "    \"seed\": SEED,\n",
    "    \"sample_rate\": 22050,\n",
    "    \"chunk_seconds\": 5.0,\n",
    "    \"stride_seconds\": 2.5,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 0,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"max_files_per_source\": 120,\n",
    "    \"max_chunks_per_file\": 6,\n",
    "    \"phase_order\": [1, 2, 3],\n",
    "    \"epochs_per_phase\": {1: 50, 2: 50, 3: 20},\n",
    "    \"max_train_steps_per_epoch\": 60,\n",
    "    \"max_val_steps_per_epoch\": 20,\n",
    "    \"z_dim\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"loss_weights\": {\n",
    "        \"content\": 1.0,\n",
    "        \"style\": 0.8,\n",
    "        \"music\": 3.5,\n",
    "        \"content_adv\": 0.55,\n",
    "        \"content_l1\": 0.0007,\n",
    "        \"music_bias\": 0.0005,\n",
    "        \"music_only_when_mixed\": True,\n",
    "    },\n",
    "    \"phase_loss_weights\": {\n",
    "        1: {\n",
    "            \"content\": 1.0,\n",
    "            \"style\": 0.55,\n",
    "            \"music\": 0.0,\n",
    "            \"content_adv\": 0.30,\n",
    "            \"content_l1\": 0.0008,\n",
    "            \"music_bias\": 0.0,\n",
    "            \"music_only_when_mixed\": True,\n",
    "        },\n",
    "        2: {\n",
    "            \"content\": 1.0,\n",
    "            \"style\": 0.60,\n",
    "            \"music\": 0.0,\n",
    "            \"content_adv\": 1.00,\n",
    "            \"content_l1\": 0.0012,\n",
    "            \"music_bias\": 0.0,\n",
    "            \"music_only_when_mixed\": True,\n",
    "        },\n",
    "        3: {\n",
    "            \"content\": 0.8,\n",
    "            \"style\": 0.35,\n",
    "            \"music\": 4.0,\n",
    "            \"content_adv\": 0.60,\n",
    "            \"content_l1\": 0.0008,\n",
    "            \"music_bias\": 0.0005,\n",
    "            \"music_only_when_mixed\": True,\n",
    "        },\n",
    "    },\n",
    "    \"grl_lambda\": 1.00,\n",
    "    \"phase_grl_lambda\": {1: 0.25, 2: 1.00, 3: 0.80},\n",
    "    \"phase3_music_head_only\": False,\n",
    "    \"phase3_train_mode\": \"full\",\n",
    "    \"use_teacher_anchor\": False,\n",
    "    \"teacher_anchor_checkpoint\": None,\n",
    "    \"phase3_hard_negative_last_n_epochs\": PHASE3_HARD_NEGATIVE_LAST_N_EPOCHS,\n",
    "    \"phase3_backbone_lr_scale\": 0.05,\n",
    "    \"backbone_lr_mult\": 1.0,\n",
    "    \"music_lr_mult\": 2.5,\n",
    "    \"style_lr_mult\": 2.5,\n",
    "    \"adv_lr_mult\": 0.6,\n",
    "    \"print_every_steps\": 1,\n",
    "}\n",
    "\n",
    "if torch is None:\n",
    "    print(\"Torch missing. Install dependencies and re-run.\")\n",
    "elif RUN_TRAINING:\n",
    "    model, train_history, save_dir = train_curriculum_resumable(\n",
    "        cfg=CFG,\n",
    "        savename=SAVENAME,\n",
    "        mode=MODE,\n",
    "        run_until_phase=RUN_UNTIL_PHASE,\n",
    "    )\n",
    "    print(\"save_dir:\", save_dir)\n",
    "    print(\"history_rows:\", len(train_history))\n",
    "    if len(train_history):\n",
    "        display(train_history.tail(10))\n",
    "else:\n",
    "    print(\"RUN_TRAINING is False. Set it True after SAVENAME/MODE are configured.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585b8c4",
   "metadata": {},
   "source": [
    "## Stage 3: Curriculum Training And Checkpointing\n",
    "\n",
    "Training behavior in this notebook:\n",
    "- `SAVENAME` selects the run folder under `./saves/`.\n",
    "- `MODE=\"fresh\"` starts a new run; `MODE=\"resume\"` continues from `latest.pt`.\n",
    "- Step/epoch checkpoints are saved under `./saves/<SAVENAME>/checkpoints/`.\n",
    "- Resume pointer is maintained in `./saves/<SAVENAME>/run_state.json`.\n",
    "- Before/after embedding snapshots are saved as:\n",
    "  - `examples_before.csv`\n",
    "  - `examples_after.csv`\n",
    "  - `examples_compare.csv`\n",
    "\n",
    "This lets us measure not only losses, but behavioral drift between early and late training states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc9857",
   "metadata": {},
   "source": [
    "## Stage 4: Disentanglement Audit (Notebook-Only)\n",
    "\n",
    "This section implements Lab 1 validation directly in-notebook:\n",
    "\n",
    "1. **Invariance audit**: dual-soundfont renders should map to nearly identical `z_content`.\n",
    "2. **Leakage probe**: linear probe on `z_content` should be near chance; `z_style` should remain discriminative.\n",
    "3. **Gate scaling audit**: measure speech false positives, AUC, and recall at calibrated low-FPR operating points.\n",
    "\n",
    "All outputs are written to run-specific audit directories for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01a6447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit helpers loaded.\n"
     ]
    }
   ],
   "source": [
    "# Notebook-only audit helpers (shared by all Lab 1 audits)\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'torch' not in globals():\n",
    "    import torch\n",
    "if 'nn' not in globals():\n",
    "    import torch.nn as nn\n",
    "if 'F' not in globals():\n",
    "    import torch.nn.functional as F\n",
    "if 'librosa' not in globals():\n",
    "    import librosa\n",
    "if 'sf' not in globals():\n",
    "    import soundfile as sf\n",
    "if 'pretty_midi' not in globals():\n",
    "    import pretty_midi\n",
    "\n",
    "\n",
    "def _audit_device(device='auto'):\n",
    "    if device == 'auto':\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        raise RuntimeError('CUDA requested but unavailable.')\n",
    "    return device\n",
    "\n",
    "\n",
    "def _ensure_chunk_encoder_defined():\n",
    "    if \"ChunkEncoder\" in globals():\n",
    "        return globals()[\"ChunkEncoder\"]\n",
    "\n",
    "    class _GradientReversal(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x, lambda_):\n",
    "            ctx.lambda_ = float(lambda_)\n",
    "            return x.view_as(x)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "    def grad_reverse(x: torch.Tensor, lambda_: float = 1.0) -> torch.Tensor:\n",
    "        return _GradientReversal.apply(x, lambda_)\n",
    "\n",
    "    class ChunkEncoder(nn.Module):\n",
    "        def __init__(self, n_sources: int, z_dim: int = 128):\n",
    "            super().__init__()\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            )\n",
    "            self.shared = nn.Linear(128, 256)\n",
    "            self.content_head = nn.Linear(256, z_dim)\n",
    "            self.style_head = nn.Linear(256, z_dim)\n",
    "            self.style_cls = nn.Linear(z_dim, n_sources)\n",
    "            self.content_style_adv = nn.Sequential(\n",
    "                nn.Linear(z_dim, z_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(z_dim, n_sources),\n",
    "            )\n",
    "            self.music_head = nn.Linear(256, 1)\n",
    "\n",
    "        def forward(self, log_mel: torch.Tensor, grl_lambda: float = 1.0):\n",
    "            x = log_mel.unsqueeze(1)\n",
    "            h = self.backbone(x).flatten(1)\n",
    "            h = F.relu(self.shared(h))\n",
    "            z_content = F.normalize(self.content_head(h), dim=-1)\n",
    "            z_style = F.normalize(self.style_head(h), dim=-1)\n",
    "            z_content_rev = grad_reverse(z_content, lambda_=grl_lambda)\n",
    "            return {\n",
    "                \"z_content\": z_content,\n",
    "                \"z_style\": z_style,\n",
    "                \"style_logits\": self.style_cls(z_style),\n",
    "                \"content_style_logits\": self.content_style_adv(z_content_rev),\n",
    "                \"music_logit\": self.music_head(h).squeeze(-1),\n",
    "            }\n",
    "\n",
    "    globals()[\"ChunkEncoder\"] = ChunkEncoder\n",
    "    return ChunkEncoder\n",
    "\n",
    "\n",
    "def _load_audio_chunk(path: str, start_sec: float, duration_sec: float, sample_rate: int):\n",
    "    if 'load_audio_chunk_48k' in globals():\n",
    "        return load_audio_chunk_48k(path, start_sec, duration_sec, sample_rate=sample_rate)\n",
    "\n",
    "    y, _ = librosa.load(\n",
    "        path,\n",
    "        sr=sample_rate,\n",
    "        mono=True,\n",
    "        offset=max(0.0, float(start_sec)),\n",
    "        duration=float(duration_sec),\n",
    "        dtype=np.float32,\n",
    "        res_type='soxr_hq',\n",
    "    )\n",
    "    target_len = int(round(duration_sec * sample_rate))\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)), mode='constant')\n",
    "    elif len(y) > target_len:\n",
    "        y = y[:target_len]\n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f'Empty audio chunk: {path}')\n",
    "    y = librosa.util.normalize(y)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def _extract_log_mel(y: np.ndarray, sr: int):\n",
    "    if 'extract_log_mel_fast' in globals():\n",
    "        return extract_log_mel_fast(y, sr=sr)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        n_mels=96,\n",
    "        fmin=20,\n",
    "        fmax=sr // 2,\n",
    "        power=2.0,\n",
    "    )\n",
    "    return librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
    "\n",
    "\n",
    "def lab1_load_checkpoint_for_audit(\n",
    "    checkpoint_path=Path('saves/lab1_run_a/latest.pt'),\n",
    "    device='auto',\n",
    "):\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f'Checkpoint not found: {checkpoint_path}')\n",
    "\n",
    "    device = _audit_device(device)\n",
    "    payload = torch.load(str(checkpoint_path), map_location='cpu')\n",
    "    cfg = payload.get('cfg', {})\n",
    "    source_to_idx = payload.get('source_to_idx', {})\n",
    "    if not source_to_idx:\n",
    "        raise ValueError('Checkpoint missing source_to_idx.')\n",
    "\n",
    "    ChunkEncoderCls = _ensure_chunk_encoder_defined()\n",
    "    model = ChunkEncoderCls(n_sources=len(source_to_idx), z_dim=int(cfg.get('z_dim', 128))).to(device)\n",
    "    model.load_state_dict(payload['model'])\n",
    "    model.eval()\n",
    "    return model, cfg, source_to_idx, device\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def lab1_infer_file(\n",
    "    model,\n",
    "    path,\n",
    "    sample_rate,\n",
    "    sample_seconds,\n",
    "    start_sec=0.0,\n",
    "    device='cpu',\n",
    "):\n",
    "    y = _load_audio_chunk(str(path), start_sec=float(start_sec), duration_sec=float(sample_seconds), sample_rate=int(sample_rate))\n",
    "    mel = _extract_log_mel(y, sr=int(sample_rate))\n",
    "    x = torch.from_numpy(mel).unsqueeze(0).to(device, non_blocking=True)\n",
    "    out = model(x)\n",
    "\n",
    "    style_probs = torch.softmax(out['style_logits'], dim=-1)[0].detach().cpu().numpy()\n",
    "    return {\n",
    "        'z_content': out['z_content'][0].detach().cpu().numpy().astype(np.float32),\n",
    "        'z_style': out['z_style'][0].detach().cpu().numpy().astype(np.float32),\n",
    "        'music_prob': float(torch.sigmoid(out['music_logit'])[0].item()),\n",
    "        'style_probs': style_probs.astype(np.float32),\n",
    "    }\n",
    "\n",
    "\n",
    "def _read_manifest(path, force_source=None):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Manifest not found: {path}')\n",
    "    df = pd.read_csv(path)\n",
    "    if 'path' not in df.columns:\n",
    "        raise ValueError(f\"Manifest missing 'path' column: {path}\")\n",
    "    if force_source is not None:\n",
    "        df['source'] = force_source\n",
    "    if 'source' not in df.columns:\n",
    "        df['source'] = 'unknown'\n",
    "    df = df[df['path'].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "print('Audit helpers loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e2622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual render + invariance audit functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Phase 1 expansion: dual render + z_content invariance audit\n",
    "\n",
    "\n",
    "def _has_fluidsynth():\n",
    "    return shutil.which('fluidsynth') is not None\n",
    "\n",
    "\n",
    "def _safe_name(text: str, max_len: int = 90):\n",
    "    import re\n",
    "    text = re.sub(r'[^A-Za-z0-9._-]+', '_', text).strip('._')\n",
    "    return (text or 'item')[:max_len]\n",
    "\n",
    "\n",
    "def _render_midi_to_wav(midi_path: Path, wav_path: Path, rate: int, engine: str, soundfont: Path, gain: float = 0.7):\n",
    "    wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if engine == 'fluidsynth':\n",
    "        cmd = [\n",
    "            'fluidsynth', '-ni', '-F', str(wav_path), '-T', 'wav', '-r', str(rate), '-g', str(gain), str(soundfont), str(midi_path)\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    audio = pm.synthesize(fs=int(rate))\n",
    "    sf.write(str(wav_path), audio, int(rate), subtype='PCM_16')\n",
    "\n",
    "\n",
    "def lab1_render_dual_soundfont_pdmx(\n",
    "    manifests_root=Path(r'Z:/DataSets/_lab1_manifests'),\n",
    "    output_root=Path(r'Z:/DataSets/rendered/phase1_pdmx_dual_soundfont'),\n",
    "    soundfont_a=Path(r'Z:/DataSets/soundfonts/MuseScore_General.sf3'),\n",
    "    soundfont_b=Path(r'Z:/DataSets/soundfonts/TimGM6mb.sf2'),\n",
    "    max_pdmx=500,\n",
    "    seed=328,\n",
    "    rate=48000,\n",
    "    gain=0.7,\n",
    "    force=False,\n",
    "    out_manifest=Path(r'Z:/DataSets/_lab1_manifests/phase1_pdmx_dual_render_manifest.csv'),\n",
    "):\n",
    "    pdmx_manifest = manifests_root / 'pdmx_no_license_conflict_manifest.csv'\n",
    "    if not pdmx_manifest.exists():\n",
    "        raise FileNotFoundError(f'Missing {pdmx_manifest}')\n",
    "\n",
    "    df = pd.read_csv(pdmx_manifest)\n",
    "    if 'mid_path' not in df.columns:\n",
    "        raise ValueError(\"PDMX manifest must include 'mid_path'.\")\n",
    "\n",
    "    if 'exists_mid_path' in df.columns:\n",
    "        df = df[df['exists_mid_path'] == True]  # noqa: E712\n",
    "\n",
    "    df['mid_path'] = df['mid_path'].astype(str)\n",
    "    df = df[df['mid_path'].map(lambda p: Path(p).exists())].reset_index(drop=True)\n",
    "    if len(df) == 0:\n",
    "        raise RuntimeError('No valid MIDI rows after filtering.')\n",
    "\n",
    "    take_n = min(int(max_pdmx), len(df))\n",
    "    df = df.sample(take_n, random_state=int(seed)).reset_index(drop=True)\n",
    "\n",
    "    engine = 'fluidsynth' if (_has_fluidsynth() and soundfont_a.exists() and soundfont_b.exists()) else 'pretty_midi'\n",
    "    if engine == 'pretty_midi':\n",
    "        print('[WARN] Falling back to pretty_midi. This weakens the style-swap invariance test.')\n",
    "\n",
    "    rows = []\n",
    "    failures = 0\n",
    "    for i, r in df.iterrows():\n",
    "        midi_path = Path(str(r['mid_path']))\n",
    "        pair_id = hashlib.sha1(str(midi_path).encode('utf-8')).hexdigest()[:12]\n",
    "        base = _safe_name(midi_path.stem)\n",
    "\n",
    "        wav_a = output_root / 'sf_a' / f'{pair_id}_{base}.wav'\n",
    "        wav_b = output_root / 'sf_b' / f'{pair_id}_{base}.wav'\n",
    "\n",
    "        try:\n",
    "            if force or not wav_a.exists():\n",
    "                _render_midi_to_wav(midi_path, wav_a, rate=rate, engine=engine, soundfont=soundfont_a, gain=gain)\n",
    "            if force or not wav_b.exists():\n",
    "                _render_midi_to_wav(midi_path, wav_b, rate=rate, engine=engine, soundfont=soundfont_b, gain=gain)\n",
    "\n",
    "            rows.append({\n",
    "                'pair_id': pair_id,\n",
    "                'midi_path': str(midi_path),\n",
    "                'wav_a': str(wav_a),\n",
    "                'wav_b': str(wav_b),\n",
    "                'soundfont_a': str(soundfont_a),\n",
    "                'soundfont_b': str(soundfont_b),\n",
    "                'engine': engine,\n",
    "            })\n",
    "        except Exception as exc:\n",
    "            failures += 1\n",
    "            print(f'[WARN] dual render failed ({i}): {midi_path} :: {exc}')\n",
    "\n",
    "    out_df = pd.DataFrame(rows).drop_duplicates(subset=['pair_id']).reset_index(drop=True)\n",
    "    out_manifest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_df.to_csv(out_manifest, index=False)\n",
    "\n",
    "    print('[DONE] Dual SoundFont rendering')\n",
    "    print('requested:', take_n, '| rendered:', len(out_df), '| failures:', failures)\n",
    "    print('manifest:', out_manifest)\n",
    "    return out_df, out_manifest\n",
    "\n",
    "\n",
    "def lab1_run_invariance_audit(\n",
    "    checkpoint_path=Path('saves/lab1_run_a/latest.pt'),\n",
    "    pair_manifest=Path(r'Z:/DataSets/_lab1_manifests/phase1_pdmx_dual_render_manifest.csv'),\n",
    "    threshold=0.92,\n",
    "    sample_rate=None,\n",
    "    sample_seconds=None,\n",
    "    start_sec=0.0,\n",
    "    max_pairs=None,\n",
    "    device='auto',\n",
    "    out_csv=Path('saves/lab1_run_a/audits/invariance_pairs.csv'),\n",
    "    out_json=Path('saves/lab1_run_a/audits/invariance_summary.json'),\n",
    "):\n",
    "    model, cfg, _, device = lab1_load_checkpoint_for_audit(checkpoint_path=checkpoint_path, device=device)\n",
    "    sr = int(sample_rate if sample_rate is not None else cfg.get('sample_rate', 22050))\n",
    "    sec = float(sample_seconds if sample_seconds is not None else cfg.get('chunk_seconds', 5.0))\n",
    "\n",
    "    pair_df = pd.read_csv(pair_manifest)\n",
    "    req_cols = {'pair_id', 'wav_a', 'wav_b'}\n",
    "    if not req_cols.issubset(set(pair_df.columns)):\n",
    "        raise ValueError(f'Pair manifest must include columns: {sorted(req_cols)}')\n",
    "    if max_pairs is not None:\n",
    "        pair_df = pair_df.head(int(max_pairs)).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for i, r in pair_df.iterrows():\n",
    "        wav_a = Path(str(r['wav_a']))\n",
    "        wav_b = Path(str(r['wav_b']))\n",
    "        if not wav_a.exists() or not wav_b.exists():\n",
    "            rows.append({'pair_id': r['pair_id'], 'wav_a': str(wav_a), 'wav_b': str(wav_b), 'cosine_content': np.nan, 'error': 'missing_file'})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            a = lab1_infer_file(model, wav_a, sample_rate=sr, sample_seconds=sec, start_sec=start_sec, device=device)\n",
    "            b = lab1_infer_file(model, wav_b, sample_rate=sr, sample_seconds=sec, start_sec=start_sec, device=device)\n",
    "            zc_a = a['z_content']\n",
    "            zc_b = b['z_content']\n",
    "            cos = float(np.dot(zc_a, zc_b) / (np.linalg.norm(zc_a) * np.linalg.norm(zc_b) + 1e-12))\n",
    "            rows.append({\n",
    "                'pair_id': r['pair_id'],\n",
    "                'wav_a': str(wav_a),\n",
    "                'wav_b': str(wav_b),\n",
    "                'cosine_content': cos,\n",
    "                'pass_threshold': bool(cos >= float(threshold)),\n",
    "                'music_prob_a': float(a['music_prob']),\n",
    "                'music_prob_b': float(b['music_prob']),\n",
    "                'error': '',\n",
    "            })\n",
    "        except Exception as exc:\n",
    "            rows.append({'pair_id': r['pair_id'], 'wav_a': str(wav_a), 'wav_b': str(wav_b), 'cosine_content': np.nan, 'error': str(exc)})\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'[INFO] processed {i + 1}/{len(pair_df)} pairs')\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    valid = out_df['cosine_content'].dropna().to_numpy(dtype=np.float64)\n",
    "    summary = {\n",
    "        'n_pairs': int(len(out_df)),\n",
    "        'n_valid': int(valid.size),\n",
    "        'threshold': float(threshold),\n",
    "        'pass_rate': float(np.mean(valid >= threshold)) if valid.size else float('nan'),\n",
    "        'mean_cosine': float(np.mean(valid)) if valid.size else float('nan'),\n",
    "        'median_cosine': float(np.median(valid)) if valid.size else float('nan'),\n",
    "        'min_cosine': float(np.min(valid)) if valid.size else float('nan'),\n",
    "        'p10_cosine': float(np.percentile(valid, 10)) if valid.size else float('nan'),\n",
    "        'p90_cosine': float(np.percentile(valid, 90)) if valid.size else float('nan'),\n",
    "    }\n",
    "\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    out_json.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "\n",
    "    print('[DONE] Invariance audit')\n",
    "    print('pairs valid:', summary['n_valid'], '/', summary['n_pairs'])\n",
    "    print('mean cosine:', f\"{summary['mean_cosine']:.4f}\")\n",
    "    print(f\"pass rate @ {threshold:.2f}: {summary['pass_rate']:.2%}\")\n",
    "    print('csv:', out_csv)\n",
    "    print('json:', out_json)\n",
    "    return out_df, summary\n",
    "\n",
    "\n",
    "print('Dual render + invariance audit functions loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44539503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage probe function loaded.\n"
     ]
    }
   ],
   "source": [
    "# Leakage probe: linear probes on z_content vs z_style\n",
    "\n",
    "\n",
    "def lab1_run_leakage_probe(\n",
    "    checkpoint_path=Path('saves/lab1_run_a/latest.pt'),\n",
    "    xtc_manifest=Path(r'Z:/DataSets/_lab1_manifests/xtc_audio_clean.csv'),\n",
    "    phase1_audio_manifest=Path(r'Z:/DataSets/_lab1_manifests/phase1_symbolic_audio_manifest.csv'),\n",
    "    n_per_class=1000,\n",
    "    strict_balance=True,\n",
    "    seed=328,\n",
    "    sample_rate=None,\n",
    "    sample_seconds=None,\n",
    "    start_sec=0.0,\n",
    "    device='auto',\n",
    "    leakage_threshold=0.15,\n",
    "    style_acc_threshold=0.85,\n",
    "    out_dir=Path('saves/lab1_run_a/audits'),\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model, cfg, _, device = lab1_load_checkpoint_for_audit(checkpoint_path=checkpoint_path, device=device)\n",
    "    sr = int(sample_rate if sample_rate is not None else cfg.get('sample_rate', 22050))\n",
    "    sec = float(sample_seconds if sample_seconds is not None else cfg.get('chunk_seconds', 5.0))\n",
    "\n",
    "    xtc = _read_manifest(xtc_manifest, force_source='xtc_hiphop')\n",
    "    xtc = xtc[xtc['path'].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "\n",
    "    phase1 = _read_manifest(phase1_audio_manifest)\n",
    "    phase1 = phase1[phase1['source'] == 'phase1_pdmx'].copy()\n",
    "    phase1 = phase1[phase1['path'].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "\n",
    "    if len(xtc) == 0 or len(phase1) == 0:\n",
    "        raise RuntimeError('Need non-empty XTc and phase1_pdmx pools.')\n",
    "\n",
    "    if strict_balance:\n",
    "        effective_n = min(int(n_per_class), len(xtc), len(phase1))\n",
    "        if effective_n < 10:\n",
    "            raise RuntimeError(\n",
    "                f'Not enough paired samples for strict_balance. xtc={len(xtc)}, phase1_pdmx={len(phase1)}'\n",
    "            )\n",
    "        xtc = xtc.sample(effective_n, random_state=int(seed)).reset_index(drop=True)\n",
    "        phase1 = phase1.sample(effective_n, random_state=int(seed)).reset_index(drop=True)\n",
    "    else:\n",
    "        xtc = xtc.sample(min(int(n_per_class), len(xtc)), random_state=int(seed)).reset_index(drop=True)\n",
    "        phase1 = phase1.sample(min(int(n_per_class), len(phase1)), random_state=int(seed)).reset_index(drop=True)\n",
    "\n",
    "    merged = pd.concat([\n",
    "        xtc.assign(label=0, label_name='xtc_hiphop'),\n",
    "        phase1.assign(label=1, label_name='phase1_pdmx'),\n",
    "    ], ignore_index=True).sample(frac=1.0, random_state=int(seed)).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    zc_list = []\n",
    "    zs_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i, r in merged.iterrows():\n",
    "        path = Path(str(r['path']))\n",
    "        try:\n",
    "            out = lab1_infer_file(model, path, sample_rate=sr, sample_seconds=sec, start_sec=start_sec, device=device)\n",
    "            zc_list.append(out['z_content'])\n",
    "            zs_list.append(out['z_style'])\n",
    "            y_list.append(int(r['label']))\n",
    "            rows.append({'path': str(path), 'source': str(r['source']), 'label': int(r['label']), 'music_prob': float(out['music_prob']), 'error': ''})\n",
    "        except Exception as exc:\n",
    "            rows.append({'path': str(path), 'source': str(r['source']), 'label': int(r['label']), 'music_prob': np.nan, 'error': str(exc)})\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'[INFO] embedded {i + 1}/{len(merged)}')\n",
    "\n",
    "    emb_df = pd.DataFrame(rows)\n",
    "    emb_df.to_csv(out_dir / 'leakage_embeddings_index.csv', index=False)\n",
    "\n",
    "    if len(y_list) < 100:\n",
    "        raise RuntimeError('Too few valid embeddings for probe training (<100).')\n",
    "\n",
    "    X_content = np.stack(zc_list, axis=0)\n",
    "    X_style = np.stack(zs_list, axis=0)\n",
    "    y = np.asarray(y_list, dtype=np.int64)\n",
    "\n",
    "    class_counts = {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))}\n",
    "    if len(class_counts) < 2 or min(class_counts.values()) < 10:\n",
    "        raise RuntimeError(f'Insufficient class coverage after embedding extraction. Class counts: {class_counts}')\n",
    "\n",
    "    def _fit_probe(X, y, random_seed=328):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_seed, stratify=y\n",
    "        )\n",
    "        clf = LogisticRegression(max_iter=3000, random_state=random_seed, solver='lbfgs')\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        acc = float(accuracy_score(y_test, pred))\n",
    "        cm = confusion_matrix(y_test, pred, labels=[0, 1])\n",
    "        return acc, cm\n",
    "\n",
    "    baseline = float(max(np.mean(y == 0), np.mean(y == 1)))\n",
    "    content_acc, cm_content = _fit_probe(X_content, y, random_seed=int(seed))\n",
    "    style_acc, cm_style = _fit_probe(X_style, y, random_seed=int(seed))\n",
    "    leakage = float(content_acc - baseline)\n",
    "\n",
    "    summary = {\n",
    "        'n_requested_total': int(len(merged)),\n",
    "        'n_valid_embeddings': int(len(y)),\n",
    "        'strict_balance': bool(strict_balance),\n",
    "        'n_per_class_target': int(n_per_class),\n",
    "        'n_per_class_effective_xtc': int(len(xtc)),\n",
    "        'n_per_class_effective_pdmx': int(len(phase1)),\n",
    "        'class_balance_xtc': int(np.sum(y == 0)),\n",
    "        'class_balance_pdmx': int(np.sum(y == 1)),\n",
    "        'baseline_accuracy': baseline,\n",
    "        'content_probe_accuracy': content_acc,\n",
    "        'style_probe_accuracy': style_acc,\n",
    "        'content_leakage_above_baseline': leakage,\n",
    "        'content_leakage_threshold': float(leakage_threshold),\n",
    "        'style_accuracy_threshold': float(style_acc_threshold),\n",
    "        'content_pass': bool(leakage <= float(leakage_threshold)),\n",
    "        'style_pass': bool(style_acc >= float(style_acc_threshold)),\n",
    "        'overall_pass': bool(leakage <= float(leakage_threshold) and style_acc >= float(style_acc_threshold)),\n",
    "        'content_confusion_matrix_rows_true_0_1_cols_pred_0_1': cm_content.tolist(),\n",
    "        'style_confusion_matrix_rows_true_0_1_cols_pred_0_1': cm_style.tolist(),\n",
    "    }\n",
    "\n",
    "    (out_dir / 'leakage_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "    pd.DataFrame(cm_content, index=['true_xtc', 'true_pdmx'], columns=['pred_xtc', 'pred_pdmx']).to_csv(out_dir / 'leakage_cm_content.csv')\n",
    "    pd.DataFrame(cm_style, index=['true_xtc', 'true_pdmx'], columns=['pred_xtc', 'pred_pdmx']).to_csv(out_dir / 'leakage_cm_style.csv')\n",
    "\n",
    "    print('[DONE] Leakage probe')\n",
    "    print('valid embeddings:', summary['n_valid_embeddings'])\n",
    "    print('baseline acc:', f\"{summary['baseline_accuracy']:.4f}\")\n",
    "    print('content acc:', f\"{summary['content_probe_accuracy']:.4f}\")\n",
    "    print('style acc:', f\"{summary['style_probe_accuracy']:.4f}\")\n",
    "    print('content leakage:', f\"{summary['content_leakage_above_baseline']:.4f}\")\n",
    "    print(f\"content pass <= {leakage_threshold:.2f}:\", summary['content_pass'])\n",
    "    print(f\"style pass >= {style_acc_threshold:.2f}:\", summary['style_pass'])\n",
    "    print('overall pass:', summary['overall_pass'])\n",
    "    print('out dir:', out_dir)\n",
    "\n",
    "    return emb_df, summary\n",
    "\n",
    "\n",
    "print('Leakage probe function loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a6000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate scaling function loaded.\n"
     ]
    }
   ],
   "source": [
    "# Music gate scaling: LibriSpeech vs music confusion matrix and FPR\n",
    "\n",
    "\n",
    "def lab1_run_gate_scaling_eval(\n",
    "    checkpoint_path=Path('saves/lab1_run_a/latest.pt'),\n",
    "    speech_manifest=Path(r'Z:/DataSets/_lab1_manifests/libirspeech_audio_clean.csv'),\n",
    "    music_manifest=Path(r'Z:/DataSets/_lab1_manifests/cc0_audio_clean.csv'),\n",
    "    max_speech=None,\n",
    "    max_music=2000,\n",
    "    seed=328,\n",
    "    sample_rate=None,\n",
    "    sample_seconds=None,\n",
    "    start_sec=0.0,\n",
    "    device='auto',\n",
    "    threshold=0.5,\n",
    "    target_fpr=0.02,\n",
    "    out_dir=Path('saves/lab1_run_a/audits'),\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model, cfg, _, device = lab1_load_checkpoint_for_audit(checkpoint_path=checkpoint_path, device=device)\n",
    "    sr = int(sample_rate if sample_rate is not None else cfg.get('sample_rate', 22050))\n",
    "    sec = float(sample_seconds if sample_seconds is not None else cfg.get('chunk_seconds', 5.0))\n",
    "\n",
    "    speech = _read_manifest(speech_manifest, force_source='libirspeech').assign(y_true=0)\n",
    "    music = _read_manifest(music_manifest, force_source='cc0_music').assign(y_true=1)\n",
    "\n",
    "    speech = speech[speech['path'].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "    music = music[music['path'].map(lambda p: Path(str(p)).exists())].reset_index(drop=True)\n",
    "\n",
    "    if max_speech is not None and len(speech) > int(max_speech):\n",
    "        speech = speech.sample(int(max_speech), random_state=int(seed)).reset_index(drop=True)\n",
    "    if max_music is not None and len(music) > int(max_music):\n",
    "        music = music.sample(int(max_music), random_state=int(seed)).reset_index(drop=True)\n",
    "\n",
    "    eval_df = pd.concat([speech, music], ignore_index=True).sample(frac=1.0, random_state=int(seed)).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for i, r in eval_df.iterrows():\n",
    "        path = Path(str(r['path']))\n",
    "        try:\n",
    "            out = lab1_infer_file(model, path, sample_rate=sr, sample_seconds=sec, start_sec=start_sec, device=device)\n",
    "            rows.append({'path': str(path), 'source': str(r['source']), 'y_true': int(r['y_true']), 'music_prob': float(out['music_prob']), 'error': ''})\n",
    "        except Exception as exc:\n",
    "            rows.append({'path': str(path), 'source': str(r['source']), 'y_true': int(r['y_true']), 'music_prob': np.nan, 'error': str(exc)})\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f'[INFO] processed {i + 1}/{len(eval_df)} files')\n",
    "\n",
    "    pred_df = pd.DataFrame(rows)\n",
    "    pred_df.to_csv(out_dir / 'gate_predictions.csv', index=False)\n",
    "\n",
    "    valid = pred_df[pred_df['music_prob'].notna()].reset_index(drop=True)\n",
    "    if len(valid) == 0:\n",
    "        raise RuntimeError('No valid predictions produced.')\n",
    "\n",
    "    y_true = valid['y_true'].to_numpy(dtype=np.int64)\n",
    "    y_score = valid['music_prob'].to_numpy(dtype=np.float64)\n",
    "    y_pred = (y_score >= float(threshold)).astype(np.int64)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    acc = float((tp + tn) / np.sum(cm))\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) > 0 else float('nan')\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) > 0 else float('nan')\n",
    "    tpr = float(tp / (tp + fn)) if (tp + fn) > 0 else float('nan')\n",
    "    auc = float(roc_auc_score(y_true, y_score)) if len(np.unique(y_true)) > 1 else float('nan')\n",
    "\n",
    "\n",
    "    # Calibrate decision threshold to satisfy target speech FPR, then report recall tradeoff.\n",
    "    speech_scores = y_score[y_true == 0]\n",
    "    if speech_scores.size > 0:\n",
    "        threshold_cal = float(np.quantile(speech_scores, 1.0 - float(target_fpr)))\n",
    "    else:\n",
    "        threshold_cal = float('nan')\n",
    "\n",
    "    if np.isfinite(threshold_cal):\n",
    "        y_pred_cal = (y_score >= threshold_cal).astype(np.int64)\n",
    "        cm_cal = confusion_matrix(y_true, y_pred_cal, labels=[0, 1])\n",
    "        tn_c, fp_c, fn_c, tp_c = cm_cal.ravel()\n",
    "        fpr_cal = float(fp_c / (fp_c + tn_c)) if (fp_c + tn_c) > 0 else float('nan')\n",
    "        tpr_cal = float(tp_c / (tp_c + fn_c)) if (tp_c + fn_c) > 0 else float('nan')\n",
    "    else:\n",
    "        fpr_cal = float('nan')\n",
    "        tpr_cal = float('nan')\n",
    "\n",
    "    summary = {\n",
    "        'n_requested_total': int(len(eval_df)),\n",
    "        'n_valid_total': int(len(valid)),\n",
    "        'n_speech_valid': int(np.sum(y_true == 0)),\n",
    "        'n_music_valid': int(np.sum(y_true == 1)),\n",
    "        'threshold': float(threshold),\n",
    "        'accuracy': acc,\n",
    "        'fpr_speech_as_music': fpr,\n",
    "        'fnr_music_as_speech': fnr,\n",
    "        'tpr_music_recall': tpr,\n",
    "        'roc_auc': auc,\n",
    "        'target_fpr': float(target_fpr),\n",
    "        'fpr_pass': bool(fpr <= float(target_fpr)),\n",
    "        'threshold_for_target_fpr': threshold_cal,\n",
    "        'achieved_fpr_at_calibrated_threshold': fpr_cal,\n",
    "        'music_recall_at_calibrated_threshold': tpr_cal,\n",
    "        'confusion_matrix_rows_true_0_1_cols_pred_0_1': cm.tolist(),\n",
    "    }\n",
    "\n",
    "    (out_dir / 'gate_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "    pd.DataFrame(cm, index=['true_speech', 'true_music'], columns=['pred_speech', 'pred_music']).to_csv(out_dir / 'gate_confusion_matrix.csv')\n",
    "\n",
    "    print('[DONE] Gate scaling eval')\n",
    "    print('valid samples:', summary['n_valid_total'], f\"(speech={summary['n_speech_valid']}, music={summary['n_music_valid']})\")\n",
    "    print('threshold:', f\"{summary['threshold']:.3f}\")\n",
    "    print('accuracy:', f\"{summary['accuracy']:.4f}\")\n",
    "    print('speech FPR:', f\"{summary['fpr_speech_as_music']:.4f}\")\n",
    "    print(f\"target FPR <= {target_fpr:.2f}:\", summary['fpr_pass'])\n",
    "    print('music recall:', f\"{summary['tpr_music_recall']:.4f}\")\n",
    "    print('ROC AUC:', f\"{summary['roc_auc']:.4f}\")\n",
    "    print('calibrated threshold @ target FPR:', f\"{summary['threshold_for_target_fpr']:.4f}\")\n",
    "    print('achieved calibrated FPR:', f\"{summary['achieved_fpr_at_calibrated_threshold']:.4f}\")\n",
    "    print('music recall @ calibrated threshold:', f\"{summary['music_recall_at_calibrated_threshold']:.4f}\")\n",
    "    print('out dir:', out_dir)\n",
    "\n",
    "    return pred_df, summary\n",
    "\n",
    "\n",
    "print('Gate scaling function loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687d36",
   "metadata": {},
   "source": [
    "### Audit Run Controls\n",
    "\n",
    "Set toggles in the next code cell to `True` only for audits you want to execute.\n",
    "\n",
    "Recommended order for a complete audit pass:\n",
    "1. `RUN_DUAL_RENDER`\n",
    "2. `RUN_INVARIANCE`\n",
    "3. `RUN_LEAKAGE`\n",
    "4. `RUN_GATE`\n",
    "\n",
    "Keep all toggles `False` when editing/training to avoid accidental long audit runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc73b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle execution for notebook-native Lab 1 audits\n",
    "\n",
    "RUN_DUAL_RENDER = False\n",
    "RUN_INVARIANCE = False\n",
    "RUN_LEAKAGE = False\n",
    "RUN_GATE = False\n",
    "\n",
    "# You can override these defaults if needed.\n",
    "PAIR_MANIFEST = Path(r'Z:/DataSets/_lab1_manifests/phase1_pdmx_dual_render_manifest.csv')\n",
    "AUDIT_DIR = Path('saves/lab1_run_a/audits')\n",
    "\n",
    "if RUN_DUAL_RENDER:\n",
    "    dual_df, dual_manifest = lab1_render_dual_soundfont_pdmx(\n",
    "        max_pdmx=500,\n",
    "        out_manifest=PAIR_MANIFEST,\n",
    "    )\n",
    "\n",
    "if RUN_INVARIANCE:\n",
    "    invariance_df, invariance_summary = lab1_run_invariance_audit(\n",
    "        pair_manifest=PAIR_MANIFEST,\n",
    "        threshold=0.92,\n",
    "        out_csv=AUDIT_DIR / 'invariance_pairs.csv',\n",
    "        out_json=AUDIT_DIR / 'invariance_summary.json',\n",
    "    )\n",
    "    display(pd.DataFrame([invariance_summary]))\n",
    "\n",
    "if RUN_LEAKAGE:\n",
    "    leakage_df, leakage_summary = lab1_run_leakage_probe(\n",
    "        n_per_class=1000,\n",
    "        leakage_threshold=0.15,\n",
    "        style_acc_threshold=0.85,\n",
    "        out_dir=AUDIT_DIR,\n",
    "    )\n",
    "    display(pd.DataFrame([leakage_summary]))\n",
    "\n",
    "if RUN_GATE:\n",
    "    gate_df, gate_summary = lab1_run_gate_scaling_eval(\n",
    "        max_speech=None,\n",
    "        max_music=2000,\n",
    "        threshold=0.5,\n",
    "        target_fpr=0.02,\n",
    "        out_dir=AUDIT_DIR,\n",
    "    )\n",
    "    display(pd.DataFrame([gate_summary]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5c6b3",
   "metadata": {},
   "source": [
    "### Stage 5: Preflight (Fail-Fast)\n",
    "\n",
    "Preflight is the anti-waste layer:\n",
    "- Runs small leakage + gate audits in minutes, not full training cycles.\n",
    "- Returns clear PASS/FAIL gates before committing to long runs.\n",
    "- Prevents expensive experiments on unstable configs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15407e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight gate: fail fast in ~5-8 minutes before full training\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def lab1_preflight_audit(\n",
    "    checkpoint_path,\n",
    "    out_dir=Path('saves/_preflight'),\n",
    "    n_per_class=120,\n",
    "    max_speech=300,\n",
    "    max_music=300,\n",
    "    target_fpr=0.02,\n",
    "    smoke_leakage_max=0.15,\n",
    "    smoke_style_min=0.85,\n",
    "    smoke_auc_min=0.85,\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    _, leak = lab1_run_leakage_probe(\n",
    "        checkpoint_path=Path(checkpoint_path),\n",
    "        n_per_class=int(n_per_class),\n",
    "        strict_balance=True,\n",
    "        leakage_threshold=0.15,\n",
    "        style_acc_threshold=0.85,\n",
    "        out_dir=out_dir / 'leakage',\n",
    "    )\n",
    "\n",
    "    _, gate = lab1_run_gate_scaling_eval(\n",
    "        checkpoint_path=Path(checkpoint_path),\n",
    "        max_speech=int(max_speech),\n",
    "        max_music=int(max_music),\n",
    "        threshold=0.5,\n",
    "        target_fpr=float(target_fpr),\n",
    "        out_dir=out_dir / 'gate',\n",
    "    )\n",
    "\n",
    "    checks = {\n",
    "        'style_ok': leak['style_probe_accuracy'] >= float(smoke_style_min),\n",
    "        'leakage_ok': leak['content_leakage_above_baseline'] <= float(smoke_leakage_max),\n",
    "        'auc_ok': gate['roc_auc'] >= float(smoke_auc_min),\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        'checkpoint': str(checkpoint_path),\n",
    "        'checks': checks,\n",
    "        'pass': bool(all(checks.values())),\n",
    "        'metrics': {\n",
    "            'content_leakage_above_baseline': float(leak['content_leakage_above_baseline']),\n",
    "            'style_probe_accuracy': float(leak['style_probe_accuracy']),\n",
    "            'gate_fpr_at_0_5': float(gate['fpr_speech_as_music']),\n",
    "            'gate_auc': float(gate['roc_auc']),\n",
    "            'gate_recall_at_2pct_fpr': float(gate['music_recall_at_calibrated_threshold']),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    (out_dir / 'preflight_summary.json').write_text(json.dumps(result, indent=2), encoding='utf-8')\n",
    "    print('[PREFLIGHT]', 'PASS' if result['pass'] else 'FAIL')\n",
    "    print(json.dumps(result['metrics'], indent=2))\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example:\n",
    "# preflight = lab1_preflight_audit(\n",
    "#     checkpoint_path=Path('saves/lab1_run_combo_af_gate/latest.pt'),\n",
    "#     out_dir=Path('saves/lab1_run_combo_af_gate/preflight'),\n",
    "# )\n",
    "# preflight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee231959",
   "metadata": {},
   "source": [
    "### Stage 6: Micro-Train AUC Sharpener\n",
    "\n",
    "This stage performs targeted Phase-3 refinement without rerunning the full curriculum.\n",
    "\n",
    "Design goals:\n",
    "- Improve gate separation (AUC) on hard speech negatives.\n",
    "- Preserve disentanglement via teacher-student anchor on `z_content`.\n",
    "- Iterate quickly with preflight validation after each sharpening run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "607b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro-train sharpener: Phase 3 only, freeze embeddings, tune boundary layers/heads\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def lab1_microtrain_auc_sharpener(\n",
    "    init_checkpoint=Path('saves/lab1_run_combo_af_gate/latest.pt'),\n",
    "    savename='lab1_run_combo_af_gate_sharp',\n",
    "    train_mode='auc_sharpener',  # 'auc_sharpener' or 'music_head_only'\n",
    "    use_teacher_anchor=True,\n",
    "    anchor_weight=1.0,\n",
    "    epochs_phase3=15,\n",
    "    max_train_steps=50,\n",
    "    max_val_steps=20,\n",
    "    batch_size=8,\n",
    "    target_fpr=0.02,\n",
    "    preflight_auc_target=0.90,\n",
    "    lr=4e-4,\n",
    "    backbone_lr_mult=0.35,\n",
    "    music_lr_mult=2.2,\n",
    "    hard_negative_min_music_prob=0.20,\n",
    "    hard_negative_repeat=2,\n",
    "):\n",
    "    cfg = deepcopy(CFG)\n",
    "    cfg['init_checkpoint'] = str(init_checkpoint)\n",
    "    cfg['phase_order'] = [3]\n",
    "    cfg['source_map_phases'] = [1, 2, 3]\n",
    "    cfg['epochs_per_phase'] = {3: int(epochs_phase3)}\n",
    "    cfg['max_train_steps_per_epoch'] = int(max_train_steps)\n",
    "    cfg['max_val_steps_per_epoch'] = int(max_val_steps)\n",
    "    cfg['batch_size'] = int(batch_size)\n",
    "\n",
    "    # AUC sharpener mode.\n",
    "    cfg['phase3_train_mode'] = str(train_mode)\n",
    "    cfg['phase3_music_head_only'] = str(train_mode).lower() == 'music_head_only'\n",
    "\n",
    "    # Teacher anchor: keep z_content near an immutable reference checkpoint.\n",
    "    cfg['use_teacher_anchor'] = bool(use_teacher_anchor)\n",
    "    cfg['teacher_anchor_checkpoint'] = str(init_checkpoint) if bool(use_teacher_anchor) else None\n",
    "\n",
    "    # Keep disentanglement geometry stable; optimize mainly the music/speech boundary.\n",
    "    anchor_w = float(anchor_weight) if bool(use_teacher_anchor) else 0.0\n",
    "    cfg['loss_weights'] = {\n",
    "        'content': 0.0,\n",
    "        'style': 0.0,\n",
    "        'music': 3.0,\n",
    "        'content_adv': 0.0,\n",
    "        'content_l1': 0.0,\n",
    "        'music_bias': 0.0002,\n",
    "        'anchor': anchor_w,\n",
    "        'music_only_when_mixed': False,\n",
    "    }\n",
    "    cfg['phase_loss_weights'] = {\n",
    "        3: {\n",
    "            'content': 0.0,\n",
    "            'style': 0.0,\n",
    "            'music': 3.0,\n",
    "            'content_adv': 0.0,\n",
    "            'content_l1': 0.0,\n",
    "            'music_bias': 0.0002,\n",
    "            'anchor': anchor_w,\n",
    "            'music_only_when_mixed': False,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cfg['lr'] = float(lr)\n",
    "    cfg['backbone_lr_mult'] = float(backbone_lr_mult)\n",
    "    cfg['music_lr_mult'] = float(music_lr_mult)\n",
    "    cfg['style_lr_mult'] = 1.0\n",
    "    cfg['adv_lr_mult'] = 1.0\n",
    "\n",
    "    # Hard-negative curriculum controls (temporarily override globals used by phase-3 manifest builder).\n",
    "    old_hn_min = globals().get('PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB', None)\n",
    "    old_hn_rep = globals().get('PHASE3_HARD_NEGATIVE_REPEAT', None)\n",
    "    globals()['PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB'] = float(hard_negative_min_music_prob)\n",
    "    globals()['PHASE3_HARD_NEGATIVE_REPEAT'] = int(hard_negative_repeat)\n",
    "\n",
    "    try:\n",
    "        model, hist, save_dir = train_curriculum_resumable(\n",
    "            cfg=cfg,\n",
    "            savename=str(savename),\n",
    "            mode='fresh',\n",
    "            run_until_phase=None,\n",
    "        )\n",
    "    finally:\n",
    "        if old_hn_min is not None:\n",
    "            globals()['PHASE3_HARD_NEGATIVE_MIN_MUSIC_PROB'] = old_hn_min\n",
    "        if old_hn_rep is not None:\n",
    "            globals()['PHASE3_HARD_NEGATIVE_REPEAT'] = old_hn_rep\n",
    "\n",
    "    ckpt = Path(save_dir) / 'latest.pt'\n",
    "    preflight = lab1_preflight_audit(\n",
    "        checkpoint_path=ckpt,\n",
    "        out_dir=Path(save_dir) / 'preflight_after_micro',\n",
    "        n_per_class=200,\n",
    "        max_speech=300,\n",
    "        max_music=300,\n",
    "        target_fpr=float(target_fpr),\n",
    "        smoke_leakage_max=0.15,\n",
    "        smoke_style_min=0.85,\n",
    "        smoke_auc_min=float(preflight_auc_target),\n",
    "    )\n",
    "\n",
    "    summary = {\n",
    "        'save_dir': str(save_dir),\n",
    "        'checkpoint': str(ckpt),\n",
    "        'history_rows': int(len(hist)),\n",
    "        'train_mode': str(train_mode),\n",
    "        'use_teacher_anchor': bool(use_teacher_anchor),\n",
    "        'anchor_weight': anchor_w,\n",
    "        'hard_negative_min_music_prob': float(hard_negative_min_music_prob),\n",
    "        'hard_negative_repeat': int(hard_negative_repeat),\n",
    "        'preflight': preflight,\n",
    "    }\n",
    "    out_json = Path(save_dir) / 'microtrain_sharpener_summary.json'\n",
    "    out_json.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "    print('saved summary:', out_json)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example:\n",
    "# sharp = lab1_microtrain_auc_sharpener(\n",
    "#     init_checkpoint=Path('saves/lab1_run_combo_af_gate_sharp_anchor_v1/latest.pt'),\n",
    "#     savename='lab1_run_combo_af_gate_exit_v1',\n",
    "#     train_mode='auc_sharpener',\n",
    "#     use_teacher_anchor=True,\n",
    "#     anchor_weight=1.0,\n",
    "#     epochs_phase3=15,\n",
    "#     lr=4e-4,\n",
    "#     hard_negative_min_music_prob=0.2,\n",
    "#     hard_negative_repeat=2,\n",
    "# )\n",
    "# sharp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae066dd",
   "metadata": {},
   "source": [
    "## Final Evaluation And Lab 1 Report\n",
    "\n",
    "The following cells summarize the final Lab 1 checkpoint (`exit_v2`) against the official success criteria.\n",
    "\n",
    "Interpretation rule:\n",
    "- If the representation metrics (leakage/style/invariance) and ranking metric (AUC) pass, Lab 1 is considered complete.\n",
    "- Gate threshold (`0.5`) behavior is treated as calibration-sensitive and can be adjusted at inference time using validated low-FPR thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c8acd",
   "metadata": {},
   "source": [
    "### Metric Definitions (Post-Training)\n",
    "\n",
    "- **Invariance cosine**: consistency of `z_content` across soundfont swaps of the same symbolic content.\n",
    "- **Content leakage**: linear source predictability above random chance from `z_content`.\n",
    "- **Style accuracy**: linear source predictability from `z_style`.\n",
    "- **Gate AUC**: ranking quality of music vs speech probabilities (threshold-independent).\n",
    "- **Recall @ 2% FPR**: practical operating-point recall after calibrating threshold to low speech false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7cf5f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Content Leakage (z_content)</td>\n",
       "      <td>&lt;= 0.15</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Style Accuracy (z_style)</td>\n",
       "      <td>&gt;= 0.85</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music Gate AUC</td>\n",
       "      <td>&gt;= 0.90</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Invariance Cosine</td>\n",
       "      <td>&gt;= 0.92</td>\n",
       "      <td>0.999859</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        metric   target     value status\n",
       "0  Content Leakage (z_content)  <= 0.15  0.108333   PASS\n",
       "1     Style Accuracy (z_style)  >= 0.85  0.941667   PASS\n",
       "2               Music Gate AUC  >= 0.90  0.929936   PASS\n",
       "3            Invariance Cosine  >= 0.92  0.999859   PASS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate details:\n",
      " - FPR @ threshold 0.5: 0.1361\n",
      " - threshold for ~2% FPR: 0.8904\n",
      " - recall at ~2% FPR: 0.5579\n"
     ]
    }
   ],
   "source": [
    "# Result card: load final Lab 1 metrics from saved audits\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "inv_path = Path('saves/lab1_run_a/audits/baseline_lock_20260210_200739/invariance_summary.json')\n",
    "exit_path = Path('saves/lab1_run_combo_af_gate_exit_v2/audits_confidence/exit_run_summary.json')\n",
    "\n",
    "if not inv_path.exists() or not exit_path.exists():\n",
    "    raise FileNotFoundError('Expected summary files not found. Run final audits first.')\n",
    "\n",
    "inv = json.loads(inv_path.read_text(encoding='utf-8'))\n",
    "exit_summary = json.loads(exit_path.read_text(encoding='utf-8'))\n",
    "leak = exit_summary['confidence']['leakage']\n",
    "gate = exit_summary['confidence']['gate']\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        'metric': 'Content Leakage (z_content)',\n",
    "        'target': '<= 0.15',\n",
    "        'value': float(leak['content_leakage_above_baseline']),\n",
    "        'status': 'PASS' if float(leak['content_leakage_above_baseline']) <= 0.15 else 'FAIL',\n",
    "    },\n",
    "    {\n",
    "        'metric': 'Style Accuracy (z_style)',\n",
    "        'target': '>= 0.85',\n",
    "        'value': float(leak['style_probe_accuracy']),\n",
    "        'status': 'PASS' if float(leak['style_probe_accuracy']) >= 0.85 else 'FAIL',\n",
    "    },\n",
    "    {\n",
    "        'metric': 'Music Gate AUC',\n",
    "        'target': '>= 0.90',\n",
    "        'value': float(gate['roc_auc']),\n",
    "        'status': 'PASS' if float(gate['roc_auc']) >= 0.90 else 'FAIL',\n",
    "    },\n",
    "    {\n",
    "        'metric': 'Invariance Cosine',\n",
    "        'target': '>= 0.92',\n",
    "        'value': float(inv['mean_cosine']),\n",
    "        'status': 'PASS' if float(inv['mean_cosine']) >= 0.92 else 'FAIL',\n",
    "    },\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(rows)\n",
    "display(result_df)\n",
    "\n",
    "print('Gate details:')\n",
    "print(' - FPR @ threshold 0.5:', round(float(gate['fpr_speech_as_music']), 4))\n",
    "print(' - threshold for ~2% FPR:', round(float(gate['threshold_for_target_fpr']), 4))\n",
    "print(' - recall at ~2% FPR:', round(float(gate['music_recall_at_calibrated_threshold']), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df77f8e6",
   "metadata": {},
   "source": [
    "### Lab 1 Discussion And Success Summary\n",
    "\n",
    "What we accomplished:\n",
    "- Achieved stable disentanglement under curriculum and adversarial pressure.\n",
    "- Solved catastrophic forgetting during gate sharpening using teacher-anchor regularization.\n",
    "- Reached final representation targets on confidence audit:\n",
    "  - leakage: `0.108`\n",
    "  - style accuracy: `0.942`\n",
    "  - AUC: `0.930`\n",
    "  - invariance cosine: `0.9999`\n",
    "\n",
    "Why Lab 1 is complete:\n",
    "- The encoder now provides a stable, style-neutral content representation suitable for downstream remastering.\n",
    "- Remaining fixed-threshold gate behavior (`FPR@0.5`) is calibration-level, not representation failure.\n",
    "- This is sufficient to proceed to Lab 2 (genre extraction) with a validated deconstruction front-end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab1-venv)",
   "language": "python",
   "name": "lab1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
